{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd48d46a-bbf7-4e39-b7df-ed55d30cfb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# セル1: ライブラリのインポート\n",
    "import pandas as pd\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "import yaml \n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba9c965e-0bc3-4f27-be6a-ceb6b45d1412",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# セル2: 統合用クラスの定義\n",
    "class DataIntegrator:\n",
    "    \"\"\"SP-APIとKeepa APIのデータを統合するクラス\"\"\"\n",
    "    \n",
    "    def __init__(self, config_path=None):\n",
    "        \"\"\"\n",
    "        初期化と設定読み込み\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        config_path : str, optional\n",
    "            設定ファイルのパス。指定がない場合はデフォルトパスを使用\n",
    "        \"\"\"\n",
    "        # プロジェクトルートディレクトリの検出\n",
    "        self.root_dir = self._find_project_root()\n",
    "        \n",
    "        # データディレクトリとログディレクトリのパスを設定\n",
    "        self.data_dir = os.path.join(self.root_dir, 'data')\n",
    "        self.log_dir = os.path.join(self.root_dir, 'logs')\n",
    "        \n",
    "        # ディレクトリの存在確認\n",
    "        if not os.path.exists(self.data_dir):\n",
    "            os.makedirs(self.data_dir)\n",
    "        if not os.path.exists(self.log_dir):\n",
    "            os.makedirs(self.log_dir)\n",
    "            \n",
    "        # 設定ファイルの読み込み\n",
    "        self.config = self._load_config(config_path)\n",
    "        \n",
    "        # ログ機能の設定\n",
    "        self.setup_logging()\n",
    "\n",
    "    def _find_project_root(self):\n",
    "        \"\"\"プロジェクトのルートディレクトリを検出する\"\"\"\n",
    "        # 現在のファイルの絶対パスを取得\n",
    "        current_dir = os.path.abspath(os.getcwd())\n",
    "        \n",
    "        # 親ディレクトリを探索\n",
    "        path = Path(current_dir)\n",
    "        while True:\n",
    "            # .gitディレクトリがあればそれをルートとみなす\n",
    "            if (path / '.git').exists():\n",
    "                return str(path)\n",
    "            \n",
    "            # プロジェクトのルートを示す他のファイル/ディレクトリの存在チェック\n",
    "            if (path / 'setup.py').exists() or (path / 'README.md').exists():\n",
    "                return str(path)\n",
    "            \n",
    "            # これ以上上の階層がない場合は現在のディレクトリを返す\n",
    "            if path.parent == path:\n",
    "                return str(path)\n",
    "            \n",
    "            # 親ディレクトリへ\n",
    "            path = path.parent\n",
    "    \n",
    "    def _load_config(self, config_path=None):\n",
    "        \"\"\"設定ファイルを読み込む\"\"\"\n",
    "        if config_path is None:\n",
    "            config_path = os.path.join(self.root_dir, 'config', 'settings.yaml')\n",
    "            \n",
    "        try:\n",
    "            with open(config_path, 'r', encoding='utf-8') as f:\n",
    "                config = yaml.safe_load(f)\n",
    "                \n",
    "            # データ統合設定の存在確認\n",
    "            if 'data_integration' not in config:\n",
    "                config['data_integration'] = {}\n",
    "                \n",
    "            # output設定の初期化（なければデフォルト値を設定）\n",
    "            if 'output' not in config['data_integration']:\n",
    "                config['data_integration']['output'] = {\n",
    "                    'sp_api_input': 'sp_api_output_filtered.csv',\n",
    "                    'keepa_input': 'keepa_output.csv',\n",
    "                    'output_file': 'integrated_data.csv'\n",
    "                }\n",
    "                \n",
    "            # sources設定の初期化（なければ空リストを設定）\n",
    "            if 'sources' not in config['data_integration']:\n",
    "                config['data_integration']['sources'] = []\n",
    "                \n",
    "            print(f\"設定ファイルを読み込みました: {config_path}\")\n",
    "            return config\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"設定ファイルの読み込みに失敗: {str(e)}\")\n",
    "            # 読み込み失敗時はデフォルト設定を返す\n",
    "            return {\n",
    "                'data_integration': {\n",
    "                    'output': {\n",
    "                        'sp_api_input': 'sp_api_output_filtered.csv',\n",
    "                        'keepa_input': 'keepa_output.csv',\n",
    "                        'output_file': 'integrated_data.csv'\n",
    "                    },\n",
    "                    'sources': []\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    \n",
    "    \n",
    "    def setup_logging(self):\n",
    "        \"\"\"ログ機能のセットアップ\"\"\"\n",
    "        log_file = os.path.join(self.log_dir, f'integration_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log')\n",
    "        logging.basicConfig(\n",
    "            filename=log_file,\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            encoding='utf-8'\n",
    "        )\n",
    "        logging.info(f\"ログファイル: {log_file}\")\n",
    "        logging.info(\"データ統合処理を開始します\")\n",
    "        print(f\"📄 ログファイル: {log_file}\")\n",
    "\n",
    "    def load_data(self, sp_api_file=None, keepa_file=None):\n",
    "        \"\"\"\n",
    "        CSVファイルの読み込み\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        sp_api_file : str, optional\n",
    "            SP-APIデータのファイルパス（デフォルトは設定ファイルから）\n",
    "        keepa_file : str, optional\n",
    "            Keepaデータのファイルパス（デフォルトは設定ファイルから）\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            (sp_df, keepa_df) - 読み込んだデータフレーム\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 入力ファイル名が指定されていない場合は設定ファイルから取得\n",
    "            if sp_api_file is None:\n",
    "                sp_api_file = self.config['data_integration']['output']['sp_api_input']\n",
    "                \n",
    "            if keepa_file is None:\n",
    "                keepa_file = self.config['data_integration']['output']['keepa_input']\n",
    "            \n",
    "            # 相対パスの場合はdataディレクトリを基準にする\n",
    "            if not os.path.isabs(sp_api_file):\n",
    "                sp_api_file = os.path.join(self.data_dir, sp_api_file)\n",
    "                \n",
    "            if not os.path.isabs(keepa_file):\n",
    "                keepa_file = os.path.join(self.data_dir, keepa_file)\n",
    "            \n",
    "            # ファイルの存在確認とエラーメッセージの改善\n",
    "            if not os.path.exists(sp_api_file):\n",
    "                error_msg = f\"SP-APIファイルが見つかりません: {sp_api_file}\"\n",
    "                print(f\"⚠️ {error_msg}\")\n",
    "                raise FileNotFoundError(error_msg)\n",
    "                \n",
    "            if not os.path.exists(keepa_file):\n",
    "                error_msg = f\"Keepaファイルが見つかりません: {keepa_file}\"\n",
    "                print(f\"⚠️ {error_msg}\")\n",
    "                raise FileNotFoundError(error_msg)\n",
    "            \n",
    "            # SP-APIデータの読み込み\n",
    "            sp_df = pd.read_csv(sp_api_file, encoding='utf-8-sig')\n",
    "            logging.info(f\"SP-APIデータを読み込みました: {len(sp_df)}件\")\n",
    "            \n",
    "            # Keepaデータの読み込み\n",
    "            keepa_df = pd.read_csv(keepa_file, encoding='utf-8-sig')\n",
    "            logging.info(f\"Keepaデータを読み込みました: {len(keepa_df)}件\")\n",
    "            \n",
    "            return sp_df, keepa_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"データ読み込みエラー: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    # 新しいメソッドを追加\n",
    "    def load_source_data(self, source_config):\n",
    "        \"\"\"\n",
    "        ソース設定に基づいてCSVファイルを読み込む\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        source_config : dict\n",
    "            ソース設定情報\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            ファイル名をキー、データフレームを値とする辞書\n",
    "        \"\"\"\n",
    "        result = {}\n",
    "        \n",
    "        try:\n",
    "            files = source_config.get('files', [])\n",
    "            key_column = source_config.get('key_column', 'JAN')\n",
    "            \n",
    "            # JAN列の代替名リスト（よくある命名パターン）\n",
    "            jan_column_alternatives = ['JAN', 'JANコード', 'jan', 'jancode', 'jan_code', 'ean', 'EAN']\n",
    "            \n",
    "            for file in files:\n",
    "                try:\n",
    "                    # 相対パスの場合はプロジェクトのdataディレクトリを基準にする\n",
    "                    file_path = file\n",
    "                    if not os.path.isabs(file_path):\n",
    "                        file_path = os.path.join(self.data_dir, file_path)\n",
    "                    \n",
    "                    # ファイルの存在確認\n",
    "                    if not os.path.exists(file_path):\n",
    "                        logging.warning(f\"ファイルが見つかりません: {file_path}。スキップします。\")\n",
    "                        print(f\"⚠️ ファイルが見つかりません: {file_path}。スキップします。\")\n",
    "                        continue\n",
    "                    \n",
    "                    # CSVファイルを読み込む\n",
    "                    df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "                    \n",
    "                    # 実際のキー列を特定\n",
    "                    actual_key_column = key_column  # デフォルト値\n",
    "                    \n",
    "                    # 設定されたキー列がない場合は代替名から探す\n",
    "                    if key_column not in df.columns:\n",
    "                        # 代替名リストから列名を探す\n",
    "                        for alt_column in jan_column_alternatives:\n",
    "                            if alt_column in df.columns:\n",
    "                                actual_key_column = alt_column\n",
    "                                print(f\"ℹ️ {file}では「{key_column}」の代わりに「{actual_key_column}」を使用します\")\n",
    "                                break\n",
    "                    \n",
    "                    # キー列の存在確認\n",
    "                    if actual_key_column not in df.columns:\n",
    "                        logging.warning(f\"キー列 '{key_column}' またはその代替名が {file} に見つかりません。スキップします。\")\n",
    "                        print(f\"⚠️ キー列 '{key_column}' またはその代替名が {file} に見つかりません。スキップします。\")\n",
    "                        print(f\"  利用可能な列: {', '.join(df.columns)}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # 結果に追加\n",
    "                    result[file] = {\n",
    "                        'df': df,\n",
    "                        'key_column': actual_key_column\n",
    "                    }\n",
    "                    \n",
    "                    logging.info(f\"ソースデータを読み込みました: {file} ({len(df)}件), キー列: {actual_key_column}\")\n",
    "                    print(f\"📊 {file}を読み込みました: {len(df)}件, キー列: {actual_key_column}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logging.error(f\"ファイル {file} の読み込みエラー: {str(e)}\")\n",
    "                    print(f\"⚠️ {file}の読み込みエラー: {str(e)}\")\n",
    "                    continue\n",
    "                    \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"ソースデータ読み込み全体エラー: {str(e)}\")\n",
    "            return {}\n",
    "\n",
    "\n",
    "    def merge_source_data(self, base_df, source_data, source_config):\n",
    "        \"\"\"\n",
    "        ベースのデータフレームにソースデータを結合する\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        base_df : pandas.DataFrame\n",
    "            ベースのデータフレーム\n",
    "        source_data : dict\n",
    "            ファイル名をキー、データフレーム情報を値とする辞書\n",
    "        source_config : dict\n",
    "            ソース設定情報\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            結合後のデータフレーム\n",
    "        \"\"\"\n",
    "        result_df = base_df.copy()\n",
    "        source_type = source_config.get('type', '不明')\n",
    "        \n",
    "        # プレフィックスをサイト名に基づいて設定\n",
    "        if source_type.lower() == 'netsea':\n",
    "            prefix = 'ネッシー_'\n",
    "        elif source_type.lower() == 'sudeli':\n",
    "            prefix = 'スーデリ_'\n",
    "        else:\n",
    "            # その他のソースタイプの場合はそのまま使用\n",
    "            prefix = source_config.get('prefix', '')\n",
    "        \n",
    "        try:\n",
    "            # ベースDFにJAN列が存在するか確認\n",
    "            if 'JAN' not in result_df.columns:\n",
    "                logging.warning(\"ベースデータにJAN列がありません。結合できない可能性があります。\")\n",
    "                print(\"⚠️ ベースデータにJAN列がありません。結合できない可能性があります。\")\n",
    "                \n",
    "                # JAN列がない場合、元コード列からJANを取得してみる\n",
    "                if '元コード' in result_df.columns:\n",
    "                    # コードタイプがEANであれば元コードをJANとして使用\n",
    "                    if 'コードタイプ' in result_df.columns:\n",
    "                        result_df['JAN'] = result_df.apply(\n",
    "                            lambda row: row['元コード'] if row['コードタイプ'] == 'EAN' else None, \n",
    "                            axis=1\n",
    "                        )\n",
    "                        print(\"📊 元コードとコードタイプからJAN列を作成しました\")\n",
    "            \n",
    "            # 統合するソースファイルごとに個別に処理\n",
    "            for file, data in source_data.items():\n",
    "                source_df = data['df']\n",
    "                key_column = data['key_column']\n",
    "                \n",
    "                # キー列の値を文字列に変換（数値の場合があるため）\n",
    "                source_df[key_column] = source_df[key_column].astype(str)\n",
    "                if 'JAN' in result_df.columns:\n",
    "                    result_df['JAN'] = result_df['JAN'].astype(str)\n",
    "                \n",
    "                # JANコードをキーに結合\n",
    "                logging.info(f\"'{key_column}'列を'JAN'として結合: {file}\")\n",
    "                print(f\"📊 {source_type}データから'{key_column}'列を'JAN'として結合: {file}\")\n",
    "                \n",
    "                # 結合前にマッチするJANコードの数を確認\n",
    "                if 'JAN' in result_df.columns:\n",
    "                    # ベースデータのJANリスト\n",
    "                    base_jans = set(result_df['JAN'].dropna().unique())\n",
    "                    # ソースデータのJANリスト\n",
    "                    source_jans = set(source_df[key_column].dropna().unique())\n",
    "                    # 共通するJANの数\n",
    "                    common_jans = base_jans.intersection(source_jans)\n",
    "                    \n",
    "                    # 重複するJANをチェック\n",
    "                    duplicate_jans = source_df[source_df[key_column].duplicated(keep=False)][key_column].unique()\n",
    "                    if len(duplicate_jans) > 0:\n",
    "                        duplicate_count = len(duplicate_jans)\n",
    "                        example_duplicates = list(duplicate_jans)[:3]  # 最大3つまで表示\n",
    "                        print(f\"ℹ️ {file}内に{duplicate_count}件の重複JANを検出: {', '.join(example_duplicates)}など\")\n",
    "                        print(f\"ℹ️ 重複JANは各JANの最初のデータのみを使用します\")\n",
    "                    \n",
    "                    # 重要な修正：各JANの最初のエントリのみを保持する\n",
    "                    # drop_duplicates()メソッドのkeep='first'引数で最初の行のみを残す\n",
    "                    source_df_unique = source_df.drop_duplicates(subset=[key_column], keep='first')\n",
    "                    \n",
    "                    # 重複削除後の結果を表示\n",
    "                    removed_count = len(source_df) - len(source_df_unique)\n",
    "                    if removed_count > 0:\n",
    "                        print(f\"ℹ️ 重複を除去: {len(source_df)}行 → {len(source_df_unique)}行 ({removed_count}行削除)\")\n",
    "                    \n",
    "                    # マッチするJANの例を表示（最大5つ）\n",
    "                    if common_jans:\n",
    "                        example_jans = list(common_jans)[:5]\n",
    "                        print(f\"ℹ️ マッチするJANの例: {', '.join(example_jans)}\")\n",
    "                        \n",
    "                        # マッチするJANがある場合のみ処理を続行\n",
    "                        # 列名にプレフィックスを追加\n",
    "                        source_df_renamed = source_df_unique.copy()  # 重複除去済みのデータフレームを使用\n",
    "                        rename_dict = {}\n",
    "                        \n",
    "                        # キー列以外の列名にプレフィックスを追加\n",
    "                        for col in source_df_unique.columns:\n",
    "                            if col != key_column:\n",
    "                                rename_dict[col] = f\"{prefix}{col}\"\n",
    "                        \n",
    "                        source_df_renamed = source_df_renamed.rename(columns=rename_dict)\n",
    "                        \n",
    "                        # 結合前の列数とデータ数を記録\n",
    "                        pre_merge_columns = len(result_df.columns)\n",
    "                        \n",
    "                        # マッチしたJANコードを持つ行のみにフィルタリング\n",
    "                        filtered_source_df = source_df_renamed[source_df_renamed[key_column].isin(common_jans)]\n",
    "                        \n",
    "                        if not filtered_source_df.empty:\n",
    "                            # 結合実行\n",
    "                            result_df = pd.merge(\n",
    "                                result_df,\n",
    "                                filtered_source_df,\n",
    "                                left_on='JAN',\n",
    "                                right_on=key_column,\n",
    "                                how='left',\n",
    "                                suffixes=('', f'_{file}')  # 重複列の処理\n",
    "                            )\n",
    "                            \n",
    "                            # 結合結果のチェック\n",
    "                            post_merge_columns = len(result_df.columns)\n",
    "                            added_columns = post_merge_columns - pre_merge_columns\n",
    "                            \n",
    "                            # 重複キー列を削除\n",
    "                            if key_column != 'JAN' and key_column in result_df.columns:\n",
    "                                result_df = result_df.drop(columns=[key_column])\n",
    "                            \n",
    "                            # 実際にマッチしたデータの確認\n",
    "                            match_count = 0\n",
    "                            if added_columns > 0:\n",
    "                                # 追加された最初の列を見つける\n",
    "                                for col in result_df.columns[-added_columns:]:\n",
    "                                    if col in result_df.columns:\n",
    "                                        match_count = result_df[col].notna().sum()\n",
    "                                        break\n",
    "                            \n",
    "                            print(f\"✅ 結合完了: {len(common_jans)}件のJANがマッチ、{match_count}行のデータに情報追加、{added_columns}列追加\")\n",
    "                            logging.info(f\"{file}のデータを結合しました: マッチJAN {len(common_jans)}件、マッチ行 {match_count}件、列数 {added_columns}列追加\")\n",
    "                        else:\n",
    "                            print(f\"⚠️ マッチするJANコードがフィルタリング後に残りませんでした。結合をスキップします。\")\n",
    "                    else:\n",
    "                        print(f\"⚠️ マッチするJANコードがありません。結合をスキップします。\")\n",
    "                else:\n",
    "                    print(f\"⚠️ 結合をスキップ: ベースデータにJAN列がありません\")\n",
    "                    logging.warning(f\"結合をスキップ: ベースデータにJAN列がありません\")\n",
    "                \n",
    "            print(f\"✅ {source_type}データの結合完了: 現在 {len(result_df.columns)}列\")\n",
    "            return result_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"ソースデータ結合エラー: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return base_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def merge_data(self, sp_df, keepa_df):\n",
    "        \"\"\"データの結合\"\"\"\n",
    "        try:\n",
    "            # ASINをキーにして結合\n",
    "            merged_df = pd.merge(\n",
    "                sp_df,\n",
    "                keepa_df,\n",
    "                on='ASIN',\n",
    "                how='outer',\n",
    "                suffixes=('_sp', '_keepa')\n",
    "            )\n",
    "            \n",
    "            logging.info(f\"データを結合しました: {len(merged_df)}件\")\n",
    "            return merged_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"データ結合エラー: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def rearrange_columns(self, df):\n",
    "        \"\"\"\n",
    "        カラムの並び替え\n",
    "        \n",
    "        指定された列順序のカラムを先頭に配置し、それ以外のカラムは末尾に保持します\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pandas.DataFrame\n",
    "            並び替え対象のデータフレーム\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            列が並び替えられたデータフレーム\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 望ましい列順を定義\n",
    "            column_order = [\n",
    "                # 基本情報1\n",
    "                'ASIN', 'JAN', '商品名', 'カテゴリー', 'メーカー型番', 'レビュー有無', \n",
    "                'メーカー名', 'ブランド名', '総出品者数', 'セット数', '商品追跡日', \n",
    "                '商品発売日', '追跡開始からの経過日数', 'アダルト商品対象',\n",
    "    \n",
    "                # 基本情報2\n",
    "                '参考価格', 'パッケージ最長辺', 'パッケージ中辺', 'パッケージ最短辺', \n",
    "                'パッケージ重量', '現在ランキング', '30日間平均ランキング', \n",
    "                '90日間平均ランキング', '180日間平均ランキング', 'amazonURL', \n",
    "                'KeepaURL', 'バリエーションASIN',\n",
    "    \n",
    "                # 価格情報\n",
    "                'Amazon価格', 'カート価格', 'カート価格送料', 'カート価格のポイント', \n",
    "                'リードタイム（時間）', 'FBA最安値', 'FBA最安値のポイント', \n",
    "                '自己発送最安値', '自己発送最安値の送料', '自己発送最安値のポイント', \n",
    "                'FBA_販売手数料', 'FBA_配送代行手数料',\n",
    "    \n",
    "                # 出品者情報\n",
    "                'amazon本体有無1', 'amazon本体有無2', 'FBA数', '自己発送数', \n",
    "                'FBA最安値出品者数', '自己発送最安値出品者数', \n",
    "                'amazon_30日間在庫切れ率', 'amazon_90日間在庫切れ率',\n",
    "    \n",
    "                # 販売数情報\n",
    "                '30日間_総販売数', '30日間_新品販売数', '30日間_中古販売数', \n",
    "                '30日間_コレクター販売数', 'Keepa30日間販売数', \n",
    "                '90日間_総販売数', '90日間_新品販売数', '90日間_中古販売数', \n",
    "                '90日間_コレクター販売数', 'Keepa90日間販売数',\n",
    "                '180日間_総販売数', '180日間_新品販売数', '180日間_中古販売数', \n",
    "                '180日間_コレクター販売数', 'Keepa180日間販売数',\n",
    "    \n",
    "                # 価格履歴\n",
    "                'amazon価格_現在価格', 'amazon価格_最高価格', 'amazon価格_最低価格',\n",
    "                'amazon価格_30日平均価格', 'amazon価格_90日平均価格', \n",
    "                'amazon価格_180日平均価格', '新品価格_現在価格', '新品価格_最高価格',\n",
    "                '新品価格_最低価格', '新品価格_30日平均価格', '新品価格_90日平均価格',\n",
    "                '新品価格_180日平均価格',\n",
    "    \n",
    "                # その他\n",
    "                '画像URL', '元コード', 'コードタイプ'\n",
    "            ]\n",
    "            \n",
    "            # 存在する列のみを抽出（エラー防止のため）\n",
    "            specified_columns = [col for col in column_order if col in df.columns]\n",
    "            \n",
    "            # 指定されていない残りの列（追加された列など）を取得\n",
    "            remaining_columns = [col for col in df.columns if col not in column_order]\n",
    "            \n",
    "            # 指定列 + 残りの列の順で新しい列順を作成\n",
    "            new_column_order = specified_columns + remaining_columns\n",
    "            \n",
    "            # 並び替えを実行\n",
    "            df = df[new_column_order]\n",
    "            \n",
    "            # 結果をログに記録\n",
    "            logging.info(f\"カラムを並び替えました: 指定列 {len(specified_columns)}列 + 追加列 {len(remaining_columns)}列\")\n",
    "            print(f\"📊 カラム並び替え: 指定列 {len(specified_columns)}列 + 追加列 {len(remaining_columns)}列\")\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"カラム並び替えエラー: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def save_data(self, df, output_file=None):\n",
    "        \"\"\"\n",
    "        統合データの保存\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pandas.DataFrame\n",
    "            保存するデータフレーム\n",
    "        output_file : str, optional\n",
    "            出力ファイル名（デフォルトは設定ファイルから）\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 出力ファイル名が指定されていない場合は設定ファイルから取得\n",
    "            if output_file is None:\n",
    "                output_file = self.config['data_integration']['output']['output_file']\n",
    "                \n",
    "            # 相対パスの場合はデータディレクトリを基準にする\n",
    "            if not os.path.isabs(output_file):\n",
    "                output_file = os.path.join(self.data_dir, output_file)\n",
    "                \n",
    "            # 出力ディレクトリの存在確認\n",
    "            output_dir = os.path.dirname(output_file)\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "                \n",
    "            df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "            logging.info(f\"統合データを保存しました: {output_file}\")\n",
    "            print(f\"✅ {len(df)}件のデータを {output_file} に保存しました\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"データ保存エラー: {str(e)}\")\n",
    "            raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea9d76a1-834f-4eb5-ac4c-a2e8398d0e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 現在のカラム確認用コード\n",
    "# print(\"=== SP-APIのカラム ===\")\n",
    "# sp_df = pd.read_csv('sp_api_output_filtered.csv', encoding='utf-8-sig')\n",
    "# print(sp_df.columns.tolist())\n",
    "\n",
    "# print(\"\\n=== Keepaのカラム ===\")\n",
    "# keepa_df = pd.read_csv('keepa_output.csv', encoding='utf-8-sig')\n",
    "# print(keepa_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "698f9c6e-10ba-4693-b75b-1a63d86fe1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "設定ファイルを読み込みました: C:\\Users\\inato\\Documents\\amazon-research\\config\\settings.yaml\n",
      "📄 ログファイル: C:\\Users\\inato\\Documents\\amazon-research\\logs\\integration_20250407_141356.log\n",
      "📂 プロジェクトルートディレクトリ: C:\\Users\\inato\\Documents\\amazon-research\n",
      "📂 データディレクトリ: C:\\Users\\inato\\Documents\\amazon-research\\data\n",
      "📂 ログディレクトリ: C:\\Users\\inato\\Documents\\amazon-research\\logs\n",
      "\n",
      "📄 設定ファイルの情報:\n",
      "  - SP-API入力ファイル: sp_api_output_filtered.csv\n",
      "  - Keepa入力ファイル: keepa_output.csv\n",
      "  - 出力ファイル: integrated_data.csv\n",
      "\n",
      "📄 追加ソース情報:\n",
      "  ソース1 (netsea): netsea_scraping.csv\n",
      "  ソース2 (sudeli): sudeli_scraping.csv\n",
      "  ソース3 (yahoo): yahoo_shopping_items.csv\n",
      "  ソース4 (yoriyasu): yoriyasu_prices.csv\n",
      "📊 SP-APIデータ: 99件\n",
      "📊 Keepaデータ: 99件\n",
      "\n",
      "📊 ソースデータ読み込み (netsea)\n",
      "📊 netsea_scraping.csvを読み込みました: 316件, キー列: JANコード\n",
      "📊 netseaデータから'JANコード'列を'JAN'として結合: netsea_scraping.csv\n",
      "ℹ️ netsea_scraping.csv内に16件の重複JANを検出: 4980901211667, 4529052003808, 4970883013632など\n",
      "ℹ️ 重複JANは各JANの最初のデータのみを使用します\n",
      "ℹ️ 重複を除去: 316行 → 300行 (16行削除)\n",
      "⚠️ マッチするJANコードがありません。結合をスキップします。\n",
      "✅ netseaデータの結合完了: 現在 83列\n",
      "✅ netseaデータの結合完了: 現在 83列\n",
      "\n",
      "📊 ソースデータ読み込み (sudeli)\n",
      "📊 sudeli_scraping.csvを読み込みました: 2345件, キー列: JANコード\n",
      "📊 sudeliデータから'JANコード'列を'JAN'として結合: sudeli_scraping.csv\n",
      "ℹ️ sudeli_scraping.csv内に1026件の重複JANを検出: 4580663871668, 4580663871705, 4580663874454など\n",
      "ℹ️ 重複JANは各JANの最初のデータのみを使用します\n",
      "ℹ️ 重複を除去: 2345行 → 1299行 (1046行削除)\n",
      "⚠️ マッチするJANコードがありません。結合をスキップします。\n",
      "✅ sudeliデータの結合完了: 現在 83列\n",
      "✅ sudeliデータの結合完了: 現在 83列\n",
      "\n",
      "📊 ソースデータ読み込み (yahoo)\n",
      "📊 yahoo_shopping_items.csvを読み込みました: 2998件, キー列: JAN\n",
      "📊 yahooデータから'JAN'列を'JAN'として結合: yahoo_shopping_items.csv\n",
      "⚠️ マッチするJANコードがありません。結合をスキップします。\n",
      "✅ yahooデータの結合完了: 現在 83列\n",
      "✅ yahooデータの結合完了: 現在 83列\n",
      "\n",
      "📊 ソースデータ読み込み (yoriyasu)\n",
      "📊 yoriyasu_prices.csvを読み込みました: 1001件, キー列: JANコード\n",
      "📊 yoriyasuデータから'JANコード'列を'JAN'として結合: yoriyasu_prices.csv\n",
      "⚠️ マッチするJANコードがありません。結合をスキップします。\n",
      "✅ yoriyasuデータの結合完了: 現在 83列\n",
      "✅ yoriyasuデータの結合完了: 現在 83列\n",
      "📊 カラム並び替え: 指定列 67列 + 追加列 16列\n",
      "✅ 99件のデータを C:\\Users\\inato\\Documents\\amazon-research\\data\\integrated_data.csv に保存しました\n",
      "\n",
      "=== 統合後のカラム構成 ===\n",
      "\n",
      "全カラム数: 83\n",
      "\n",
      "=== 統合結果の概要 ===\n",
      "・総データ件数: 99件\n",
      "・カラム数: 83列\n",
      "\n",
      "=== 統合結果のサンプル（最初の3件）===\n",
      "         ASIN              JAN メーカー型番   レビュー有無  総出品者数  セット数    商品追跡日  \\\n",
      "0  B000BNCBEE  4902508020268.0   2026  7502588      4     1  2195280   \n",
      "1  B000FNXVCQ  4902508040310.0   E031  7502704     19     1   230700   \n",
      "2  B000FQULEE  4987300505915.0    NaN  7502620      7     1   238200   \n",
      "\n",
      "        商品発売日  追跡開始からの経過日数  アダルト商品対象   参考価格  パッケージ最長辺  パッケージ中辺  パッケージ最短辺  \\\n",
      "0  19970512.0         3685     False  700.0      21.0      7.8       6.0   \n",
      "1         NaN         5050     False  500.0      35.2     10.8       4.8   \n",
      "2  20131010.0         5044     False    NaN      16.6      4.6       3.4   \n",
      "\n",
      "   パッケージ重量  現在ランキング  30日間平均ランキング  90日間平均ランキング  180日間平均ランキング  \\\n",
      "0    81.65    10070         8594         7450          7903   \n",
      "1    40.00     4435         5706         5869          4247   \n",
      "2   120.00    58137        68317        70813         54530   \n",
      "\n",
      "                                amazonURL  \\\n",
      "0  https://www.amazon.co.jp/dp/B000BNCBEE   \n",
      "1  https://www.amazon.co.jp/dp/B000FNXVCQ   \n",
      "2  https://www.amazon.co.jp/dp/B000FQULEE   \n",
      "\n",
      "                                   KeepaURL バリエーションASIN  Amazon価格   カート価格  \\\n",
      "0  https://keepa.com/#!product/5-B000BNCBEE         NaN       NaN     NaN   \n",
      "1  https://keepa.com/#!product/5-B000FNXVCQ         NaN       NaN     NaN   \n",
      "2  https://keepa.com/#!product/5-B000FQULEE         NaN       NaN  1320.0   \n",
      "\n",
      "   カート価格送料  カート価格のポイント  FBA最安値  FBA最安値のポイント  自己発送最安値  自己発送最安値の送料  \\\n",
      "0      NaN         NaN  2280.0          0.0   2100.0         0.0   \n",
      "1      NaN         NaN   700.0          0.0    780.0         1.0   \n",
      "2    550.0        13.0     NaN          NaN   1789.0         0.0   \n",
      "\n",
      "   自己発送最安値のポイント  FBA数  自己発送数  FBA最安値出品者数  自己発送最安値出品者数  amazon_30日間在庫切れ率  \\\n",
      "0           0.0     3      1           2            1               100   \n",
      "1           8.0     5     14           1            1               100   \n",
      "2           0.0     0      7           0            1               100   \n",
      "\n",
      "   amazon_90日間在庫切れ率  30日間_総販売数  30日間_新品販売数  30日間_中古販売数  30日間_コレクター販売数  \\\n",
      "0               100         30          30           0              0   \n",
      "1               100         35          35           0              0   \n",
      "2               100         23          23           0              0   \n",
      "\n",
      "   Keepa30日間販売数  90日間_総販売数  90日間_新品販売数  90日間_中古販売数  90日間_コレクター販売数  \\\n",
      "0            21         99          99           0              0   \n",
      "1            35        117         117           0              0   \n",
      "2            16         59          59           0              0   \n",
      "\n",
      "   Keepa90日間販売数  180日間_総販売数  180日間_新品販売数  180日間_中古販売数  180日間_コレクター販売数  \\\n",
      "0            86         280          280            0               0   \n",
      "1           118         335          335            0               0   \n",
      "2            41         195          195            0               0   \n",
      "\n",
      "   Keepa180日間販売数  amazon価格_現在価格  amazon価格_最高価格  amazon価格_最低価格  \\\n",
      "0            253             -1          770.0          504.0   \n",
      "1            336             -1          550.0          338.0   \n",
      "2            155             -1         1175.0          882.0   \n",
      "\n",
      "   amazon価格_30日平均価格  amazon価格_90日平均価格  amazon価格_180日平均価格  新品価格_現在価格  \\\n",
      "0                -1                -1                 -1       2100   \n",
      "1                -1                -1                522        488   \n",
      "2                -1                -1                 -1       1100   \n",
      "\n",
      "   新品価格_最高価格  新品価格_最低価格  新品価格_30日平均価格  新品価格_90日平均価格  新品価格_180日平均価格  \\\n",
      "0       3200         10          2100          2146           2300   \n",
      "1        927        320           488           488            487   \n",
      "2       1348        800          1100          1100           1065   \n",
      "\n",
      "                                               画像URL        元コード コードタイプ  \\\n",
      "0  https://images-na.ssl-images-amazon.com/images...  B000BNCBEE   ASIN   \n",
      "1  https://images-na.ssl-images-amazon.com/images...  B000FNXVCQ   ASIN   \n",
      "2  https://images-na.ssl-images-amazon.com/images...  B000FQULEE   ASIN   \n",
      "\n",
      "                            商品名_sp              カテゴリー_sp メーカー名_sp ブランド名_sp  \\\n",
      "0                        ピジョン ミルカー           BABY_BOTTLE     ピジョン     ピジョン   \n",
      "1  ピジョン ナイロンブラシ 2WAYタイプ ガラス製哺乳びん専用        CLEANING_BRUSH     ピジョン     ピジョン   \n",
      "2                 ラカルト 薬用ラカルト・ニュー5  TOOTH_CLEANING_AGENT   エスエス製薬   エスエス製薬   \n",
      "\n",
      "   Amazon本体有無1  新品総出品者数        カートセラーID                        商品名_keepa  \\\n",
      "0        False        4             NaN                        ピジョン ミルカー   \n",
      "1        False       19             NaN  ピジョン ナイロンブラシ 2WAYタイプ ガラス製哺乳びん専用   \n",
      "2        False        7  A1GDEBS71ZMV2F                 ラカルト 薬用ラカルト・ニュー5   \n",
      "\n",
      "   カテゴリー_keepa メーカー名_keepa ブランド名_keepa  amazon本体有無  90日間_総販売数.1  90日間_新品販売数.1  \\\n",
      "0    344845011        ピジョン        ピジョン          -1           99            99   \n",
      "1    344845011        ピジョン        ピジョン          -1          117           117   \n",
      "2    160384011      エスエス製薬      エスエス製薬          -1           59            59   \n",
      "\n",
      "   90日間_中古販売数.1  90日間_コレクター販売数.1  \n",
      "0             0                0  \n",
      "1             0                0  \n",
      "2             0                0  \n",
      "\n",
      "✨ 処理完了！ データを integrated_data.csv に保存しました\n"
     ]
    }
   ],
   "source": [
    "# セル3: 実行用コード\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # インテグレーターのインスタンス作成\n",
    "        integrator = DataIntegrator()\n",
    "        \n",
    "        # 各種パスの確認と表示\n",
    "        print(f\"📂 プロジェクトルートディレクトリ: {integrator.root_dir}\")\n",
    "        print(f\"📂 データディレクトリ: {integrator.data_dir}\")\n",
    "        print(f\"📂 ログディレクトリ: {integrator.log_dir}\")\n",
    "        \n",
    "        # 設定ファイルから読み込んだファイル名を表示\n",
    "        config = integrator.config['data_integration']['output']\n",
    "        print(f\"\\n📄 設定ファイルの情報:\")\n",
    "        print(f\"  - SP-API入力ファイル: {config['sp_api_input']}\")\n",
    "        print(f\"  - Keepa入力ファイル: {config['keepa_input']}\")\n",
    "        print(f\"  - 出力ファイル: {config['output_file']}\")\n",
    "        \n",
    "        # 追加ソース情報を表示\n",
    "        sources = integrator.config['data_integration'].get('sources', [])\n",
    "        if sources:\n",
    "            print(\"\\n📄 追加ソース情報:\")\n",
    "            for i, source in enumerate(sources, 1):\n",
    "                source_type = source.get('type', '不明')\n",
    "                files = source.get('files', [])\n",
    "                print(f\"  ソース{i} ({source_type}): {', '.join(files)}\")\n",
    "        \n",
    "        # SP-APIとKeepaデータの読み込み\n",
    "        sp_df, keepa_df = integrator.load_data()\n",
    "        print(f\"📊 SP-APIデータ: {len(sp_df)}件\")\n",
    "        print(f\"📊 Keepaデータ: {len(keepa_df)}件\")\n",
    "        \n",
    "        # keepa_dfを保存（JAN-ASINマッピング用）\n",
    "        integrator.keepa_df = keepa_df\n",
    "        \n",
    "        # SP-APIとKeepaデータの結合\n",
    "        merged_df = integrator.merge_data(sp_df, keepa_df)\n",
    "        \n",
    "        # 追加ソースデータの結合\n",
    "        for source_config in sources:\n",
    "            source_type = source_config.get('type', '不明')\n",
    "            print(f\"\\n📊 ソースデータ読み込み ({source_type})\")\n",
    "            \n",
    "            # ソースデータの読み込み\n",
    "            source_data = integrator.load_source_data(source_config)\n",
    "            \n",
    "            if source_data:\n",
    "                # ソースデータの結合\n",
    "                merged_df = integrator.merge_source_data(merged_df, source_data, source_config)\n",
    "                print(f\"✅ {source_type}データの結合完了: 現在 {len(merged_df.columns)}列\")\n",
    "        \n",
    "        # カラムの並び替え\n",
    "        merged_df = integrator.rearrange_columns(merged_df)\n",
    "        \n",
    "        # 統合データの保存\n",
    "        integrator.save_data(merged_df)\n",
    "        \n",
    "        # カラム構成の確認\n",
    "        print(\"\\n=== 統合後のカラム構成 ===\")\n",
    "        print(f\"\\n全カラム数: {len(merged_df.columns)}\")\n",
    "        \n",
    "        # 結果の統計情報\n",
    "        print(\"\\n=== 統合結果の概要 ===\")\n",
    "        print(f\"・総データ件数: {len(merged_df)}件\")\n",
    "        print(f\"・カラム数: {len(merged_df.columns)}列\")\n",
    "        \n",
    "        # サンプルデータの表示\n",
    "        print(\"\\n=== 統合結果のサンプル（最初の3件）===\")\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        print(merged_df.head(3))\n",
    "        \n",
    "        print(f\"\\n✨ 処理完了！ データを {config['output_file']} に保存しました\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ エラーが発生しました: {str(e)}\")\n",
    "        logging.error(f\"実行時エラー: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60064145-ce12-4299-8d51-f0f1dd2ed14f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c33e6e0-c488-4477-9b8a-e04581da6274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79a4879-2a5f-4f70-bf5d-bba0b4e119fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
