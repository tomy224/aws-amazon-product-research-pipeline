{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd48d46a-bbf7-4e39-b7df-ed55d30cfb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚»ãƒ«1: ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "import pandas as pd\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "import yaml \n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba9c965e-0bc3-4f27-be6a-ceb6b45d1412",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ã‚»ãƒ«2: çµ±åˆç”¨ã‚¯ãƒ©ã‚¹ã®å®šç¾©\n",
    "class DataIntegrator:\n",
    "    \"\"\"SP-APIã¨Keepa APIã®ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã™ã‚‹ã‚¯ãƒ©ã‚¹\"\"\"\n",
    "    \n",
    "    def __init__(self, config_path=None):\n",
    "        \"\"\"\n",
    "        åˆæœŸåŒ–ã¨è¨­å®šèª­ã¿è¾¼ã¿\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        config_path : str, optional\n",
    "            è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã€‚æŒ‡å®šãŒãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ã‚¹ã‚’ä½¿ç”¨\n",
    "        \"\"\"\n",
    "        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æ¤œå‡º\n",
    "        self.root_dir = self._find_project_root()\n",
    "        \n",
    "        # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã‚’è¨­å®š\n",
    "        self.data_dir = os.path.join(self.root_dir, 'data')\n",
    "        self.log_dir = os.path.join(self.root_dir, 'logs')\n",
    "        \n",
    "        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèª\n",
    "        if not os.path.exists(self.data_dir):\n",
    "            os.makedirs(self.data_dir)\n",
    "        if not os.path.exists(self.log_dir):\n",
    "            os.makedirs(self.log_dir)\n",
    "            \n",
    "        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿\n",
    "        self.config = self._load_config(config_path)\n",
    "        \n",
    "        # ãƒ­ã‚°æ©Ÿèƒ½ã®è¨­å®š\n",
    "        self.setup_logging()\n",
    "\n",
    "    def _find_project_root(self):\n",
    "        \"\"\"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¤œå‡ºã™ã‚‹\"\"\"\n",
    "        # ç¾åœ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®çµ¶å¯¾ãƒ‘ã‚¹ã‚’å–å¾—\n",
    "        current_dir = os.path.abspath(os.getcwd())\n",
    "        \n",
    "        # è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¢ç´¢\n",
    "        path = Path(current_dir)\n",
    "        while True:\n",
    "            # .gitãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒã‚ã‚Œã°ãã‚Œã‚’ãƒ«ãƒ¼ãƒˆã¨ã¿ãªã™\n",
    "            if (path / '.git').exists():\n",
    "                return str(path)\n",
    "            \n",
    "            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆã‚’ç¤ºã™ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ«/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯\n",
    "            if (path / 'setup.py').exists() or (path / 'README.md').exists():\n",
    "                return str(path)\n",
    "            \n",
    "            # ã“ã‚Œä»¥ä¸Šä¸Šã®éšå±¤ãŒãªã„å ´åˆã¯ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¿”ã™\n",
    "            if path.parent == path:\n",
    "                return str(path)\n",
    "            \n",
    "            # è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸\n",
    "            path = path.parent\n",
    "    \n",
    "    def _load_config(self, config_path=None):\n",
    "        \"\"\"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\"\"\"\n",
    "        if config_path is None:\n",
    "            config_path = os.path.join(self.root_dir, 'config', 'settings.yaml')\n",
    "            \n",
    "        try:\n",
    "            with open(config_path, 'r', encoding='utf-8') as f:\n",
    "                config = yaml.safe_load(f)\n",
    "                \n",
    "            # ãƒ‡ãƒ¼ã‚¿çµ±åˆè¨­å®šã®å­˜åœ¨ç¢ºèª\n",
    "            if 'data_integration' not in config:\n",
    "                config['data_integration'] = {}\n",
    "                \n",
    "            # outputè¨­å®šã®åˆæœŸåŒ–ï¼ˆãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®šï¼‰\n",
    "            if 'output' not in config['data_integration']:\n",
    "                config['data_integration']['output'] = {\n",
    "                    'sp_api_input': 'sp_api_output_filtered.csv',\n",
    "                    'keepa_input': 'keepa_output.csv',\n",
    "                    'output_file': 'integrated_data.csv'\n",
    "                }\n",
    "                \n",
    "            # sourcesè¨­å®šã®åˆæœŸåŒ–ï¼ˆãªã‘ã‚Œã°ç©ºãƒªã‚¹ãƒˆã‚’è¨­å®šï¼‰\n",
    "            if 'sources' not in config['data_integration']:\n",
    "                config['data_integration']['sources'] = []\n",
    "                \n",
    "            print(f\"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {config_path}\")\n",
    "            return config\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {str(e)}\")\n",
    "            # èª­ã¿è¾¼ã¿å¤±æ•—æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’è¿”ã™\n",
    "            return {\n",
    "                'data_integration': {\n",
    "                    'output': {\n",
    "                        'sp_api_input': 'sp_api_output_filtered.csv',\n",
    "                        'keepa_input': 'keepa_output.csv',\n",
    "                        'output_file': 'integrated_data.csv'\n",
    "                    },\n",
    "                    'sources': []\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    \n",
    "    \n",
    "    def setup_logging(self):\n",
    "        \"\"\"ãƒ­ã‚°æ©Ÿèƒ½ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\"\"\"\n",
    "        log_file = os.path.join(self.log_dir, f'integration_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log')\n",
    "        logging.basicConfig(\n",
    "            filename=log_file,\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            encoding='utf-8'\n",
    "        )\n",
    "        logging.info(f\"ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file}\")\n",
    "        logging.info(\"ãƒ‡ãƒ¼ã‚¿çµ±åˆå‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™\")\n",
    "        print(f\"ğŸ“„ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file}\")\n",
    "\n",
    "    def load_data(self, sp_api_file=None, keepa_file=None):\n",
    "        \"\"\"\n",
    "        CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        sp_api_file : str, optional\n",
    "            SP-APIãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ï¼‰\n",
    "        keepa_file : str, optional\n",
    "            Keepaãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ï¼‰\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            (sp_df, keepa_df) - èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ \n",
    "        \"\"\"\n",
    "        try:\n",
    "            # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«åãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—\n",
    "            if sp_api_file is None:\n",
    "                sp_api_file = self.config['data_integration']['output']['sp_api_input']\n",
    "                \n",
    "            if keepa_file is None:\n",
    "                keepa_file = self.config['data_integration']['output']['keepa_input']\n",
    "            \n",
    "            # ç›¸å¯¾ãƒ‘ã‚¹ã®å ´åˆã¯dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’åŸºæº–ã«ã™ã‚‹\n",
    "            if not os.path.isabs(sp_api_file):\n",
    "                sp_api_file = os.path.join(self.data_dir, sp_api_file)\n",
    "                \n",
    "            if not os.path.isabs(keepa_file):\n",
    "                keepa_file = os.path.join(self.data_dir, keepa_file)\n",
    "            \n",
    "            # ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèªã¨ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æ”¹å–„\n",
    "            if not os.path.exists(sp_api_file):\n",
    "                error_msg = f\"SP-APIãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {sp_api_file}\"\n",
    "                print(f\"âš ï¸ {error_msg}\")\n",
    "                raise FileNotFoundError(error_msg)\n",
    "                \n",
    "            if not os.path.exists(keepa_file):\n",
    "                error_msg = f\"Keepaãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {keepa_file}\"\n",
    "                print(f\"âš ï¸ {error_msg}\")\n",
    "                raise FileNotFoundError(error_msg)\n",
    "            \n",
    "            # SP-APIãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\n",
    "            sp_df = pd.read_csv(sp_api_file, encoding='utf-8-sig')\n",
    "            logging.info(f\"SP-APIãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {len(sp_df)}ä»¶\")\n",
    "            \n",
    "            # Keepaãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\n",
    "            keepa_df = pd.read_csv(keepa_file, encoding='utf-8-sig')\n",
    "            logging.info(f\"Keepaãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {len(keepa_df)}ä»¶\")\n",
    "            \n",
    "            return sp_df, keepa_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    # æ–°ã—ã„ãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ \n",
    "    def load_source_data(self, source_config):\n",
    "        \"\"\"\n",
    "        ã‚½ãƒ¼ã‚¹è¨­å®šã«åŸºã¥ã„ã¦CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        source_config : dict\n",
    "            ã‚½ãƒ¼ã‚¹è¨­å®šæƒ…å ±\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ã‚­ãƒ¼ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å€¤ã¨ã™ã‚‹è¾æ›¸\n",
    "        \"\"\"\n",
    "        result = {}\n",
    "        \n",
    "        try:\n",
    "            files = source_config.get('files', [])\n",
    "            key_column = source_config.get('key_column', 'JAN')\n",
    "            \n",
    "            # JANåˆ—ã®ä»£æ›¿åãƒªã‚¹ãƒˆï¼ˆã‚ˆãã‚ã‚‹å‘½åãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰\n",
    "            jan_column_alternatives = ['JAN', 'JANã‚³ãƒ¼ãƒ‰', 'jan', 'jancode', 'jan_code', 'ean', 'EAN']\n",
    "            \n",
    "            for file in files:\n",
    "                try:\n",
    "                    # ç›¸å¯¾ãƒ‘ã‚¹ã®å ´åˆã¯ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’åŸºæº–ã«ã™ã‚‹\n",
    "                    file_path = file\n",
    "                    if not os.path.isabs(file_path):\n",
    "                        file_path = os.path.join(self.data_dir, file_path)\n",
    "                    \n",
    "                    # ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª\n",
    "                    if not os.path.exists(file_path):\n",
    "                        logging.warning(f\"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚\")\n",
    "                        print(f\"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚\")\n",
    "                        continue\n",
    "                    \n",
    "                    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
    "                    df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "                    \n",
    "                    # å®Ÿéš›ã®ã‚­ãƒ¼åˆ—ã‚’ç‰¹å®š\n",
    "                    actual_key_column = key_column  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤\n",
    "                    \n",
    "                    # è¨­å®šã•ã‚ŒãŸã‚­ãƒ¼åˆ—ãŒãªã„å ´åˆã¯ä»£æ›¿åã‹ã‚‰æ¢ã™\n",
    "                    if key_column not in df.columns:\n",
    "                        # ä»£æ›¿åãƒªã‚¹ãƒˆã‹ã‚‰åˆ—åã‚’æ¢ã™\n",
    "                        for alt_column in jan_column_alternatives:\n",
    "                            if alt_column in df.columns:\n",
    "                                actual_key_column = alt_column\n",
    "                                print(f\"â„¹ï¸ {file}ã§ã¯ã€Œ{key_column}ã€ã®ä»£ã‚ã‚Šã«ã€Œ{actual_key_column}ã€ã‚’ä½¿ç”¨ã—ã¾ã™\")\n",
    "                                break\n",
    "                    \n",
    "                    # ã‚­ãƒ¼åˆ—ã®å­˜åœ¨ç¢ºèª\n",
    "                    if actual_key_column not in df.columns:\n",
    "                        logging.warning(f\"ã‚­ãƒ¼åˆ— '{key_column}' ã¾ãŸã¯ãã®ä»£æ›¿åãŒ {file} ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚\")\n",
    "                        print(f\"âš ï¸ ã‚­ãƒ¼åˆ— '{key_column}' ã¾ãŸã¯ãã®ä»£æ›¿åãŒ {file} ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚\")\n",
    "                        print(f\"  åˆ©ç”¨å¯èƒ½ãªåˆ—: {', '.join(df.columns)}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # çµæœã«è¿½åŠ \n",
    "                    result[file] = {\n",
    "                        'df': df,\n",
    "                        'key_column': actual_key_column\n",
    "                    }\n",
    "                    \n",
    "                    logging.info(f\"ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {file} ({len(df)}ä»¶), ã‚­ãƒ¼åˆ—: {actual_key_column}\")\n",
    "                    print(f\"ğŸ“Š {file}ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {len(df)}ä»¶, ã‚­ãƒ¼åˆ—: {actual_key_column}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logging.error(f\"ãƒ•ã‚¡ã‚¤ãƒ« {file} ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}\")\n",
    "                    print(f\"âš ï¸ {file}ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}\")\n",
    "                    continue\n",
    "                    \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å…¨ä½“ã‚¨ãƒ©ãƒ¼: {str(e)}\")\n",
    "            return {}\n",
    "\n",
    "\n",
    "    def merge_source_data(self, base_df, source_data, source_config):\n",
    "        \"\"\"\n",
    "        ãƒ™ãƒ¼ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆã™ã‚‹\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        base_df : pandas.DataFrame\n",
    "            ãƒ™ãƒ¼ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ \n",
    "        source_data : dict\n",
    "            ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ã‚­ãƒ¼ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ æƒ…å ±ã‚’å€¤ã¨ã™ã‚‹è¾æ›¸\n",
    "        source_config : dict\n",
    "            ã‚½ãƒ¼ã‚¹è¨­å®šæƒ…å ±\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            çµåˆå¾Œã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ \n",
    "        \"\"\"\n",
    "        result_df = base_df.copy()\n",
    "        source_type = source_config.get('type', 'ä¸æ˜')\n",
    "        \n",
    "        # ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’ã‚µã‚¤ãƒˆåã«åŸºã¥ã„ã¦è¨­å®š\n",
    "        if source_type.lower() == 'netsea':\n",
    "            prefix = 'ãƒãƒƒã‚·ãƒ¼_'\n",
    "        elif source_type.lower() == 'sudeli':\n",
    "            prefix = 'ã‚¹ãƒ¼ãƒ‡ãƒª_'\n",
    "        else:\n",
    "            # ãã®ä»–ã®ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã®å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨\n",
    "            prefix = source_config.get('prefix', '')\n",
    "        \n",
    "        try:\n",
    "            # ãƒ™ãƒ¼ã‚¹DFã«JANåˆ—ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª\n",
    "            if 'JAN' not in result_df.columns:\n",
    "                logging.warning(\"ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã«JANåˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚çµåˆã§ããªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\")\n",
    "                print(\"âš ï¸ ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã«JANåˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚çµåˆã§ããªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\")\n",
    "                \n",
    "                # JANåˆ—ãŒãªã„å ´åˆã€å…ƒã‚³ãƒ¼ãƒ‰åˆ—ã‹ã‚‰JANã‚’å–å¾—ã—ã¦ã¿ã‚‹\n",
    "                if 'å…ƒã‚³ãƒ¼ãƒ‰' in result_df.columns:\n",
    "                    # ã‚³ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—ãŒEANã§ã‚ã‚Œã°å…ƒã‚³ãƒ¼ãƒ‰ã‚’JANã¨ã—ã¦ä½¿ç”¨\n",
    "                    if 'ã‚³ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—' in result_df.columns:\n",
    "                        result_df['JAN'] = result_df.apply(\n",
    "                            lambda row: row['å…ƒã‚³ãƒ¼ãƒ‰'] if row['ã‚³ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—'] == 'EAN' else None, \n",
    "                            axis=1\n",
    "                        )\n",
    "                        print(\"ğŸ“Š å…ƒã‚³ãƒ¼ãƒ‰ã¨ã‚³ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—ã‹ã‚‰JANåˆ—ã‚’ä½œæˆã—ã¾ã—ãŸ\")\n",
    "            \n",
    "            # çµ±åˆã™ã‚‹ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«å€‹åˆ¥ã«å‡¦ç†\n",
    "            for file, data in source_data.items():\n",
    "                source_df = data['df']\n",
    "                key_column = data['key_column']\n",
    "                \n",
    "                # ã‚­ãƒ¼åˆ—ã®å€¤ã‚’æ–‡å­—åˆ—ã«å¤‰æ›ï¼ˆæ•°å€¤ã®å ´åˆãŒã‚ã‚‹ãŸã‚ï¼‰\n",
    "                source_df[key_column] = source_df[key_column].astype(str)\n",
    "                if 'JAN' in result_df.columns:\n",
    "                    result_df['JAN'] = result_df['JAN'].astype(str)\n",
    "                \n",
    "                # JANã‚³ãƒ¼ãƒ‰ã‚’ã‚­ãƒ¼ã«çµåˆ\n",
    "                logging.info(f\"'{key_column}'åˆ—ã‚’'JAN'ã¨ã—ã¦çµåˆ: {file}\")\n",
    "                print(f\"ğŸ“Š {source_type}ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰'{key_column}'åˆ—ã‚’'JAN'ã¨ã—ã¦çµåˆ: {file}\")\n",
    "                \n",
    "                # çµåˆå‰ã«ãƒãƒƒãƒã™ã‚‹JANã‚³ãƒ¼ãƒ‰ã®æ•°ã‚’ç¢ºèª\n",
    "                if 'JAN' in result_df.columns:\n",
    "                    # ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®JANãƒªã‚¹ãƒˆ\n",
    "                    base_jans = set(result_df['JAN'].dropna().unique())\n",
    "                    # ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®JANãƒªã‚¹ãƒˆ\n",
    "                    source_jans = set(source_df[key_column].dropna().unique())\n",
    "                    # å…±é€šã™ã‚‹JANã®æ•°\n",
    "                    common_jans = base_jans.intersection(source_jans)\n",
    "                    \n",
    "                    # é‡è¤‡ã™ã‚‹JANã‚’ãƒã‚§ãƒƒã‚¯\n",
    "                    duplicate_jans = source_df[source_df[key_column].duplicated(keep=False)][key_column].unique()\n",
    "                    if len(duplicate_jans) > 0:\n",
    "                        duplicate_count = len(duplicate_jans)\n",
    "                        example_duplicates = list(duplicate_jans)[:3]  # æœ€å¤§3ã¤ã¾ã§è¡¨ç¤º\n",
    "                        print(f\"â„¹ï¸ {file}å†…ã«{duplicate_count}ä»¶ã®é‡è¤‡JANã‚’æ¤œå‡º: {', '.join(example_duplicates)}ãªã©\")\n",
    "                        print(f\"â„¹ï¸ é‡è¤‡JANã¯å„JANã®æœ€åˆã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ç”¨ã—ã¾ã™\")\n",
    "                    \n",
    "                    # é‡è¦ãªä¿®æ­£ï¼šå„JANã®æœ€åˆã®ã‚¨ãƒ³ãƒˆãƒªã®ã¿ã‚’ä¿æŒã™ã‚‹\n",
    "                    # drop_duplicates()ãƒ¡ã‚½ãƒƒãƒ‰ã®keep='first'å¼•æ•°ã§æœ€åˆã®è¡Œã®ã¿ã‚’æ®‹ã™\n",
    "                    source_df_unique = source_df.drop_duplicates(subset=[key_column], keep='first')\n",
    "                    \n",
    "                    # é‡è¤‡å‰Šé™¤å¾Œã®çµæœã‚’è¡¨ç¤º\n",
    "                    removed_count = len(source_df) - len(source_df_unique)\n",
    "                    if removed_count > 0:\n",
    "                        print(f\"â„¹ï¸ é‡è¤‡ã‚’é™¤å»: {len(source_df)}è¡Œ â†’ {len(source_df_unique)}è¡Œ ({removed_count}è¡Œå‰Šé™¤)\")\n",
    "                    \n",
    "                    # ãƒãƒƒãƒã™ã‚‹JANã®ä¾‹ã‚’è¡¨ç¤ºï¼ˆæœ€å¤§5ã¤ï¼‰\n",
    "                    if common_jans:\n",
    "                        example_jans = list(common_jans)[:5]\n",
    "                        print(f\"â„¹ï¸ ãƒãƒƒãƒã™ã‚‹JANã®ä¾‹: {', '.join(example_jans)}\")\n",
    "                        \n",
    "                        # ãƒãƒƒãƒã™ã‚‹JANãŒã‚ã‚‹å ´åˆã®ã¿å‡¦ç†ã‚’ç¶šè¡Œ\n",
    "                        # åˆ—åã«ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’è¿½åŠ \n",
    "                        source_df_renamed = source_df_unique.copy()  # é‡è¤‡é™¤å»æ¸ˆã¿ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½¿ç”¨\n",
    "                        rename_dict = {}\n",
    "                        \n",
    "                        # ã‚­ãƒ¼åˆ—ä»¥å¤–ã®åˆ—åã«ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’è¿½åŠ \n",
    "                        for col in source_df_unique.columns:\n",
    "                            if col != key_column:\n",
    "                                rename_dict[col] = f\"{prefix}{col}\"\n",
    "                        \n",
    "                        source_df_renamed = source_df_renamed.rename(columns=rename_dict)\n",
    "                        \n",
    "                        # çµåˆå‰ã®åˆ—æ•°ã¨ãƒ‡ãƒ¼ã‚¿æ•°ã‚’è¨˜éŒ²\n",
    "                        pre_merge_columns = len(result_df.columns)\n",
    "                        \n",
    "                        # ãƒãƒƒãƒã—ãŸJANã‚³ãƒ¼ãƒ‰ã‚’æŒã¤è¡Œã®ã¿ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "                        filtered_source_df = source_df_renamed[source_df_renamed[key_column].isin(common_jans)]\n",
    "                        \n",
    "                        if not filtered_source_df.empty:\n",
    "                            # çµåˆå®Ÿè¡Œ\n",
    "                            result_df = pd.merge(\n",
    "                                result_df,\n",
    "                                filtered_source_df,\n",
    "                                left_on='JAN',\n",
    "                                right_on=key_column,\n",
    "                                how='left',\n",
    "                                suffixes=('', f'_{file}')  # é‡è¤‡åˆ—ã®å‡¦ç†\n",
    "                            )\n",
    "                            \n",
    "                            # çµåˆçµæœã®ãƒã‚§ãƒƒã‚¯\n",
    "                            post_merge_columns = len(result_df.columns)\n",
    "                            added_columns = post_merge_columns - pre_merge_columns\n",
    "                            \n",
    "                            # é‡è¤‡ã‚­ãƒ¼åˆ—ã‚’å‰Šé™¤\n",
    "                            if key_column != 'JAN' and key_column in result_df.columns:\n",
    "                                result_df = result_df.drop(columns=[key_column])\n",
    "                            \n",
    "                            # å®Ÿéš›ã«ãƒãƒƒãƒã—ãŸãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª\n",
    "                            match_count = 0\n",
    "                            if added_columns > 0:\n",
    "                                # è¿½åŠ ã•ã‚ŒãŸæœ€åˆã®åˆ—ã‚’è¦‹ã¤ã‘ã‚‹\n",
    "                                for col in result_df.columns[-added_columns:]:\n",
    "                                    if col in result_df.columns:\n",
    "                                        match_count = result_df[col].notna().sum()\n",
    "                                        break\n",
    "                            \n",
    "                            print(f\"âœ… çµåˆå®Œäº†: {len(common_jans)}ä»¶ã®JANãŒãƒãƒƒãƒã€{match_count}è¡Œã®ãƒ‡ãƒ¼ã‚¿ã«æƒ…å ±è¿½åŠ ã€{added_columns}åˆ—è¿½åŠ \")\n",
    "                            logging.info(f\"{file}ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆã—ã¾ã—ãŸ: ãƒãƒƒãƒJAN {len(common_jans)}ä»¶ã€ãƒãƒƒãƒè¡Œ {match_count}ä»¶ã€åˆ—æ•° {added_columns}åˆ—è¿½åŠ \")\n",
    "                        else:\n",
    "                            print(f\"âš ï¸ ãƒãƒƒãƒã™ã‚‹JANã‚³ãƒ¼ãƒ‰ãŒãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã«æ®‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚çµåˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚\")\n",
    "                    else:\n",
    "                        print(f\"âš ï¸ ãƒãƒƒãƒã™ã‚‹JANã‚³ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚çµåˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚\")\n",
    "                else:\n",
    "                    print(f\"âš ï¸ çµåˆã‚’ã‚¹ã‚­ãƒƒãƒ—: ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã«JANåˆ—ãŒã‚ã‚Šã¾ã›ã‚“\")\n",
    "                    logging.warning(f\"çµåˆã‚’ã‚¹ã‚­ãƒƒãƒ—: ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã«JANåˆ—ãŒã‚ã‚Šã¾ã›ã‚“\")\n",
    "                \n",
    "            print(f\"âœ… {source_type}ãƒ‡ãƒ¼ã‚¿ã®çµåˆå®Œäº†: ç¾åœ¨ {len(result_df.columns)}åˆ—\")\n",
    "            return result_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿çµåˆã‚¨ãƒ©ãƒ¼: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return base_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def merge_data(self, sp_df, keepa_df):\n",
    "        \"\"\"ãƒ‡ãƒ¼ã‚¿ã®çµåˆ\"\"\"\n",
    "        try:\n",
    "            # ASINã‚’ã‚­ãƒ¼ã«ã—ã¦çµåˆ\n",
    "            merged_df = pd.merge(\n",
    "                sp_df,\n",
    "                keepa_df,\n",
    "                on='ASIN',\n",
    "                how='outer',\n",
    "                suffixes=('_sp', '_keepa')\n",
    "            )\n",
    "            \n",
    "            logging.info(f\"ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆã—ã¾ã—ãŸ: {len(merged_df)}ä»¶\")\n",
    "            return merged_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"ãƒ‡ãƒ¼ã‚¿çµåˆã‚¨ãƒ©ãƒ¼: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def rearrange_columns(self, df):\n",
    "        \"\"\"\n",
    "        ã‚«ãƒ©ãƒ ã®ä¸¦ã³æ›¿ãˆ\n",
    "        \n",
    "        æŒ‡å®šã•ã‚ŒãŸåˆ—é †åºã®ã‚«ãƒ©ãƒ ã‚’å…ˆé ­ã«é…ç½®ã—ã€ãã‚Œä»¥å¤–ã®ã‚«ãƒ©ãƒ ã¯æœ«å°¾ã«ä¿æŒã—ã¾ã™\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pandas.DataFrame\n",
    "            ä¸¦ã³æ›¿ãˆå¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ \n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            åˆ—ãŒä¸¦ã³æ›¿ãˆã‚‰ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ \n",
    "        \"\"\"\n",
    "        try:\n",
    "            # æœ›ã¾ã—ã„åˆ—é †ã‚’å®šç¾©\n",
    "            column_order = [\n",
    "                # åŸºæœ¬æƒ…å ±1\n",
    "                'ASIN', 'JAN', 'å•†å“å', 'ã‚«ãƒ†ã‚´ãƒªãƒ¼', 'ãƒ¡ãƒ¼ã‚«ãƒ¼å‹ç•ª', 'ãƒ¬ãƒ“ãƒ¥ãƒ¼æœ‰ç„¡', \n",
    "                'ãƒ¡ãƒ¼ã‚«ãƒ¼å', 'ãƒ–ãƒ©ãƒ³ãƒ‰å', 'ç·å‡ºå“è€…æ•°', 'ã‚»ãƒƒãƒˆæ•°', 'å•†å“è¿½è·¡æ—¥', \n",
    "                'å•†å“ç™ºå£²æ—¥', 'è¿½è·¡é–‹å§‹ã‹ã‚‰ã®çµŒéæ—¥æ•°', 'ã‚¢ãƒ€ãƒ«ãƒˆå•†å“å¯¾è±¡',\n",
    "    \n",
    "                # åŸºæœ¬æƒ…å ±2\n",
    "                'å‚è€ƒä¾¡æ ¼', 'ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸æœ€é•·è¾º', 'ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ä¸­è¾º', 'ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸æœ€çŸ­è¾º', \n",
    "                'ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸é‡é‡', 'ç¾åœ¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°', '30æ—¥é–“å¹³å‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°', \n",
    "                '90æ—¥é–“å¹³å‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°', '180æ—¥é–“å¹³å‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°', 'amazonURL', \n",
    "                'KeepaURL', 'ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ASIN',\n",
    "    \n",
    "                # ä¾¡æ ¼æƒ…å ±\n",
    "                'Amazonä¾¡æ ¼', 'ã‚«ãƒ¼ãƒˆä¾¡æ ¼', 'ã‚«ãƒ¼ãƒˆä¾¡æ ¼é€æ–™', 'ã‚«ãƒ¼ãƒˆä¾¡æ ¼ã®ãƒã‚¤ãƒ³ãƒˆ', \n",
    "                'ãƒªãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ ï¼ˆæ™‚é–“ï¼‰', 'FBAæœ€å®‰å€¤', 'FBAæœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ', \n",
    "                'è‡ªå·±ç™ºé€æœ€å®‰å€¤', 'è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®é€æ–™', 'è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ', \n",
    "                'FBA_è²©å£²æ‰‹æ•°æ–™', 'FBA_é…é€ä»£è¡Œæ‰‹æ•°æ–™',\n",
    "    \n",
    "                # å‡ºå“è€…æƒ…å ±\n",
    "                'amazonæœ¬ä½“æœ‰ç„¡1', 'amazonæœ¬ä½“æœ‰ç„¡2', 'FBAæ•°', 'è‡ªå·±ç™ºé€æ•°', \n",
    "                'FBAæœ€å®‰å€¤å‡ºå“è€…æ•°', 'è‡ªå·±ç™ºé€æœ€å®‰å€¤å‡ºå“è€…æ•°', \n",
    "                'amazon_30æ—¥é–“åœ¨åº«åˆ‡ã‚Œç‡', 'amazon_90æ—¥é–“åœ¨åº«åˆ‡ã‚Œç‡',\n",
    "    \n",
    "                # è²©å£²æ•°æƒ…å ±\n",
    "                '30æ—¥é–“_ç·è²©å£²æ•°', '30æ—¥é–“_æ–°å“è²©å£²æ•°', '30æ—¥é–“_ä¸­å¤è²©å£²æ•°', \n",
    "                '30æ—¥é–“_ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼è²©å£²æ•°', 'Keepa30æ—¥é–“è²©å£²æ•°', \n",
    "                '90æ—¥é–“_ç·è²©å£²æ•°', '90æ—¥é–“_æ–°å“è²©å£²æ•°', '90æ—¥é–“_ä¸­å¤è²©å£²æ•°', \n",
    "                '90æ—¥é–“_ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼è²©å£²æ•°', 'Keepa90æ—¥é–“è²©å£²æ•°',\n",
    "                '180æ—¥é–“_ç·è²©å£²æ•°', '180æ—¥é–“_æ–°å“è²©å£²æ•°', '180æ—¥é–“_ä¸­å¤è²©å£²æ•°', \n",
    "                '180æ—¥é–“_ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼è²©å£²æ•°', 'Keepa180æ—¥é–“è²©å£²æ•°',\n",
    "    \n",
    "                # ä¾¡æ ¼å±¥æ­´\n",
    "                'amazonä¾¡æ ¼_ç¾åœ¨ä¾¡æ ¼', 'amazonä¾¡æ ¼_æœ€é«˜ä¾¡æ ¼', 'amazonä¾¡æ ¼_æœ€ä½ä¾¡æ ¼',\n",
    "                'amazonä¾¡æ ¼_30æ—¥å¹³å‡ä¾¡æ ¼', 'amazonä¾¡æ ¼_90æ—¥å¹³å‡ä¾¡æ ¼', \n",
    "                'amazonä¾¡æ ¼_180æ—¥å¹³å‡ä¾¡æ ¼', 'æ–°å“ä¾¡æ ¼_ç¾åœ¨ä¾¡æ ¼', 'æ–°å“ä¾¡æ ¼_æœ€é«˜ä¾¡æ ¼',\n",
    "                'æ–°å“ä¾¡æ ¼_æœ€ä½ä¾¡æ ¼', 'æ–°å“ä¾¡æ ¼_30æ—¥å¹³å‡ä¾¡æ ¼', 'æ–°å“ä¾¡æ ¼_90æ—¥å¹³å‡ä¾¡æ ¼',\n",
    "                'æ–°å“ä¾¡æ ¼_180æ—¥å¹³å‡ä¾¡æ ¼',\n",
    "    \n",
    "                # ãã®ä»–\n",
    "                'ç”»åƒURL', 'å…ƒã‚³ãƒ¼ãƒ‰', 'ã‚³ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—'\n",
    "            ]\n",
    "            \n",
    "            # å­˜åœ¨ã™ã‚‹åˆ—ã®ã¿ã‚’æŠ½å‡ºï¼ˆã‚¨ãƒ©ãƒ¼é˜²æ­¢ã®ãŸã‚ï¼‰\n",
    "            specified_columns = [col for col in column_order if col in df.columns]\n",
    "            \n",
    "            # æŒ‡å®šã•ã‚Œã¦ã„ãªã„æ®‹ã‚Šã®åˆ—ï¼ˆè¿½åŠ ã•ã‚ŒãŸåˆ—ãªã©ï¼‰ã‚’å–å¾—\n",
    "            remaining_columns = [col for col in df.columns if col not in column_order]\n",
    "            \n",
    "            # æŒ‡å®šåˆ— + æ®‹ã‚Šã®åˆ—ã®é †ã§æ–°ã—ã„åˆ—é †ã‚’ä½œæˆ\n",
    "            new_column_order = specified_columns + remaining_columns\n",
    "            \n",
    "            # ä¸¦ã³æ›¿ãˆã‚’å®Ÿè¡Œ\n",
    "            df = df[new_column_order]\n",
    "            \n",
    "            # çµæœã‚’ãƒ­ã‚°ã«è¨˜éŒ²\n",
    "            logging.info(f\"ã‚«ãƒ©ãƒ ã‚’ä¸¦ã³æ›¿ãˆã¾ã—ãŸ: æŒ‡å®šåˆ— {len(specified_columns)}åˆ— + è¿½åŠ åˆ— {len(remaining_columns)}åˆ—\")\n",
    "            print(f\"ğŸ“Š ã‚«ãƒ©ãƒ ä¸¦ã³æ›¿ãˆ: æŒ‡å®šåˆ— {len(specified_columns)}åˆ— + è¿½åŠ åˆ— {len(remaining_columns)}åˆ—\")\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"ã‚«ãƒ©ãƒ ä¸¦ã³æ›¿ãˆã‚¨ãƒ©ãƒ¼: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def save_data(self, df, output_file=None):\n",
    "        \"\"\"\n",
    "        çµ±åˆãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pandas.DataFrame\n",
    "            ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ \n",
    "        output_file : str, optional\n",
    "            å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ï¼‰\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—\n",
    "            if output_file is None:\n",
    "                output_file = self.config['data_integration']['output']['output_file']\n",
    "                \n",
    "            # ç›¸å¯¾ãƒ‘ã‚¹ã®å ´åˆã¯ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’åŸºæº–ã«ã™ã‚‹\n",
    "            if not os.path.isabs(output_file):\n",
    "                output_file = os.path.join(self.data_dir, output_file)\n",
    "                \n",
    "            # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèª\n",
    "            output_dir = os.path.dirname(output_file)\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "                \n",
    "            df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "            logging.info(f\"çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_file}\")\n",
    "            print(f\"âœ… {len(df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’ {output_file} ã«ä¿å­˜ã—ã¾ã—ãŸ\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}\")\n",
    "            raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea9d76a1-834f-4eb5-ac4c-a2e8398d0e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ç¾åœ¨ã®ã‚«ãƒ©ãƒ ç¢ºèªç”¨ã‚³ãƒ¼ãƒ‰\n",
    "# print(\"=== SP-APIã®ã‚«ãƒ©ãƒ  ===\")\n",
    "# sp_df = pd.read_csv('sp_api_output_filtered.csv', encoding='utf-8-sig')\n",
    "# print(sp_df.columns.tolist())\n",
    "\n",
    "# print(\"\\n=== Keepaã®ã‚«ãƒ©ãƒ  ===\")\n",
    "# keepa_df = pd.read_csv('keepa_output.csv', encoding='utf-8-sig')\n",
    "# print(keepa_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "698f9c6e-10ba-4693-b75b-1a63d86fe1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: C:\\Users\\inato\\Documents\\amazon-research\\config\\settings.yaml\n",
      "ğŸ“„ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: C:\\Users\\inato\\Documents\\amazon-research\\logs\\integration_20250407_141356.log\n",
      "ğŸ“‚ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: C:\\Users\\inato\\Documents\\amazon-research\n",
      "ğŸ“‚ ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: C:\\Users\\inato\\Documents\\amazon-research\\data\n",
      "ğŸ“‚ ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: C:\\Users\\inato\\Documents\\amazon-research\\logs\n",
      "\n",
      "ğŸ“„ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®æƒ…å ±:\n",
      "  - SP-APIå…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«: sp_api_output_filtered.csv\n",
      "  - Keepaå…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«: keepa_output.csv\n",
      "  - å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: integrated_data.csv\n",
      "\n",
      "ğŸ“„ è¿½åŠ ã‚½ãƒ¼ã‚¹æƒ…å ±:\n",
      "  ã‚½ãƒ¼ã‚¹1 (netsea): netsea_scraping.csv\n",
      "  ã‚½ãƒ¼ã‚¹2 (sudeli): sudeli_scraping.csv\n",
      "  ã‚½ãƒ¼ã‚¹3 (yahoo): yahoo_shopping_items.csv\n",
      "  ã‚½ãƒ¼ã‚¹4 (yoriyasu): yoriyasu_prices.csv\n",
      "ğŸ“Š SP-APIãƒ‡ãƒ¼ã‚¿: 99ä»¶\n",
      "ğŸ“Š Keepaãƒ‡ãƒ¼ã‚¿: 99ä»¶\n",
      "\n",
      "ğŸ“Š ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ (netsea)\n",
      "ğŸ“Š netsea_scraping.csvã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: 316ä»¶, ã‚­ãƒ¼åˆ—: JANã‚³ãƒ¼ãƒ‰\n",
      "ğŸ“Š netseaãƒ‡ãƒ¼ã‚¿ã‹ã‚‰'JANã‚³ãƒ¼ãƒ‰'åˆ—ã‚’'JAN'ã¨ã—ã¦çµåˆ: netsea_scraping.csv\n",
      "â„¹ï¸ netsea_scraping.csvå†…ã«16ä»¶ã®é‡è¤‡JANã‚’æ¤œå‡º: 4980901211667, 4529052003808, 4970883013632ãªã©\n",
      "â„¹ï¸ é‡è¤‡JANã¯å„JANã®æœ€åˆã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ç”¨ã—ã¾ã™\n",
      "â„¹ï¸ é‡è¤‡ã‚’é™¤å»: 316è¡Œ â†’ 300è¡Œ (16è¡Œå‰Šé™¤)\n",
      "âš ï¸ ãƒãƒƒãƒã™ã‚‹JANã‚³ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚çµåˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚\n",
      "âœ… netseaãƒ‡ãƒ¼ã‚¿ã®çµåˆå®Œäº†: ç¾åœ¨ 83åˆ—\n",
      "âœ… netseaãƒ‡ãƒ¼ã‚¿ã®çµåˆå®Œäº†: ç¾åœ¨ 83åˆ—\n",
      "\n",
      "ğŸ“Š ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ (sudeli)\n",
      "ğŸ“Š sudeli_scraping.csvã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: 2345ä»¶, ã‚­ãƒ¼åˆ—: JANã‚³ãƒ¼ãƒ‰\n",
      "ğŸ“Š sudeliãƒ‡ãƒ¼ã‚¿ã‹ã‚‰'JANã‚³ãƒ¼ãƒ‰'åˆ—ã‚’'JAN'ã¨ã—ã¦çµåˆ: sudeli_scraping.csv\n",
      "â„¹ï¸ sudeli_scraping.csvå†…ã«1026ä»¶ã®é‡è¤‡JANã‚’æ¤œå‡º: 4580663871668, 4580663871705, 4580663874454ãªã©\n",
      "â„¹ï¸ é‡è¤‡JANã¯å„JANã®æœ€åˆã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ç”¨ã—ã¾ã™\n",
      "â„¹ï¸ é‡è¤‡ã‚’é™¤å»: 2345è¡Œ â†’ 1299è¡Œ (1046è¡Œå‰Šé™¤)\n",
      "âš ï¸ ãƒãƒƒãƒã™ã‚‹JANã‚³ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚çµåˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚\n",
      "âœ… sudeliãƒ‡ãƒ¼ã‚¿ã®çµåˆå®Œäº†: ç¾åœ¨ 83åˆ—\n",
      "âœ… sudeliãƒ‡ãƒ¼ã‚¿ã®çµåˆå®Œäº†: ç¾åœ¨ 83åˆ—\n",
      "\n",
      "ğŸ“Š ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ (yahoo)\n",
      "ğŸ“Š yahoo_shopping_items.csvã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: 2998ä»¶, ã‚­ãƒ¼åˆ—: JAN\n",
      "ğŸ“Š yahooãƒ‡ãƒ¼ã‚¿ã‹ã‚‰'JAN'åˆ—ã‚’'JAN'ã¨ã—ã¦çµåˆ: yahoo_shopping_items.csv\n",
      "âš ï¸ ãƒãƒƒãƒã™ã‚‹JANã‚³ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚çµåˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚\n",
      "âœ… yahooãƒ‡ãƒ¼ã‚¿ã®çµåˆå®Œäº†: ç¾åœ¨ 83åˆ—\n",
      "âœ… yahooãƒ‡ãƒ¼ã‚¿ã®çµåˆå®Œäº†: ç¾åœ¨ 83åˆ—\n",
      "\n",
      "ğŸ“Š ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ (yoriyasu)\n",
      "ğŸ“Š yoriyasu_prices.csvã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: 1001ä»¶, ã‚­ãƒ¼åˆ—: JANã‚³ãƒ¼ãƒ‰\n",
      "ğŸ“Š yoriyasuãƒ‡ãƒ¼ã‚¿ã‹ã‚‰'JANã‚³ãƒ¼ãƒ‰'åˆ—ã‚’'JAN'ã¨ã—ã¦çµåˆ: yoriyasu_prices.csv\n",
      "âš ï¸ ãƒãƒƒãƒã™ã‚‹JANã‚³ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚çµåˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚\n",
      "âœ… yoriyasuãƒ‡ãƒ¼ã‚¿ã®çµåˆå®Œäº†: ç¾åœ¨ 83åˆ—\n",
      "âœ… yoriyasuãƒ‡ãƒ¼ã‚¿ã®çµåˆå®Œäº†: ç¾åœ¨ 83åˆ—\n",
      "ğŸ“Š ã‚«ãƒ©ãƒ ä¸¦ã³æ›¿ãˆ: æŒ‡å®šåˆ— 67åˆ— + è¿½åŠ åˆ— 16åˆ—\n",
      "âœ… 99ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’ C:\\Users\\inato\\Documents\\amazon-research\\data\\integrated_data.csv ã«ä¿å­˜ã—ã¾ã—ãŸ\n",
      "\n",
      "=== çµ±åˆå¾Œã®ã‚«ãƒ©ãƒ æ§‹æˆ ===\n",
      "\n",
      "å…¨ã‚«ãƒ©ãƒ æ•°: 83\n",
      "\n",
      "=== çµ±åˆçµæœã®æ¦‚è¦ ===\n",
      "ãƒ»ç·ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: 99ä»¶\n",
      "ãƒ»ã‚«ãƒ©ãƒ æ•°: 83åˆ—\n",
      "\n",
      "=== çµ±åˆçµæœã®ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€åˆã®3ä»¶ï¼‰===\n",
      "         ASIN              JAN ãƒ¡ãƒ¼ã‚«ãƒ¼å‹ç•ª   ãƒ¬ãƒ“ãƒ¥ãƒ¼æœ‰ç„¡  ç·å‡ºå“è€…æ•°  ã‚»ãƒƒãƒˆæ•°    å•†å“è¿½è·¡æ—¥  \\\n",
      "0  B000BNCBEE  4902508020268.0   2026  7502588      4     1  2195280   \n",
      "1  B000FNXVCQ  4902508040310.0   E031  7502704     19     1   230700   \n",
      "2  B000FQULEE  4987300505915.0    NaN  7502620      7     1   238200   \n",
      "\n",
      "        å•†å“ç™ºå£²æ—¥  è¿½è·¡é–‹å§‹ã‹ã‚‰ã®çµŒéæ—¥æ•°  ã‚¢ãƒ€ãƒ«ãƒˆå•†å“å¯¾è±¡   å‚è€ƒä¾¡æ ¼  ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸æœ€é•·è¾º  ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ä¸­è¾º  ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸æœ€çŸ­è¾º  \\\n",
      "0  19970512.0         3685     False  700.0      21.0      7.8       6.0   \n",
      "1         NaN         5050     False  500.0      35.2     10.8       4.8   \n",
      "2  20131010.0         5044     False    NaN      16.6      4.6       3.4   \n",
      "\n",
      "   ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸é‡é‡  ç¾åœ¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°  30æ—¥é–“å¹³å‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°  90æ—¥é–“å¹³å‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°  180æ—¥é–“å¹³å‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°  \\\n",
      "0    81.65    10070         8594         7450          7903   \n",
      "1    40.00     4435         5706         5869          4247   \n",
      "2   120.00    58137        68317        70813         54530   \n",
      "\n",
      "                                amazonURL  \\\n",
      "0  https://www.amazon.co.jp/dp/B000BNCBEE   \n",
      "1  https://www.amazon.co.jp/dp/B000FNXVCQ   \n",
      "2  https://www.amazon.co.jp/dp/B000FQULEE   \n",
      "\n",
      "                                   KeepaURL ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ASIN  Amazonä¾¡æ ¼   ã‚«ãƒ¼ãƒˆä¾¡æ ¼  \\\n",
      "0  https://keepa.com/#!product/5-B000BNCBEE         NaN       NaN     NaN   \n",
      "1  https://keepa.com/#!product/5-B000FNXVCQ         NaN       NaN     NaN   \n",
      "2  https://keepa.com/#!product/5-B000FQULEE         NaN       NaN  1320.0   \n",
      "\n",
      "   ã‚«ãƒ¼ãƒˆä¾¡æ ¼é€æ–™  ã‚«ãƒ¼ãƒˆä¾¡æ ¼ã®ãƒã‚¤ãƒ³ãƒˆ  FBAæœ€å®‰å€¤  FBAæœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ  è‡ªå·±ç™ºé€æœ€å®‰å€¤  è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®é€æ–™  \\\n",
      "0      NaN         NaN  2280.0          0.0   2100.0         0.0   \n",
      "1      NaN         NaN   700.0          0.0    780.0         1.0   \n",
      "2    550.0        13.0     NaN          NaN   1789.0         0.0   \n",
      "\n",
      "   è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ  FBAæ•°  è‡ªå·±ç™ºé€æ•°  FBAæœ€å®‰å€¤å‡ºå“è€…æ•°  è‡ªå·±ç™ºé€æœ€å®‰å€¤å‡ºå“è€…æ•°  amazon_30æ—¥é–“åœ¨åº«åˆ‡ã‚Œç‡  \\\n",
      "0           0.0     3      1           2            1               100   \n",
      "1           8.0     5     14           1            1               100   \n",
      "2           0.0     0      7           0            1               100   \n",
      "\n",
      "   amazon_90æ—¥é–“åœ¨åº«åˆ‡ã‚Œç‡  30æ—¥é–“_ç·è²©å£²æ•°  30æ—¥é–“_æ–°å“è²©å£²æ•°  30æ—¥é–“_ä¸­å¤è²©å£²æ•°  30æ—¥é–“_ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼è²©å£²æ•°  \\\n",
      "0               100         30          30           0              0   \n",
      "1               100         35          35           0              0   \n",
      "2               100         23          23           0              0   \n",
      "\n",
      "   Keepa30æ—¥é–“è²©å£²æ•°  90æ—¥é–“_ç·è²©å£²æ•°  90æ—¥é–“_æ–°å“è²©å£²æ•°  90æ—¥é–“_ä¸­å¤è²©å£²æ•°  90æ—¥é–“_ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼è²©å£²æ•°  \\\n",
      "0            21         99          99           0              0   \n",
      "1            35        117         117           0              0   \n",
      "2            16         59          59           0              0   \n",
      "\n",
      "   Keepa90æ—¥é–“è²©å£²æ•°  180æ—¥é–“_ç·è²©å£²æ•°  180æ—¥é–“_æ–°å“è²©å£²æ•°  180æ—¥é–“_ä¸­å¤è²©å£²æ•°  180æ—¥é–“_ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼è²©å£²æ•°  \\\n",
      "0            86         280          280            0               0   \n",
      "1           118         335          335            0               0   \n",
      "2            41         195          195            0               0   \n",
      "\n",
      "   Keepa180æ—¥é–“è²©å£²æ•°  amazonä¾¡æ ¼_ç¾åœ¨ä¾¡æ ¼  amazonä¾¡æ ¼_æœ€é«˜ä¾¡æ ¼  amazonä¾¡æ ¼_æœ€ä½ä¾¡æ ¼  \\\n",
      "0            253             -1          770.0          504.0   \n",
      "1            336             -1          550.0          338.0   \n",
      "2            155             -1         1175.0          882.0   \n",
      "\n",
      "   amazonä¾¡æ ¼_30æ—¥å¹³å‡ä¾¡æ ¼  amazonä¾¡æ ¼_90æ—¥å¹³å‡ä¾¡æ ¼  amazonä¾¡æ ¼_180æ—¥å¹³å‡ä¾¡æ ¼  æ–°å“ä¾¡æ ¼_ç¾åœ¨ä¾¡æ ¼  \\\n",
      "0                -1                -1                 -1       2100   \n",
      "1                -1                -1                522        488   \n",
      "2                -1                -1                 -1       1100   \n",
      "\n",
      "   æ–°å“ä¾¡æ ¼_æœ€é«˜ä¾¡æ ¼  æ–°å“ä¾¡æ ¼_æœ€ä½ä¾¡æ ¼  æ–°å“ä¾¡æ ¼_30æ—¥å¹³å‡ä¾¡æ ¼  æ–°å“ä¾¡æ ¼_90æ—¥å¹³å‡ä¾¡æ ¼  æ–°å“ä¾¡æ ¼_180æ—¥å¹³å‡ä¾¡æ ¼  \\\n",
      "0       3200         10          2100          2146           2300   \n",
      "1        927        320           488           488            487   \n",
      "2       1348        800          1100          1100           1065   \n",
      "\n",
      "                                               ç”»åƒURL        å…ƒã‚³ãƒ¼ãƒ‰ ã‚³ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—  \\\n",
      "0  https://images-na.ssl-images-amazon.com/images...  B000BNCBEE   ASIN   \n",
      "1  https://images-na.ssl-images-amazon.com/images...  B000FNXVCQ   ASIN   \n",
      "2  https://images-na.ssl-images-amazon.com/images...  B000FQULEE   ASIN   \n",
      "\n",
      "                            å•†å“å_sp              ã‚«ãƒ†ã‚´ãƒªãƒ¼_sp ãƒ¡ãƒ¼ã‚«ãƒ¼å_sp ãƒ–ãƒ©ãƒ³ãƒ‰å_sp  \\\n",
      "0                        ãƒ”ã‚¸ãƒ§ãƒ³ ãƒŸãƒ«ã‚«ãƒ¼           BABY_BOTTLE     ãƒ”ã‚¸ãƒ§ãƒ³     ãƒ”ã‚¸ãƒ§ãƒ³   \n",
      "1  ãƒ”ã‚¸ãƒ§ãƒ³ ãƒŠã‚¤ãƒ­ãƒ³ãƒ–ãƒ©ã‚· 2WAYã‚¿ã‚¤ãƒ— ã‚¬ãƒ©ã‚¹è£½å“ºä¹³ã³ã‚“å°‚ç”¨        CLEANING_BRUSH     ãƒ”ã‚¸ãƒ§ãƒ³     ãƒ”ã‚¸ãƒ§ãƒ³   \n",
      "2                 ãƒ©ã‚«ãƒ«ãƒˆ è–¬ç”¨ãƒ©ã‚«ãƒ«ãƒˆãƒ»ãƒ‹ãƒ¥ãƒ¼5  TOOTH_CLEANING_AGENT   ã‚¨ã‚¹ã‚¨ã‚¹è£½è–¬   ã‚¨ã‚¹ã‚¨ã‚¹è£½è–¬   \n",
      "\n",
      "   Amazonæœ¬ä½“æœ‰ç„¡1  æ–°å“ç·å‡ºå“è€…æ•°        ã‚«ãƒ¼ãƒˆã‚»ãƒ©ãƒ¼ID                        å•†å“å_keepa  \\\n",
      "0        False        4             NaN                        ãƒ”ã‚¸ãƒ§ãƒ³ ãƒŸãƒ«ã‚«ãƒ¼   \n",
      "1        False       19             NaN  ãƒ”ã‚¸ãƒ§ãƒ³ ãƒŠã‚¤ãƒ­ãƒ³ãƒ–ãƒ©ã‚· 2WAYã‚¿ã‚¤ãƒ— ã‚¬ãƒ©ã‚¹è£½å“ºä¹³ã³ã‚“å°‚ç”¨   \n",
      "2        False        7  A1GDEBS71ZMV2F                 ãƒ©ã‚«ãƒ«ãƒˆ è–¬ç”¨ãƒ©ã‚«ãƒ«ãƒˆãƒ»ãƒ‹ãƒ¥ãƒ¼5   \n",
      "\n",
      "   ã‚«ãƒ†ã‚´ãƒªãƒ¼_keepa ãƒ¡ãƒ¼ã‚«ãƒ¼å_keepa ãƒ–ãƒ©ãƒ³ãƒ‰å_keepa  amazonæœ¬ä½“æœ‰ç„¡  90æ—¥é–“_ç·è²©å£²æ•°.1  90æ—¥é–“_æ–°å“è²©å£²æ•°.1  \\\n",
      "0    344845011        ãƒ”ã‚¸ãƒ§ãƒ³        ãƒ”ã‚¸ãƒ§ãƒ³          -1           99            99   \n",
      "1    344845011        ãƒ”ã‚¸ãƒ§ãƒ³        ãƒ”ã‚¸ãƒ§ãƒ³          -1          117           117   \n",
      "2    160384011      ã‚¨ã‚¹ã‚¨ã‚¹è£½è–¬      ã‚¨ã‚¹ã‚¨ã‚¹è£½è–¬          -1           59            59   \n",
      "\n",
      "   90æ—¥é–“_ä¸­å¤è²©å£²æ•°.1  90æ—¥é–“_ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼è²©å£²æ•°.1  \n",
      "0             0                0  \n",
      "1             0                0  \n",
      "2             0                0  \n",
      "\n",
      "âœ¨ å‡¦ç†å®Œäº†ï¼ ãƒ‡ãƒ¼ã‚¿ã‚’ integrated_data.csv ã«ä¿å­˜ã—ã¾ã—ãŸ\n"
     ]
    }
   ],
   "source": [
    "# ã‚»ãƒ«3: å®Ÿè¡Œç”¨ã‚³ãƒ¼ãƒ‰\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # ã‚¤ãƒ³ãƒ†ã‚°ãƒ¬ãƒ¼ã‚¿ãƒ¼ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ\n",
    "        integrator = DataIntegrator()\n",
    "        \n",
    "        # å„ç¨®ãƒ‘ã‚¹ã®ç¢ºèªã¨è¡¨ç¤º\n",
    "        print(f\"ğŸ“‚ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {integrator.root_dir}\")\n",
    "        print(f\"ğŸ“‚ ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {integrator.data_dir}\")\n",
    "        print(f\"ğŸ“‚ ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {integrator.log_dir}\")\n",
    "        \n",
    "        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚“ã ãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¡¨ç¤º\n",
    "        config = integrator.config['data_integration']['output']\n",
    "        print(f\"\\nğŸ“„ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®æƒ…å ±:\")\n",
    "        print(f\"  - SP-APIå…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {config['sp_api_input']}\")\n",
    "        print(f\"  - Keepaå…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {config['keepa_input']}\")\n",
    "        print(f\"  - å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {config['output_file']}\")\n",
    "        \n",
    "        # è¿½åŠ ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’è¡¨ç¤º\n",
    "        sources = integrator.config['data_integration'].get('sources', [])\n",
    "        if sources:\n",
    "            print(\"\\nğŸ“„ è¿½åŠ ã‚½ãƒ¼ã‚¹æƒ…å ±:\")\n",
    "            for i, source in enumerate(sources, 1):\n",
    "                source_type = source.get('type', 'ä¸æ˜')\n",
    "                files = source.get('files', [])\n",
    "                print(f\"  ã‚½ãƒ¼ã‚¹{i} ({source_type}): {', '.join(files)}\")\n",
    "        \n",
    "        # SP-APIã¨Keepaãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\n",
    "        sp_df, keepa_df = integrator.load_data()\n",
    "        print(f\"ğŸ“Š SP-APIãƒ‡ãƒ¼ã‚¿: {len(sp_df)}ä»¶\")\n",
    "        print(f\"ğŸ“Š Keepaãƒ‡ãƒ¼ã‚¿: {len(keepa_df)}ä»¶\")\n",
    "        \n",
    "        # keepa_dfã‚’ä¿å­˜ï¼ˆJAN-ASINãƒãƒƒãƒ”ãƒ³ã‚°ç”¨ï¼‰\n",
    "        integrator.keepa_df = keepa_df\n",
    "        \n",
    "        # SP-APIã¨Keepaãƒ‡ãƒ¼ã‚¿ã®çµåˆ\n",
    "        merged_df = integrator.merge_data(sp_df, keepa_df)\n",
    "        \n",
    "        # è¿½åŠ ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®çµåˆ\n",
    "        for source_config in sources:\n",
    "            source_type = source_config.get('type', 'ä¸æ˜')\n",
    "            print(f\"\\nğŸ“Š ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ ({source_type})\")\n",
    "            \n",
    "            # ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\n",
    "            source_data = integrator.load_source_data(source_config)\n",
    "            \n",
    "            if source_data:\n",
    "                # ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®çµåˆ\n",
    "                merged_df = integrator.merge_source_data(merged_df, source_data, source_config)\n",
    "                print(f\"âœ… {source_type}ãƒ‡ãƒ¼ã‚¿ã®çµåˆå®Œäº†: ç¾åœ¨ {len(merged_df.columns)}åˆ—\")\n",
    "        \n",
    "        # ã‚«ãƒ©ãƒ ã®ä¸¦ã³æ›¿ãˆ\n",
    "        merged_df = integrator.rearrange_columns(merged_df)\n",
    "        \n",
    "        # çµ±åˆãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜\n",
    "        integrator.save_data(merged_df)\n",
    "        \n",
    "        # ã‚«ãƒ©ãƒ æ§‹æˆã®ç¢ºèª\n",
    "        print(\"\\n=== çµ±åˆå¾Œã®ã‚«ãƒ©ãƒ æ§‹æˆ ===\")\n",
    "        print(f\"\\nå…¨ã‚«ãƒ©ãƒ æ•°: {len(merged_df.columns)}\")\n",
    "        \n",
    "        # çµæœã®çµ±è¨ˆæƒ…å ±\n",
    "        print(\"\\n=== çµ±åˆçµæœã®æ¦‚è¦ ===\")\n",
    "        print(f\"ãƒ»ç·ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(merged_df)}ä»¶\")\n",
    "        print(f\"ãƒ»ã‚«ãƒ©ãƒ æ•°: {len(merged_df.columns)}åˆ—\")\n",
    "        \n",
    "        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º\n",
    "        print(\"\\n=== çµ±åˆçµæœã®ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€åˆã®3ä»¶ï¼‰===\")\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        print(merged_df.head(3))\n",
    "        \n",
    "        print(f\"\\nâœ¨ å‡¦ç†å®Œäº†ï¼ ãƒ‡ãƒ¼ã‚¿ã‚’ {config['output_file']} ã«ä¿å­˜ã—ã¾ã—ãŸ\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}\")\n",
    "        logging.error(f\"å®Ÿè¡Œæ™‚ã‚¨ãƒ©ãƒ¼: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60064145-ce12-4299-8d51-f0f1dd2ed14f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c33e6e0-c488-4477-9b8a-e04581da6274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79a4879-2a5f-4f70-bf5d-bba0b4e119fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
