import boto3
import pandas as pd
import io
import os
import json
import logging
import traceback
from datetime import datetime

# S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
s3 = boto3.client('s3')
BUCKET_NAME = os.environ.get("BUCKET_NAME", "your-bucket-name")

# ãƒ­ã‚¬ãƒ¼ã®è¨­å®š
logger = logging.getLogger()
logger.setLevel(logging.INFO)

def lambda_handler(event, context):
    """
    ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’è¡Œã†Lambdaé–¢æ•°
    data_filtered.pyã¨åŒã˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…
    
    event = {
        "input_file": "input/2025-06-03/chunk_001/calculated_data.csv",
        "chunk_path": "output/2025-06-03/chunk_001"
    }
    """
    
    try:
        # ã‚¤ãƒ™ãƒ³ãƒˆã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
        input_file = event.get("input_file")
        chunk_path = event.get("chunk_path", "").rstrip('/')
        
        if not input_file or not chunk_path:
            logger.error("å¿…è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
            return {
                "statusCode": 400,
                "message": "input_file ã¾ãŸã¯ chunk_path ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“"
            }
        
        logger.info(f"ğŸš€ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é–‹å§‹:")
        logger.info(f"  - å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {input_file}")
        logger.info(f"  - å‡ºåŠ›ãƒ‘ã‚¹: {chunk_path}")
        
        # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        df = read_csv_from_s3(input_file)
        logger.info(f"ğŸ“Š å…¥åŠ›ãƒ‡ãƒ¼ã‚¿: {len(df)}è¡Œ, {len(df.columns)}åˆ—")
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è¨­å®šã‚’èª­ã¿è¾¼ã¿
        filter_config = load_filter_config()
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¨ã‚½ãƒ¼ãƒˆå‡¦ç†ã‚’å®Ÿè¡Œ
        filtered_df = filter_and_sort_data(df, filter_config)
        
        if filtered_df is None or len(filtered_df) == 0:
            logger.warning("âš ï¸ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã§ã™")
            # ç©ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
            filtered_df = pd.DataFrame()
        
        # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®è¨­å®š
        output_file = f"{chunk_path}/filtered_data.csv"
        
        # çµæœã‚’S3ã«ä¿å­˜
        save_dataframe_to_s3(filtered_df, output_file)
        
        # çµ±è¨ˆæƒ…å ±ã®è¨ˆç®—
        original_count = len(df)
        filtered_count = len(filtered_df)
        excluded_count = original_count - filtered_count
        
        logger.info(f"âœ… ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å®Œäº†: {output_file}")
        
        return {
            "statusCode": 200,
            "message": "ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãŒå®Œäº†ã—ã¾ã—ãŸ",
            "input_file": input_file,
            "output_file": output_file,
            "original_records": int(original_count),
            "filtered_records": int(filtered_count),
            "excluded_records": int(excluded_count),
            "filter_rate": round((filtered_count / original_count * 100), 2) if original_count > 0 else 0,
            "copy_task": {
                "src_key": output_file,
                "dst_key": output_file.replace("output/", "input/")
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}", exc_info=True)
        return {
            "statusCode": 500,
            "message": f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
            "error": str(e),
            "copy_task": {
                "src_key": "",
                "dst_key": ""
            }
        }

def read_csv_from_s3(file_key):
    """S3ã‹ã‚‰CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    try:
        response = s3.get_object(Bucket=BUCKET_NAME, Key=file_key)
        content = response["Body"].read()
        # JANã‚³ãƒ¼ãƒ‰ã‚’æ–‡å­—åˆ—ã¨ã—ã¦èª­ã¿è¾¼ã‚€
        df = pd.read_csv(io.BytesIO(content), encoding="utf-8-sig", dtype={'JAN': str})
        
        # JANã‚³ãƒ¼ãƒ‰ã®å½¢å¼ä¿®æ­£
        if 'JAN' in df.columns:
            df['JAN'] = df['JAN'].astype(str).str.replace('.0$', '', regex=True)
            
        return df
    except Exception as e:
        logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {file_key}: {str(e)}")
        raise

def save_dataframe_to_s3(df, file_key):
    """DataFrameã‚’S3ã«CSVã¨ã—ã¦ä¿å­˜"""
    try:
        csv_buffer = io.StringIO()
        df.to_csv(csv_buffer, index=False, encoding='utf-8-sig')
        
        s3.put_object(
            Bucket=BUCKET_NAME,
            Key=file_key,
            Body=csv_buffer.getvalue().encode('utf-8-sig'),
            ContentType='text/csv'
        )
        logger.info(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {file_key}")
    except Exception as e:
        logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼ {file_key}: {str(e)}")
        raise

def load_filter_config():
    """
    S3ã‹ã‚‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è¨­å®šã‚’èª­ã¿è¾¼ã‚€
    è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨
    """
    try:
        # S3ã‹ã‚‰è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        response = s3.get_object(Bucket=BUCKET_NAME, Key='config/settings.yaml')
        content = response["Body"].read().decode('utf-8')
        
        import yaml
        config = yaml.safe_load(content)
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¡ä»¶ã‚’å–å¾—
        filter_conditions = config.get('filter_conditions', {})
        
        logger.info(f"ğŸ“‹ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è¨­å®šã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        return filter_conditions
        
    except Exception as e:
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒèª­ã¿è¾¼ã‚ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨
        logger.warning(f"âš ï¸ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨: {str(e)}")
        return {
            'profit_min': 100,
            'profit_rate_min': 0,
            'profit_rate_max': 40
        }

def filter_and_sort_data(df, filter_config):
    """
    ãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¨ä¸¦ã³æ›¿ãˆã‚’è¡Œã†ï¼ˆdata_filtered.pyã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
    
    Args:
        df (pandas.DataFrame): å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        filter_config (dict): ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è¨­å®š
    
    Returns:
        pandas.DataFrame: å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    """
    try:
        logger.info(f"ğŸ“ˆ å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(df)}ä»¶")
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¡ä»¶ã‚’å–å¾—
        profit_min = filter_config.get('profit_min', 100)
        profit_rate_min = filter_config.get('profit_rate_min', 0)
        profit_rate_max = filter_config.get('profit_rate_max', 40)
        
        logger.info(f"ğŸ” ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¡ä»¶:")
        logger.info(f"  - åˆ©ç›Šé¡ {profit_min}å††ä»¥ä¸Š")
        logger.info(f"  - åˆ©ç›Šç‡ {profit_rate_min}%ä»¥ä¸Š{profit_rate_max}%ä»¥ä¸‹")
        
        # åˆ©ç›Šé¡ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆNULLå€¤ã¯æ¡ä»¶ã‚’æº€ãŸã•ãªã„ã¨ã¿ãªã™ï¼‰
        if 'æ‰‹æ•°æ–™ãƒ»åˆ©ç›Š_åˆ©ç›Šé¡' in df.columns:
            profit_mask = (~df['æ‰‹æ•°æ–™ãƒ»åˆ©ç›Š_åˆ©ç›Šé¡'].isna()) & (df['æ‰‹æ•°æ–™ãƒ»åˆ©ç›Š_åˆ©ç›Šé¡'] >= profit_min)
            logger.info(f"ğŸ“Š åˆ©ç›Šé¡æ¡ä»¶é©ç”¨: {profit_mask.sum()}/{len(df)}ä»¶ãŒæ¡ä»¶ã‚’æº€ãŸã™")
        else:
            logger.warning("âš ï¸ 'æ‰‹æ•°æ–™ãƒ»åˆ©ç›Š_åˆ©ç›Šé¡'åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            profit_mask = pd.Series(True, index=df.index)
        
        # åˆ©ç›Šç‡ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆNULLå€¤ã¯æ¡ä»¶ã‚’æº€ãŸã•ãªã„ã¨ã¿ãªã™ï¼‰
        if 'æ‰‹æ•°æ–™ãƒ»åˆ©ç›Š_åˆ©ç›Šç‡' in df.columns:
            # %è¨˜å·ãŒã‚ã‚Œã°å‰Šé™¤ã—ã¦æ•°å€¤ã«å¤‰æ›
            rate_column = df['æ‰‹æ•°æ–™ãƒ»åˆ©ç›Š_åˆ©ç›Šç‡'].copy()
            if rate_column.dtype == 'object':  # æ–‡å­—åˆ—å‹ã®å ´åˆ
                # éNaNå€¤ã®ã¿å‡¦ç†
                mask = ~rate_column.isna()
                rate_column.loc[mask] = rate_column.loc[mask].astype(str).str.replace('%', '').astype(float)
                
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¡ä»¶ã‚’æ§‹ç¯‰ï¼ˆNULLå€¤ã¯æ¡ä»¶ã‚’æº€ãŸã•ãªã„ã¨ã¿ãªã™ï¼‰
            rate_mask = (~rate_column.isna()) & ((rate_column >= profit_rate_min) & (rate_column <= profit_rate_max))
            logger.info(f"ğŸ“Š åˆ©ç›Šç‡æ¡ä»¶é©ç”¨: {rate_mask.sum()}/{len(df)}ä»¶ãŒæ¡ä»¶ã‚’æº€ãŸã™")
        else:
            logger.warning("âš ï¸ 'æ‰‹æ•°æ–™ãƒ»åˆ©ç›Š_åˆ©ç›Šç‡'åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            rate_mask = pd.Series(True, index=df.index)
        
        # ä¸¡æ–¹ã®æ¡ä»¶ã‚’çµ„ã¿åˆã‚ã›ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        combined_mask = profit_mask & rate_mask
        filtered_df = df[combined_mask].copy()
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®ã‚µã‚¤ã‚º
        filtered_size = len(filtered_df)
        excluded_size = len(df) - filtered_size
        logger.info(f"ğŸ“Š ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {filtered_size}ä»¶ (é™¤å¤–: {excluded_size}ä»¶)")
        
        # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã§ä¸¦ã³æ›¿ãˆ
        logger.info("ğŸ”¢ ã€Œç¾åœ¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã€ã§æ˜‡é †ã«ä¸¦ã³æ›¿ãˆä¸­...")
        if 'ç¾åœ¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°' in filtered_df.columns:
            filtered_df = filtered_df.sort_values('ç¾åœ¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°', 
                                               ascending=True, 
                                               na_position='last').reset_index(drop=True)
            logger.info("âœ… ã‚½ãƒ¼ãƒˆå®Œäº†")
        else:
            logger.warning("âš ï¸ 'ç¾åœ¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°'åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚½ãƒ¼ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            
        # ğŸ”§ data_filtered.pyã¨åŒã˜Excelå¯¾å¿œãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå‡¦ç†
        filtered_df = apply_excel_formatting(filtered_df)
        
        return filtered_df
        
    except Exception as e:
        logger.error(f"âŒ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
        logger.error(traceback.format_exc())
        return None

def apply_excel_formatting(df):
    """
    Exceläº’æ›ã®æ•°å€¤ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå‡¦ç†ã‚’é©ç”¨
    data_filtered.pyã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯
    """
    try:
        # JANåˆ—ã‚’ ="" å½¢å¼ã«å¤‰æ›ã—ã€å°æ•°ç‚¹.0ã‚’é™¤ã
        if 'JAN' in df.columns:
            df['JAN'] = df['JAN'].apply(
                lambda x: f'="{str(x).split(".")[0]}"' if pd.notna(x) else ''
            )
            logger.info("âœ… JANåˆ—ã®Excelå½¢å¼å¤‰æ›å®Œäº†")

        # ã‚«ãƒ†ã‚´ãƒªãƒ¼IDåˆ—ã‚’ ="" å½¢å¼ã«å¤‰æ›ã—ã€å°æ•°ç‚¹.0ã‚’é™¤ã
        if 'ã‚«ãƒ†ã‚´ãƒªãƒ¼ID' in df.columns:
            df['ã‚«ãƒ†ã‚´ãƒªãƒ¼ID'] = df['ã‚«ãƒ†ã‚´ãƒªãƒ¼ID'].apply(
                lambda x: f'="{str(x).split(".")[0]}"' if pd.notna(x) else ''
            )
            logger.info("âœ… ã‚«ãƒ†ã‚´ãƒªãƒ¼IDåˆ—ã®Excelå½¢å¼å¤‰æ›å®Œäº†")
        elif 'ã‚«ãƒ†ã‚´ãƒªãƒ¼' in df.columns:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼š'ã‚«ãƒ†ã‚´ãƒªãƒ¼'åˆ—ãŒã‚ã‚‹å ´åˆ
            df['ã‚«ãƒ†ã‚´ãƒªãƒ¼'] = df['ã‚«ãƒ†ã‚´ãƒªãƒ¼'].apply(
                lambda x: f'="{str(x).split(".")[0]}"' if pd.notna(x) else ''
            )
            logger.info("âœ… ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ—ã®Excelå½¢å¼å¤‰æ›å®Œäº†")

        # ãƒ¡ãƒ¼ã‚«ãƒ¼å‹ç•ªåˆ—ã‚’ ="" å½¢å¼ã«å¤‰æ›ï¼ˆNaNå¯¾ç­–ã¤ãï¼‰
        if 'ãƒ¡ãƒ¼ã‚«ãƒ¼å‹ç•ª' in df.columns:
            df['ãƒ¡ãƒ¼ã‚«ãƒ¼å‹ç•ª'] = df['ãƒ¡ãƒ¼ã‚«ãƒ¼å‹ç•ª'].apply(
                lambda x: f'="{str(x).split(".")[0]}"' if pd.notna(x) else ''
            )
            logger.info("âœ… ãƒ¡ãƒ¼ã‚«ãƒ¼å‹ç•ªåˆ—ã®Excelå½¢å¼å¤‰æ›å®Œäº†")
        
        logger.info("âœ… Exceläº’æ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå‡¦ç†å®Œäº†")
        return df
        
    except Exception as e:
        logger.error(f"âŒ Excelå½¢å¼å¤‰æ›ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return df