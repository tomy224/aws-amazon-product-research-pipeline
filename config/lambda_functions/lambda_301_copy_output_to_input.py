import boto3
import os
import logging
import json

# ãƒ­ã‚¬ãƒ¼ã®è¨­å®š
logger = logging.getLogger()
logger.setLevel(logging.INFO)

s3 = boto3.client("s3")

BUCKET_NAME = os.environ.get("BUCKET_NAME", "your-bucket-name")

def lambda_handler(event, context):
    """
    S3ãƒ•ã‚¡ã‚¤ãƒ«ã‚’outputã‹ã‚‰inputã«ã‚³ãƒ”ãƒ¼ã™ã‚‹Lambdaé–¢æ•°
    
    ğŸ” è©³ç´°ãƒ‡ãƒãƒƒã‚°ç‰ˆï¼š
    - å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°ãƒ­ã‚°
    - S3ãƒã‚±ãƒƒãƒˆåã®ç¢ºèª
    - ãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°å­˜åœ¨ãƒã‚§ãƒƒã‚¯
    - ã‚¨ãƒ©ãƒ¼ã®è©³ç´°åˆ†æ
    """
    
    # ğŸ” ãƒ‡ãƒãƒƒã‚°ï¼šå…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’è©³ç´°ã«ãƒ­ã‚°å‡ºåŠ›
    logger.info("=" * 60)
    logger.info("ğŸš€ Copy Lambdaé–‹å§‹")
    logger.info(f"ğŸ—‚ï¸ ãƒã‚±ãƒƒãƒˆå: {BUCKET_NAME}")
    logger.info(f"ğŸ“¥ å—ä¿¡ã‚¤ãƒ™ãƒ³ãƒˆ: {json.dumps(event, indent=2, ensure_ascii=False)}")
    logger.info("=" * 60)
    
    files = event.get("files", [])
    if not files:
        logger.error("âŒ ã‚³ãƒ”ãƒ¼å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
        raise ValueError("ã‚³ãƒ”ãƒ¼å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")

    results = []
    errors = []

    logger.info(f"ğŸ“Š å‡¦ç†é–‹å§‹: {len(files)}ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†")

    for i, f in enumerate(files):
        src_key = f.get("src_key")
        dst_key = f.get("dst_key")

        logger.info(f"\nğŸ“ ãƒ•ã‚¡ã‚¤ãƒ« {i+1}/{len(files)}:")
        logger.info(f"  ğŸ“¤ ã‚³ãƒ”ãƒ¼å…ƒ: '{src_key}'")
        logger.info(f"  ğŸ“¥ ã‚³ãƒ”ãƒ¼å…ˆ: '{dst_key}'")
        
        # ğŸ” åŸºæœ¬çš„ãªå…¥åŠ›ãƒã‚§ãƒƒã‚¯
        if not src_key or not dst_key:
            error_msg = f"âŒ src_keyã¾ãŸã¯dst_keyãŒç©ºã§ã™"
            logger.error(f"  {error_msg}")
            errors.append({
                "file_index": i+1, 
                "error": error_msg, 
                "src_key": src_key, 
                "dst_key": dst_key
            })
            continue

        # ğŸ” ç©ºæ–‡å­—åˆ—ãƒã‚§ãƒƒã‚¯
        if src_key.strip() == "" or dst_key.strip() == "":
            error_msg = f"âŒ src_keyã¾ãŸã¯dst_keyãŒç©ºæ–‡å­—åˆ—ã§ã™"
            logger.error(f"  {error_msg}")
            errors.append({
                "file_index": i+1, 
                "error": error_msg, 
                "src_key": src_key, 
                "dst_key": dst_key
            })
            continue

        try:
            # ğŸ” ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°å­˜åœ¨ç¢ºèª
            logger.info(f"  ğŸ” ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèªä¸­...")
            logger.info(f"      ãƒã‚±ãƒƒãƒˆ: '{BUCKET_NAME}'")
            logger.info(f"      ã‚­ãƒ¼: '{src_key}'")
            
            try:
                # ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèª
                response = s3.head_object(Bucket=BUCKET_NAME, Key=src_key)
                file_size = response.get('ContentLength', 0)
                last_modified = response.get('LastModified', 'Unknown')
                content_type = response.get('ContentType', 'Unknown')
                
                logger.info(f"  âœ… ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèªOK:")
                logger.info(f"      ã‚µã‚¤ã‚º: {file_size:,} bytes")
                logger.info(f"      æ›´æ–°æ—¥æ™‚: {last_modified}")
                logger.info(f"      Content-Type: {content_type}")
                
                # ğŸ” ç©ºãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒã‚§ãƒƒã‚¯
                if file_size == 0:
                    logger.warning(f"  âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ã¯ç©ºã§ã™ (0 bytes)")
                    errors.append({
                        "file_index": i+1, 
                        "error": "ç©ºãƒ•ã‚¡ã‚¤ãƒ«", 
                        "src_key": src_key, 
                        "dst_key": dst_key,
                        "file_size": file_size
                    })
                    continue
                    
            except s3.exceptions.NoSuchKey as e:
                error_msg = f"âŒ ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
                logger.error(f"  {error_msg}")
                logger.error(f"  ğŸ“‹ NoSuchKeyè©³ç´°: {str(e)}")
                
                # ğŸ” ãƒã‚±ãƒƒãƒˆå†…ã®é¡ä¼¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã—ã¦ã¿ã‚‹
                try:
                    # ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—
                    folder_path = '/'.join(src_key.split('/')[:-1]) + '/'
                    logger.info(f"  ğŸ—‚ï¸ ãƒ•ã‚©ãƒ«ãƒ€ '{folder_path}' ã®å†…å®¹ç¢ºèªä¸­...")
                    
                    paginator = s3.get_paginator('list_objects_v2')
                    pages = paginator.paginate(Bucket=BUCKET_NAME, Prefix=folder_path)
                    
                    found_files = []
                    for page in pages:
                        if 'Contents' in page:
                            for obj in page['Contents']:
                                found_files.append(obj['Key'])
                    
                    logger.info(f"  ğŸ“‹ ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ãƒ•ã‚¡ã‚¤ãƒ« ({len(found_files)}ä»¶):")
                    for file_key in found_files[:10]:  # æœ€å¤§10ä»¶è¡¨ç¤º
                        logger.info(f"      - {file_key}")
                    
                    if len(found_files) > 10:
                        logger.info(f"      ... ä»– {len(found_files) - 10} ä»¶")
                        
                except Exception as list_error:
                    logger.error(f"  âŒ ãƒ•ã‚©ãƒ«ãƒ€å†…å®¹ç¢ºèªã‚¨ãƒ©ãƒ¼: {str(list_error)}")
                
                errors.append({
                    "file_index": i+1, 
                    "error": error_msg, 
                    "src_key": src_key, 
                    "dst_key": dst_key,
                    "details": str(e)
                })
                continue
                
            except Exception as e:
                error_msg = f"âŒ ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèªã‚¨ãƒ©ãƒ¼: {str(e)}"
                logger.error(f"  {error_msg}")
                errors.append({
                    "file_index": i+1, 
                    "error": error_msg, 
                    "src_key": src_key, 
                    "dst_key": dst_key,
                    "details": str(e)
                })
                continue

            # ğŸ” ã‚³ãƒ”ãƒ¼å‡¦ç†ã®è©³ç´°å®Ÿè¡Œ
            logger.info(f"  ğŸ”„ ã‚³ãƒ”ãƒ¼å®Ÿè¡Œä¸­...")
            
            copy_source = {"Bucket": BUCKET_NAME, "Key": src_key}
            logger.info(f"      CopySource: {copy_source}")
            logger.info(f"      Destination Bucket: '{BUCKET_NAME}'")
            logger.info(f"      Destination Key: '{dst_key}'")
            
            s3.copy_object(
                Bucket=BUCKET_NAME,
                CopySource=copy_source,
                Key=dst_key
            )

            # ğŸ” ã‚³ãƒ”ãƒ¼å¾Œã®ç¢ºèª
            try:
                copy_response = s3.head_object(Bucket=BUCKET_NAME, Key=dst_key)
                copy_size = copy_response.get('ContentLength', 0)
                logger.info(f"  âœ… ã‚³ãƒ”ãƒ¼æˆåŠŸ:")
                logger.info(f"      ã‚³ãƒ”ãƒ¼å…ˆã‚µã‚¤ã‚º: {copy_size:,} bytes")
                
                results.append({
                    "from": src_key, 
                    "to": dst_key, 
                    "size": copy_size,
                    "status": "success"
                })
                
            except Exception as e:
                error_msg = f"âŒ ã‚³ãƒ”ãƒ¼å¾Œç¢ºèªã‚¨ãƒ©ãƒ¼: {str(e)}"
                logger.error(f"  {error_msg}")
                errors.append({
                    "file_index": i+1, 
                    "error": error_msg, 
                    "src_key": src_key, 
                    "dst_key": dst_key
                })

        except Exception as e:
            error_msg = f"âŒ ã‚³ãƒ”ãƒ¼å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}"
            logger.error(f"  {error_msg}")
            logger.error(f"  ğŸ“‹ ã‚¨ãƒ©ãƒ¼è©³ç´°: {type(e).__name__}")
            
            # ğŸ” ç‰¹åˆ¥ãªã‚¨ãƒ©ãƒ¼å‡¦ç†
            if "NoSuchKey" in str(e):
                logger.error(f"  ğŸ¯ NoSuchKeyã‚¨ãƒ©ãƒ¼ã®è©³ç´°åˆ†æ:")
                logger.error(f"      - ãƒã‚±ãƒƒãƒˆå: '{BUCKET_NAME}'")
                logger.error(f"      - ãƒ•ã‚¡ã‚¤ãƒ«ã‚­ãƒ¼: '{src_key}'")
                logger.error(f"      - ã‚­ãƒ¼ã®é•·ã•: {len(src_key)} æ–‡å­—")
                logger.error(f"      - åˆ¶å¾¡æ–‡å­—ãƒã‚§ãƒƒã‚¯: {repr(src_key)}")
            
            errors.append({
                "file_index": i+1, 
                "error": error_msg, 
                "src_key": src_key, 
                "dst_key": dst_key,
                "error_type": type(e).__name__
            })

    # ğŸ” æœ€çµ‚çµæœã‚µãƒãƒªãƒ¼
    success_count = len(results)
    error_count = len(errors)
    total_count = len(files)
    
    logger.info("=" * 60)
    logger.info(f"ğŸ å‡¦ç†å®Œäº†ã‚µãƒãƒªãƒ¼:")
    logger.info(f"  ğŸ“Š åˆè¨ˆ: {total_count}ä»¶")
    logger.info(f"  âœ… æˆåŠŸ: {success_count}ä»¶")
    logger.info(f"  âŒ ã‚¨ãƒ©ãƒ¼: {error_count}ä»¶")
    
    # ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚‹å ´åˆã¯è©³ç´°ã‚’å‡ºåŠ›
    if errors:
        logger.error("âŒ ã‚¨ãƒ©ãƒ¼è©³ç´°:")
        for error in errors:
            logger.error(f"  ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«{error['file_index']}: {error['error']}")
            logger.error(f"      src: '{error['src_key']}'")
            logger.error(f"      dst: '{error['dst_key']}'")
            if 'details' in error:
                logger.error(f"      è©³ç´°: {error['details']}")
    
    logger.info("=" * 60)

    return {
        "message": f"{success_count}/{total_count} ä»¶ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ (ã‚¨ãƒ©ãƒ¼: {error_count}ä»¶)",
        "copied": results,
        "errors": errors,
        "summary": {
            "total": total_count,
            "success": success_count,
            "error": error_count
        },
        "debug_info": {
            "bucket_name": BUCKET_NAME,
            "execution_time": context.get_remaining_time_in_millis() if context else "N/A"
        }
    }