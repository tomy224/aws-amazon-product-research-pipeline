import boto3
import pandas as pd
import io
import os
import logging
import re
from datetime import datetime

# S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
s3 = boto3.client('s3')
BUCKET_NAME = os.environ.get("BUCKET_NAME", "your-bucket-name")

# ãƒ­ã‚¬ãƒ¼ã®è¨­å®š
logger = logging.getLogger()
logger.setLevel(logging.INFO)

def lambda_handler(event, context):
    """
    4ã¤ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ±åˆã™ã‚‹æœ€çµ‚å‡¦ç†ï¼ˆoutputå‡ºåŠ›ç‰ˆï¼‰
    
    ğŸ”§ ä¿®æ­£ç‚¹ï¼š
    - output ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜
    - copy_task ã‚’è¿”ã™ã‚ˆã†ã«å¤‰æ›´
    - input ãƒ•ã‚©ãƒ«ãƒ€ã«ã¯ä¿å­˜ã—ãªã„
    
    event = {
        "keepa_file": "output/2025-05-26/chunk_001/merged_keepa_results_filtered.csv",
        "pricing_file": "output/2025-05-26/chunk_001/merged_pricing_filtered.csv", 
        "yahoraku_file": "output/2025-05-26/chunk_001/merged_yahoraku.csv",
        "chunk_path": "output/2025-05-26/chunk_001"
    }
    """
    
    try:
        # ã‚¤ãƒ™ãƒ³ãƒˆã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—
        keepa_file = event.get("keepa_file")
        pricing_file = event.get("pricing_file")
        yahoraku_file = event.get("yahoraku_file")
        chunk_path = event.get("chunk_path", "").rstrip('/')
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¤œè¨¼
        if not all([keepa_file, pricing_file, yahoraku_file, chunk_path]):
            logger.error("å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
            return {
                "statusCode": 400,
                "message": "å¿…è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™"
            }
        
        logger.info(f"ğŸš€ Final Integration é–‹å§‹:")
        logger.info(f"  - Keepa: {keepa_file}")
        logger.info(f"  - Pricing: {pricing_file}")
        logger.info(f"  - YahoRaku: {yahoraku_file}")
        logger.info(f"  - Chunk Path: {chunk_path}")
        
        # å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        # 1. Keepaãƒ‡ãƒ¼ã‚¿ï¼ˆåŸºæº–ã¨ãªã‚‹ãƒ‡ãƒ¼ã‚¿ï¼‰
        keepa_df = read_csv_from_s3(keepa_file)
        logger.info(f"ğŸ“Š Keepaãƒ‡ãƒ¼ã‚¿: {len(keepa_df)}è¡Œ, {len(keepa_df.columns)}åˆ—")
        
        # 2. Pricingãƒ‡ãƒ¼ã‚¿
        pricing_df = read_csv_from_s3(pricing_file)
        logger.info(f"ğŸ“Š Pricingãƒ‡ãƒ¼ã‚¿: {len(pricing_df)}è¡Œ, {len(pricing_df.columns)}åˆ—")
        
        # 3. YahoRakuãƒ‡ãƒ¼ã‚¿
        yahoraku_df = read_csv_from_s3(yahoraku_file)
        logger.info(f"ğŸ“Š YahoRakuãƒ‡ãƒ¼ã‚¿: {len(yahoraku_df)}è¡Œ, {len(yahoraku_df.columns)}åˆ—")
        
        # data_integrator.pyã¨åŒã˜é †åºã§çµåˆ
        # ã‚¹ãƒ†ãƒƒãƒ—1: Keepaã¨Pricingã‚’çµåˆï¼ˆASINã‚­ãƒ¼ï¼‰
        merged_df = merge_keepa_pricing(keepa_df, pricing_df)
        logger.info(f"âœ… Keepa + Pricing: {len(merged_df)}è¡Œ, {len(merged_df.columns)}åˆ—")
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: YahoRakuãƒ‡ãƒ¼ã‚¿ã‚’ASINã§æ¨ªå±•é–‹çµåˆ
        merged_df = merge_yahoraku_data_by_asin(merged_df, yahoraku_df)
        logger.info(f"âœ… + YahoRaku: {len(merged_df)}è¡Œ, {len(merged_df.columns)}åˆ—")
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: data_integrator.pyã¨åŒã˜åˆ—é †åºã«ä¸¦ã³æ›¿ãˆ
        merged_df = rearrange_columns_like_integrator(merged_df)
        
        # ã‚¹ãƒ†ãƒƒãƒ—4: JANã‚³ãƒ¼ãƒ‰ã®å½¢å¼ã‚’çµ±ä¸€
        merged_df = fix_jan_format(merged_df)
        
        # ğŸ”§ ä¿®æ­£ï¼šå‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’outputãƒ•ã‚©ãƒ«ãƒ€ã«è¨­å®š
        output_key = f"{chunk_path}/final_integrated_data.csv"
        
        logger.info(f"ğŸ“ å‡ºåŠ›å…ˆ: {output_key}")
        
        # CSVã¨ã—ã¦ä¿å­˜
        csv_buffer = io.StringIO()
        merged_df.to_csv(csv_buffer, index=False, encoding='utf-8-sig')
        
        # ğŸ”§ ä¿®æ­£ï¼šoutputãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜
        s3.put_object(
            Bucket=BUCKET_NAME,
            Key=output_key,
            Body=csv_buffer.getvalue().encode('utf-8-sig'),
            ContentType='text/csv'
        )
        
        # çµ±è¨ˆæƒ…å ±ã®è¨ˆç®—
        jan_count = merged_df['JAN'].notna().sum() if 'JAN' in merged_df.columns else 0
        
        logger.info(f"âœ… çµ±åˆå®Œäº†: {output_key}")
        logger.info(f"ğŸ“Š çµæœ: {len(merged_df)}è¡Œ, {len(merged_df.columns)}åˆ—")
        
        # ğŸ”§ ä¿®æ­£ï¼šcopy_taskã‚’è¿”ã™ã‚ˆã†ã«å¤‰æ›´
        return {
            "statusCode": 200,
            "message": "ãƒ‡ãƒ¼ã‚¿çµ±åˆãŒå®Œäº†ã—ã¾ã—ãŸ",
            "output_file": output_key,
            "total_records": int(len(merged_df)),
            "total_columns": len(merged_df.columns),
            "jan_records": int(jan_count),
            "files_integrated": {
                "keepa": keepa_file,
                "pricing": pricing_file,
                "yahoraku": yahoraku_file
            },
            "copy_task": {
                "src_key": output_key,
                "dst_key": output_key.replace("output/", "input/")
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ çµ±åˆå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}", exc_info=True)
        return {
            "statusCode": 500,
            "message": f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
            "error": str(e),
            "copy_task": {
                "src_key": "",
                "dst_key": ""
            }
        }

def read_csv_from_s3(file_key):
    """
    S3ã‹ã‚‰CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€ï¼ˆJANã‚’æ–‡å­—åˆ—ã¨ã—ã¦å‡¦ç†ï¼‰
    """
    try:
        response = s3.get_object(Bucket=BUCKET_NAME, Key=file_key)
        content = response["Body"].read()
        # JANã‚³ãƒ¼ãƒ‰ã‚’æ–‡å­—åˆ—ã¨ã—ã¦èª­ã¿è¾¼ã‚€ï¼ˆé‡è¦ï¼ï¼‰
        df = pd.read_csv(io.BytesIO(content), encoding="utf-8-sig", dtype={'JAN': str})
        
        # JANã‚³ãƒ¼ãƒ‰ã®å½¢å¼ã‚’ä¿®æ­£ï¼ˆæµ®å‹•å°æ•°ç‚¹è¡¨è¨˜ã‚’ä¿®æ­£ï¼‰
        if 'JAN' in df.columns:
            df['JAN'] = df['JAN'].astype(str).str.replace('.0$', '', regex=True)
            
        return df
    except Exception as e:
        logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {file_key}: {str(e)}")
        raise

def merge_keepa_pricing(keepa_df, pricing_df):
    """
    Keepaã¨Pricingãƒ‡ãƒ¼ã‚¿ã‚’çµåˆï¼ˆdata_integrator.pyã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
    """
    # Pricingãƒ‡ãƒ¼ã‚¿ã®å‹ä¿®æ­£ï¼ˆdata_integrator.pyã¨åŒã˜å‡¦ç†ï¼‰
    if 'è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ' in pricing_df.columns:
        # æ–‡å­—åˆ—ã®'False'ã¨'True'ã‚’æ•°å€¤ã«å¤‰æ›
        pricing_df['è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ'] = pricing_df['è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ'].replace(['False', 'True'], [0, 1])
        # NaNå€¤ã‚’ä¸€æ™‚çš„ãªæ•°å€¤ã«ç½®ãæ›ãˆ
        pricing_df['è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ'] = pricing_df['è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ'].fillna(-999)
        # æ•°å€¤å‹ã«å¤‰æ›
        pricing_df['è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ'] = pd.to_numeric(pricing_df['è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ'], errors='coerce')
    
    # Keepaã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ã¦å·¦çµåˆï¼ˆdata_integrator.pyã¨åŒã˜ï¼‰
    merged_df = pd.merge(
        keepa_df,     # Keepaãƒ‡ãƒ¼ã‚¿ã‚’åŸºæº–
        pricing_df,
        on='ASIN',
        how='left',   # Keepaãƒ‡ãƒ¼ã‚¿ã‚’åŸºæº–ã«å·¦çµåˆ
        suffixes=('', '_sp')  # Keepaã®åˆ—åã‚’å„ªå…ˆ
    )
    
    # JANåˆ—ã®çµ±åˆï¼ˆKeepaå„ªå…ˆï¼‰
    if 'JAN_sp' in merged_df.columns:
        merged_df['JAN'] = merged_df['JAN'].combine_first(merged_df['JAN_sp'])
        merged_df = merged_df.drop(columns=['JAN_sp'])
        logger.info("âœ… JANåˆ—ã‚’çµ±åˆã—ã¾ã—ãŸ (Keepaå„ªå…ˆ)")
    
    # è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆåˆ—ã®å¾Œå‡¦ç†
    if 'è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ' in merged_df.columns:
        # æ–‡å­—åˆ—ã‚„ãƒ–ãƒ¼ãƒ«å€¤ã‚’æ•°å€¤ã«å¤‰æ›
        merged_df['è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ'] = merged_df['è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ'].replace(['False', 'True'], [0, 1])
        merged_df['è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ'] = merged_df['è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ'].replace([False, True], [0, 1])
        # -999ã‚’å†ã³Noneã«æˆ»ã™
        merged_df['è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ'] = merged_df['è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ'].replace(-999, None)
        # æ•°å€¤å‹ã«å¤‰æ›
        merged_df['è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ'] = pd.to_numeric(merged_df['è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ'], errors='coerce')
    
    return merged_df

def merge_yahoraku_data_by_asin(base_df, yahoraku_df):
    """
    YahoRakuãƒ‡ãƒ¼ã‚¿ã‚’ASINãƒ™ãƒ¼ã‚¹ã§æ¨ªå±•é–‹ã—ã¦çµåˆï¼ˆdata_integrator.pyã¨åŒã˜å‡¦ç†ï¼‰
    """
    logger.info(f"ğŸ”„ Yahoo/Rakutençµåˆé–‹å§‹")
    
    # ASINãŒå­˜åœ¨ã—ãªã„å ´åˆã®å‡¦ç†
    if 'ASIN' not in base_df.columns:
        logger.warning("âš ï¸ ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã«ASINåˆ—ãŒã‚ã‚Šã¾ã›ã‚“")
        return base_df
    
    if 'ASIN' not in yahoraku_df.columns:
        logger.warning("âš ï¸ YahoRakuãƒ‡ãƒ¼ã‚¿ã«ASINåˆ—ãŒã‚ã‚Šã¾ã›ã‚“")
        return base_df
    
    # ASINåˆ—ã‚’æ–‡å­—åˆ—ã«å¤‰æ›
    base_df['ASIN'] = base_df['ASIN'].astype(str)
    yahoraku_df['ASIN'] = yahoraku_df['ASIN'].astype(str)
    
    # APIåˆ—ã®å­˜åœ¨ç¢ºèª
    if 'API' not in yahoraku_df.columns:
        logger.warning("âš ï¸ YahoRakuãƒ‡ãƒ¼ã‚¿ã«APIåˆ—ãŒã‚ã‚Šã¾ã›ã‚“")
        return base_df
    
    # APIåˆ¥ã«åˆ†å‰²
    yahoo_df = yahoraku_df[yahoraku_df['API'] == 'Yahoo'].copy()
    rakuten_df = yahoraku_df[yahoraku_df['API'] == 'Rakuten'].copy()
    
    logger.info(f"ğŸ“Š Yahoo: {len(yahoo_df)}ä»¶, Rakuten: {len(rakuten_df)}ä»¶")
    
    # å„ASINã”ã¨ã«å‡¦ç†
    base_asins = base_df['ASIN'].dropna().unique()
    added_yahoo_data = 0
    added_rakuten_data = 0
    
    for asin in base_asins:
        # Yahooæƒ…å ±ï¼ˆä¸Šä½3ä»¶ï¼‰
        yahoo_rows = yahoo_df[yahoo_df['ASIN'] == asin].head(3)
        for i, (_, row) in enumerate(yahoo_rows.iterrows(), 1):
            for col in ['ä¾¡æ ¼', 'é€æ–™æ¡ä»¶', 'å•†å“URL']:
                if col in row and pd.notna(row[col]):
                    col_name = f'ãƒ¤ãƒ•ãƒ¼_{col}_{i}'
                    base_df.loc[base_df['ASIN'] == asin, col_name] = row[col]
                    added_yahoo_data += 1
        
        # Rakutenæƒ…å ±ï¼ˆä¸Šä½3ä»¶ï¼‰
        rakuten_rows = rakuten_df[rakuten_df['ASIN'] == asin].head(3)
        for i, (_, row) in enumerate(rakuten_rows.iterrows(), 1):
            for col in ['ä¾¡æ ¼', 'é€æ–™æ¡ä»¶', 'å•†å“URL']:
                if col in row and pd.notna(row[col]):
                    col_name = f'æ¥½å¤©_{col}_{i}'
                    base_df.loc[base_df['ASIN'] == asin, col_name] = row[col]
                    added_rakuten_data += 1
    
    logger.info(f"âœ… Yahoo/Rakutençµåˆå®Œäº† - Yahoo: {added_yahoo_data}å€‹, Rakuten: {added_rakuten_data}å€‹ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ")
    
    return base_df

def rearrange_columns_like_integrator(df):
    """
    data_integrator.pyã¨åŒã˜åˆ—é †åºã«ä¸¦ã³æ›¿ãˆ
    """
    # data_integrator.pyã¨åŒã˜åˆ—é †åºå®šç¾©
    column_order = [
        # åŸºæœ¬æƒ…å ±1
        'ASIN', 'JAN', 'å•†å“å', 'ã‚«ãƒ†ã‚´ãƒªãƒ¼', 'ãƒ¡ãƒ¼ã‚«ãƒ¼å‹ç•ª', 'ãƒ¬ãƒ“ãƒ¥ãƒ¼æœ‰ç„¡', 
        'ãƒ¡ãƒ¼ã‚«ãƒ¼å', 'ãƒ–ãƒ©ãƒ³ãƒ‰å', 'ç·å‡ºå“è€…æ•°', 'ã‚»ãƒƒãƒˆæ•°', 'å•†å“è¿½è·¡æ—¥', 
        'å•†å“ç™ºå£²æ—¥', 'è¿½è·¡é–‹å§‹ã‹ã‚‰ã®çµŒéæ—¥æ•°', 'ã‚¢ãƒ€ãƒ«ãƒˆå•†å“å¯¾è±¡',

        # åŸºæœ¬æƒ…å ±2
        'å‚è€ƒä¾¡æ ¼', 'ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸æœ€é•·è¾º', 'ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ä¸­è¾º', 'ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸æœ€çŸ­è¾º', 
        'ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸é‡é‡', 'ç¾åœ¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°', '30æ—¥é–“å¹³å‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°', 
        '90æ—¥é–“å¹³å‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°', '180æ—¥é–“å¹³å‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°', 'amazonURL', 
        'KeepaURL', 'ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ASIN',

        # ä¾¡æ ¼æƒ…å ±
        'Amazonä¾¡æ ¼', 'ã‚«ãƒ¼ãƒˆä¾¡æ ¼', 'ã‚«ãƒ¼ãƒˆä¾¡æ ¼é€æ–™', 'ã‚«ãƒ¼ãƒˆä¾¡æ ¼ã®ãƒã‚¤ãƒ³ãƒˆ', 
        'ãƒªãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ ï¼ˆæ™‚é–“ï¼‰', 'FBAæœ€å®‰å€¤', 'FBAæœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ', 
        'è‡ªå·±ç™ºé€æœ€å®‰å€¤', 'è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®é€æ–™', 'è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ', 
        'FBA_è²©å£²æ‰‹æ•°æ–™', 'FBA_é…é€ä»£è¡Œæ‰‹æ•°æ–™',

        # å‡ºå“è€…æƒ…å ±
        'amazonæœ¬ä½“æœ‰ç„¡1', 'amazonæœ¬ä½“æœ‰ç„¡2', 'FBAæ•°', 'è‡ªå·±ç™ºé€æ•°', 
        'FBAæœ€å®‰å€¤å‡ºå“è€…æ•°', 'è‡ªå·±ç™ºé€æœ€å®‰å€¤å‡ºå“è€…æ•°', 
        'amazon_30æ—¥é–“åœ¨åº«åˆ‡ã‚Œç‡', 'amazon_90æ—¥é–“åœ¨åº«åˆ‡ã‚Œç‡',

        # è²©å£²æ•°æƒ…å ±
        '30æ—¥é–“_ç·è²©å£²æ•°', '30æ—¥é–“_æ–°å“è²©å£²æ•°', '30æ—¥é–“_ä¸­å¤è²©å£²æ•°', 
        '30æ—¥é–“_ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼è²©å£²æ•°', 'Keepa30æ—¥é–“è²©å£²æ•°', 
        '90æ—¥é–“_ç·è²©å£²æ•°', '90æ—¥é–“_æ–°å“è²©å£²æ•°', '90æ—¥é–“_ä¸­å¤è²©å£²æ•°', 
        '90æ—¥é–“_ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼è²©å£²æ•°', 'Keepa90æ—¥é–“è²©å£²æ•°',
        '180æ—¥é–“_ç·è²©å£²æ•°', '180æ—¥é–“_æ–°å“è²©å£²æ•°', '180æ—¥é–“_ä¸­å¤è²©å£²æ•°', 
        '180æ—¥é–“_ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼è²©å£²æ•°', 'Keepa180æ—¥é–“è²©å£²æ•°',

        # ä¾¡æ ¼å±¥æ­´
        'amazonä¾¡æ ¼_ç¾åœ¨ä¾¡æ ¼', 'amazonä¾¡æ ¼_æœ€é«˜ä¾¡æ ¼', 'amazonä¾¡æ ¼_æœ€ä½ä¾¡æ ¼',
        'amazonä¾¡æ ¼_30æ—¥å¹³å‡ä¾¡æ ¼', 'amazonä¾¡æ ¼_90æ—¥å¹³å‡ä¾¡æ ¼', 
        'amazonä¾¡æ ¼_180æ—¥å¹³å‡ä¾¡æ ¼', 'æ–°å“ä¾¡æ ¼_ç¾åœ¨ä¾¡æ ¼', 'æ–°å“ä¾¡æ ¼_æœ€é«˜ä¾¡æ ¼',
        'æ–°å“ä¾¡æ ¼_æœ€ä½ä¾¡æ ¼', 'æ–°å“ä¾¡æ ¼_30æ—¥å¹³å‡ä¾¡æ ¼', 'æ–°å“ä¾¡æ ¼_90æ—¥å¹³å‡ä¾¡æ ¼',
        'æ–°å“ä¾¡æ ¼_180æ—¥å¹³å‡ä¾¡æ ¼',

        # ãã®ä»–
        'ç”»åƒURL', 'å…ƒã‚³ãƒ¼ãƒ‰', 'ã‚³ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—'
    ]
    
    # å­˜åœ¨ã™ã‚‹åˆ—ã®ã¿ã‚’æŠ½å‡º
    specified_columns = [col for col in column_order if col in df.columns]
    
    # æŒ‡å®šã•ã‚Œã¦ã„ãªã„æ®‹ã‚Šã®åˆ—ï¼ˆãƒ¤ãƒ•ãƒ¼ãƒ»æ¥½å¤©åˆ—ãªã©ï¼‰ã‚’å–å¾—
    remaining_columns = [col for col in df.columns if col not in column_order]
    
    # æŒ‡å®šåˆ— + æ®‹ã‚Šã®åˆ—ã®é †ã§æ–°ã—ã„åˆ—é †ã‚’ä½œæˆ
    new_column_order = specified_columns + remaining_columns
    
    # ä¸¦ã³æ›¿ãˆã‚’å®Ÿè¡Œ
    df = df[new_column_order]
    
    logger.info(f"âœ… ã‚«ãƒ©ãƒ ä¸¦ã³æ›¿ãˆå®Œäº†: æŒ‡å®šåˆ— {len(specified_columns)}åˆ— + è¿½åŠ åˆ— {len(remaining_columns)}åˆ—")
    
    return df

def fix_jan_format(df):
    """
    JANã‚³ãƒ¼ãƒ‰ã®å½¢å¼ã‚’çµ±ä¸€ï¼ˆdata_integrator.pyã¨åŒã˜å‡¦ç†ï¼‰
    """
    if 'JAN' in df.columns:
        # æ–‡å­—åˆ—ã¨ã—ã¦å‡¦ç†
        df['JAN'] = df['JAN'].astype(str)
        # æµ®å‹•å°æ•°ç‚¹å½¢å¼ã‚’ä¿®æ­£
        df['JAN'] = df['JAN'].replace(r'\.0$', '', regex=True)
        # æŒ‡æ•°è¡¨è¨˜ã‚’ä¿®æ­£
        df['JAN'] = df['JAN'].apply(
            lambda x: f"{float(x):.0f}" if re.match(r'\d+\.\d+e\+\d+', str(x).lower()) else x
        )
        logger.info("âœ… JANã‚³ãƒ¼ãƒ‰å½¢å¼ã‚’çµ±ä¸€ã—ã¾ã—ãŸ")
    
    return df