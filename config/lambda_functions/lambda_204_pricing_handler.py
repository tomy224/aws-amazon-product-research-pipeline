import boto3
import os
import logging
import time
from datetime import datetime

from modules.apis.sp_api import AmazonProductAPI

s3 = boto3.client("s3")
logger = logging.getLogger()
logger.setLevel(logging.INFO)

def lambda_handler(event, context):
    """
    ãƒ—ãƒ©ã‚¤ã‚·ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã™ã‚‹Lambdaé–¢æ•°
    
    ğŸ”§ ä¿®æ­£ç‚¹ï¼š
    - ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã®å ´åˆã®å¯¾å¿œ
    - ç©ºãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚‚S3ã«ä¿å­˜ï¼ˆcopy_taskãŒæ­£å¸¸ã«å‹•ä½œã™ã‚‹ãŸã‚ï¼‰
    - copy_taskã®æ¡ä»¶åˆ†å²ã‚’è¿½åŠ 
    """
    input_key = event.get("input_key")
    batch_index = event.get("batch_index", 0)
    total_batches = event.get("total_batches", 1)
    
    is_last_batch = (batch_index >= total_batches - 1)
    
    if not input_key:
        return {"statusCode": 400, "message": "input_key ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"}

    bucket = os.environ.get("BUCKET_NAME", "your-bucket-name")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
    path_parts = input_key.split('/')
    filename = path_parts[-1]
    directory = '/'.join(path_parts[:-1])
    output_directory = directory.replace("input", "output")
    basename = filename.rsplit('.', 1)[0]
    
    output_key = f"{output_directory}/{basename}_pricing.csv"
    filtered_key = f"{output_directory}/{basename}_pricing_filtered.csv"
    
    logger.info(f"ğŸ“ å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {input_key}")
    logger.info(f"ğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {output_key}")
    logger.info(f"ğŸ“ ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«: {filtered_key}")

    tmp_input = "/tmp/input.csv"
    tmp_output = "/tmp/output.csv"
    tmp_filtered_output = "/tmp/filtered_output.csv"

    try:
        # S3ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        s3.download_file(bucket, input_key, tmp_input)
        logger.info(f"âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {input_key}")

        # APIåˆæœŸåŒ–
        analyzer = AmazonProductAPI()

        # ãƒ—ãƒ©ã‚¤ã‚·ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿å‡¦ç†
        logger.info("ğŸ”„ ãƒ—ãƒ©ã‚¤ã‚·ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–‹å§‹...")
        all_data, filtered_data = analyzer.process_pricing_data(
            input_file=tmp_input,
            output_file=tmp_output,
            batch_size=20
        )

        logger.info(f"ğŸ“Š å‡¦ç†çµæœ: å…¨ä½“ {len(all_data)}ä»¶, ãƒ•ã‚£ãƒ«ã‚¿å¾Œ {len(filtered_data)}ä»¶")

        # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        s3.upload_file(tmp_output, bucket, output_key)
        logger.info(f"âœ… ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰: {output_key}")

        # ğŸ”§ é‡è¦ãªä¿®æ­£ï¼šãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†
        copy_task_src_key = ""
        copy_task_dst_key = ""
        
        if filtered_data and len(filtered_data) > 0:
            # ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆï¼šé€šå¸¸ã®å‡¦ç†
            logger.info(f"ğŸ“Š ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚ã‚Š: {len(filtered_data)}ä»¶")
            
            analyzer.save_results(filtered_data, tmp_filtered_output)
            s3.upload_file(tmp_filtered_output, bucket, filtered_key)
            logger.info(f"âœ… ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰: {filtered_key}")
            
            # copy_taskã‚’è¨­å®š
            copy_task_src_key = filtered_key
            copy_task_dst_key = filtered_key.replace("output/", "input/")
            
        else:
            # ğŸ”§ ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã®å ´åˆï¼šç©ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
            logger.warning(f"âš ï¸ ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã§ã™")
            
            # ç©ºã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼ã®ã¿ï¼‰
            import pandas as pd
            empty_df = pd.DataFrame()
            
            # å…ƒãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã¯ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ä¿æŒ
            if all_data and len(all_data) > 0:
                # å…ƒãƒ‡ãƒ¼ã‚¿ã®åˆ—æ§‹é€ ã‚’ä½¿ç”¨ã—ã¦ç©ºã®DataFrameã‚’ä½œæˆ
                sample_data = all_data[0]
                empty_df = pd.DataFrame(columns=sample_data.keys())
            
            # ç©ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
            empty_df.to_csv(tmp_filtered_output, index=False, encoding='utf-8-sig')
            s3.upload_file(tmp_filtered_output, bucket, filtered_key)
            logger.info(f"âœ… ç©ºã®ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ: {filtered_key}")
            
            # ğŸ”§ ç©ºãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯copy_taskã‚’ç©ºã«ã™ã‚‹ï¼ˆã‚³ãƒ”ãƒ¼ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰
            copy_task_src_key = ""
            copy_task_dst_key = ""
            logger.info("ğŸ“‹ ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã®ãŸã‚ã€copy_taskã‚’ç©ºã«è¨­å®š")

        # JANã‚³ãƒ¼ãƒ‰ä¸€è¦§ã‚’æŠ½å‡º
        jan_codes = []
        if filtered_data and len(filtered_data) > 0:
            jan_codes = sorted(set(row["JAN"] for row in filtered_data if "JAN" in row and row["JAN"]))

        # ğŸ”§ æˆ»ã‚Šå€¤ã®ä¿®æ­£ï¼šcopy_taskã®æ¡ä»¶åˆ†å²
        result = {
            "statusCode": 200,
            "input": input_key,
            "output": output_key,
            "filtered_output": filtered_key,
            "total_items": len(all_data),
            "filtered_items": len(filtered_data),
            "is_last_batch": is_last_batch,
            "jan_codes": jan_codes,
            "copy_task": {
                "src_key": copy_task_src_key,
                "dst_key": copy_task_dst_key
            },
            "has_filtered_data": len(filtered_data) > 0  # ğŸ”§ è¿½åŠ ï¼šãƒ•ã‚£ãƒ«ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æœ‰ç„¡ã‚’æ˜ç¤º
        }
        
        logger.info(f"ğŸ¯ copy_taskè¨­å®š:")
        logger.info(f"  src_key: '{copy_task_src_key}'")
        logger.info(f"  dst_key: '{copy_task_dst_key}'")
        logger.info(f"  has_filtered_data: {len(filtered_data) > 0}")
        
        return result

    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {str(e)}")
        import traceback
        logger.error(f"ğŸ“Š ã‚¨ãƒ©ãƒ¼è©³ç´°: {traceback.format_exc()}")
        
        return {
            "statusCode": 500,
            "message": "ãƒ—ãƒ©ã‚¤ã‚·ãƒ³ã‚°å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚",
            "error": str(e),
            "input": input_key,
            "copy_task": {
                "src_key": "",
                "dst_key": ""
            },
            "has_filtered_data": False
        }