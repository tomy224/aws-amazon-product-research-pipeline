#!/usr/bin/env python
# coding: utf-8

"""
ãƒ‡ãƒ¼ã‚¿çµ±åˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã€SP-APIã¨Keepa APIã®ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã™ã‚‹ãŸã‚ã®æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚
çµ±åˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã¯CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã•ã‚Œã¾ã™ã€‚
"""

import sys
import os
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æ¤œç´¢ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = str(Path(__file__).resolve().parents[2])  # modules/integration ã‹ã‚‰2éšå±¤ä¸Š
if project_root not in sys.path:
    sys.path.insert(0, project_root)

import pandas as pd
import logging
from datetime import datetime
import re

# è‡ªä½œãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from modules.utils.file_utils import find_project_root, load_yaml_config
from modules.utils.logger_utils import setup_logging, get_logger

# ãƒ­ã‚¬ãƒ¼ã®å–å¾—
logger = get_logger(__name__)

class DataIntegrator:
    """SP-APIã¨Keepa APIã®ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config_path=None):
        """
        åˆæœŸåŒ–ã¨è¨­å®šèª­ã¿è¾¼ã¿
        
        Parameters:
        -----------
        config_path : str, optional
            è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã€‚æŒ‡å®šãŒãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ã‚¹ã‚’ä½¿ç”¨
        """
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æ¤œå‡º
        self.root_dir = find_project_root()
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã‚’è¨­å®š
        self.data_dir = os.path.join(self.root_dir, 'data')
        self.log_dir = os.path.join(self.root_dir, 'logs')
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèª
        os.makedirs(self.data_dir, exist_ok=True)
        os.makedirs(self.log_dir, exist_ok=True)
            
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
        self.config = self._load_config(config_path)
        
        # ãƒ­ã‚°æ©Ÿèƒ½ã®è¨­å®š
        self._setup_logging()
    
    def _load_config(self, config_path=None):
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
        # load_yaml_configã‚’ä½¿ç”¨ã—ã¦è¨­å®šã‚’èª­ã¿è¾¼ã‚€
        config = load_yaml_config(config_path)
        
        # ãƒ‡ãƒ¼ã‚¿çµ±åˆè¨­å®šã®å­˜åœ¨ç¢ºèª
        if 'data_integration' not in config:
            config['data_integration'] = {}
            
        # outputè¨­å®šã®åˆæœŸåŒ–ï¼ˆãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®šï¼‰
        if 'output' not in config['data_integration']:
            config['data_integration']['output'] = {
                'sp_api_input': 'sp_api_output_filtered.csv',
                'keepa_input': 'keepa_output.csv',
                'output_file': 'integrated_data.csv'
            }
            
        # sourcesè¨­å®šã®åˆæœŸåŒ–ï¼ˆãªã‘ã‚Œã°ç©ºãƒªã‚¹ãƒˆã‚’è¨­å®šï¼‰
        if 'sources' not in config['data_integration']:
            config['data_integration']['sources'] = []
            
        logger.info(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        return config
    
    def _setup_logging(self):
        """ãƒ­ã‚°æ©Ÿèƒ½ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        log_file = setup_logging(
            self.log_dir, 
            "integration", 
            console_level=logging.INFO, 
            file_level=logging.DEBUG
        )
        logger.info("ãƒ‡ãƒ¼ã‚¿çµ±åˆå‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™")
        print(f"ğŸ“„ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file}")

    def load_data(self, sp_api_file=None, keepa_file=None):
        """
        CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
        
        Parameters:
        -----------
        sp_api_file : str, optional
            SP-APIãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ï¼‰
        keepa_file : str, optional
            Keepaãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ï¼‰
            
        Returns:
        --------
        tuple
            (sp_df, keepa_df) - èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        """
        try:
            # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«åãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—
            if sp_api_file is None:
                sp_api_file = self.config['data_integration']['output']['sp_api_input']
                
            if keepa_file is None:
                keepa_file = self.config['data_integration']['output']['keepa_input']
            
            # ç›¸å¯¾ãƒ‘ã‚¹ã®å ´åˆã¯dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’åŸºæº–ã«ã™ã‚‹
            if not os.path.isabs(sp_api_file):
                sp_api_file = os.path.join(self.data_dir, sp_api_file)
                
            if not os.path.isabs(keepa_file):
                keepa_file = os.path.join(self.data_dir, keepa_file)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèªã¨ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æ”¹å–„
            if not os.path.exists(sp_api_file):
                error_msg = f"SP-APIãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {sp_api_file}"
                logger.error(error_msg)
                raise FileNotFoundError(error_msg)
                
            if not os.path.exists(keepa_file):
                error_msg = f"Keepaãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {keepa_file}"
                logger.error(error_msg)
                raise FileNotFoundError(error_msg)
            
            # SP-APIãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ - JANã‚’æ–‡å­—åˆ—ã¨ã—ã¦èª­ã¿è¾¼ã‚€
            sp_df = pd.read_csv(sp_api_file, encoding='utf-8-sig', dtype={'JAN': str, 'å…ƒã‚³ãƒ¼ãƒ‰': str})
            logger.info(f"SP-APIãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {len(sp_df)}ä»¶")
            print(f"ğŸ“Š SP-APIãƒ‡ãƒ¼ã‚¿: {len(sp_df)}ä»¶")
            
            # Keepaãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ - JANã‚’æ–‡å­—åˆ—ã¨ã—ã¦èª­ã¿è¾¼ã‚€
            keepa_df = pd.read_csv(keepa_file, encoding='utf-8-sig', dtype={'JAN': str})
            logger.info(f"Keepaãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {len(keepa_df)}ä»¶")
            print(f"ğŸ“Š Keepaãƒ‡ãƒ¼ã‚¿: {len(keepa_df)}ä»¶")
            
            # JANãŒæµ®å‹•å°æ•°ç‚¹ã«ãªã£ã¦ã„ã‚‹å ´åˆã®ä¿®æ­£å‡¦ç†
            if 'JAN' in sp_df.columns:
                sp_df['JAN'] = sp_df['JAN'].astype(str).str.replace('.0$', '', regex=True)
            if 'JAN' in keepa_df.columns:
                keepa_df['JAN'] = keepa_df['JAN'].astype(str).str.replace('.0$', '', regex=True)
            
            return sp_df, keepa_df
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise

    def load_source_data(self, source_config):
        """
        ã‚½ãƒ¼ã‚¹è¨­å®šã«åŸºã¥ã„ã¦CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        
        Parameters:
        -----------
        source_config : dict
            ã‚½ãƒ¼ã‚¹è¨­å®šæƒ…å ±
            
        Returns:
        --------
        dict
            ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ã‚­ãƒ¼ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å€¤ã¨ã™ã‚‹è¾æ›¸
        """
        result = {}
        
        try:
            files = source_config.get('files', [])
            key_column = source_config.get('key_column', 'JAN')
            
            # JANåˆ—ã®ä»£æ›¿åãƒªã‚¹ãƒˆï¼ˆã‚ˆãã‚ã‚‹å‘½åãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
            jan_column_alternatives = ['JAN', 'JANã‚³ãƒ¼ãƒ‰', 'jan', 'jancode', 'jan_code', 'ean', 'EAN']
            
            for file in files:
                try:
                    # ç›¸å¯¾ãƒ‘ã‚¹ã®å ´åˆã¯ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’åŸºæº–ã«ã™ã‚‹
                    file_path = file
                    if not os.path.isabs(file_path):
                        file_path = os.path.join(self.data_dir, file_path)
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
                    if not os.path.exists(file_path):
                        logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                        print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                        continue
                    
                    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
                    df = pd.read_csv(file_path, encoding='utf-8-sig')
                    
                    # å®Ÿéš›ã®ã‚­ãƒ¼åˆ—ã‚’ç‰¹å®š
                    actual_key_column = key_column  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                    
                    # è¨­å®šã•ã‚ŒãŸã‚­ãƒ¼åˆ—ãŒãªã„å ´åˆã¯ä»£æ›¿åã‹ã‚‰æ¢ã™
                    if key_column not in df.columns:
                        # ä»£æ›¿åãƒªã‚¹ãƒˆã‹ã‚‰åˆ—åã‚’æ¢ã™
                        for alt_column in jan_column_alternatives:
                            if alt_column in df.columns:
                                actual_key_column = alt_column
                                print(f"â„¹ï¸ {file}ã§ã¯ã€Œ{key_column}ã€ã®ä»£ã‚ã‚Šã«ã€Œ{actual_key_column}ã€ã‚’ä½¿ç”¨ã—ã¾ã™")
                                break
                    
                    # ã‚­ãƒ¼åˆ—ã®å­˜åœ¨ç¢ºèª
                    if actual_key_column not in df.columns:
                        logger.warning(f"ã‚­ãƒ¼åˆ— '{key_column}' ã¾ãŸã¯ãã®ä»£æ›¿åãŒ {file} ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                        print(f"âš ï¸ ã‚­ãƒ¼åˆ— '{key_column}' ã¾ãŸã¯ãã®ä»£æ›¿åãŒ {file} ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                        print(f"  åˆ©ç”¨å¯èƒ½ãªåˆ—: {', '.join(df.columns)}")
                        continue
                    
                    # çµæœã«è¿½åŠ 
                    result[file] = {
                        'df': df,
                        'key_column': actual_key_column
                    }
                    
                    logger.info(f"ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {file} ({len(df)}ä»¶), ã‚­ãƒ¼åˆ—: {actual_key_column}")
                    print(f"ğŸ“Š {file}ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {len(df)}ä»¶, ã‚­ãƒ¼åˆ—: {actual_key_column}")
                    
                except Exception as e:
                    logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ« {file} ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
                    print(f"âš ï¸ {file}ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
                    continue
                    
            return result
            
        except Exception as e:
            logger.error(f"ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å…¨ä½“ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {}


    
    def _merge_yahoo_rakuten_data(self, base_df, source_data):
        """
        Yahoo/Rakutenãƒ‡ãƒ¼ã‚¿ã‚’æ¨ªå±•é–‹ã—ã¦çµåˆã™ã‚‹ç‰¹åˆ¥ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆãƒ‡ãƒãƒƒã‚°å¼·åŒ–ç‰ˆï¼‰
        
        Parameters:
        -----------
        base_df : pandas.DataFrame
            ãƒ™ãƒ¼ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        source_data : dict
            ãƒ¤ãƒ•ãƒ¼/æ¥½å¤©ã®ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
                
        Returns:
        --------
        pandas.DataFrame
            çµåˆå¾Œã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        """
        result_df = base_df.copy()
        
        # ğŸ” ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®ASINæƒ…å ±ã‚’ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
        print(f"\nğŸ” Yahoo/Rakutençµåˆãƒ‡ãƒãƒƒã‚°æƒ…å ±:")
        
        if 'ASIN' not in result_df.columns:
            logger.warning("ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã«ASINåˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚Yahoo/Rakutenãƒ‡ãƒ¼ã‚¿ã¯çµåˆã§ãã¾ã›ã‚“ã€‚")
            print("âš ï¸ ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã«ASINåˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚Yahoo/Rakutenãƒ‡ãƒ¼ã‚¿ã¯çµåˆã§ãã¾ã›ã‚“ã€‚")
            return result_df
        
        # ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®ASINæƒ…å ±ã‚’è©³ç´°ãƒã‚§ãƒƒã‚¯
        base_asins = result_df['ASIN'].dropna().unique()
        print(f"  ğŸ“Š ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ï¼ˆKeepaï¼‰ã®ASIN:")
        print(f"     - ç·ASINæ•°: {len(base_asins)}ä»¶")
        print(f"     - ASINä¾‹: {list(base_asins)[:5]}...")  # æœ€åˆã®5ã¤ã‚’è¡¨ç¤º
        
        # ASINåˆ—ã‚’æ–‡å­—åˆ—ã«å¤‰æ›
        result_df['ASIN'] = result_df['ASIN'].astype(str).str.replace('.0$', '', regex=True)
        
        total_added_columns = 0
        
        for file, data in source_data.items():
            source_df = data['df']
            key_column = data['key_column']
            
            print(f"\n  ğŸ“„ å‡¦ç†ä¸­ãƒ•ã‚¡ã‚¤ãƒ«: {file}")
            print(f"     - ã‚­ãƒ¼åˆ—: {key_column}")
            print(f"     - ç·è¡Œæ•°: {len(source_df)}ä»¶")
            
            # ğŸ” ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®ASINæƒ…å ±ã‚’ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
            if key_column in source_df.columns:
                # ã‚­ãƒ¼åˆ—ã‚’æ–‡å­—åˆ—ã«å¤‰æ›
                source_df[key_column] = source_df[key_column].astype(str).str.replace('.0$', '', regex=True)
                
                source_asins = source_df[key_column].dropna().unique()
                print(f"     - ã‚½ãƒ¼ã‚¹ASINæ•°: {len(source_asins)}ä»¶")
                print(f"     - ã‚½ãƒ¼ã‚¹ASINä¾‹: {list(source_asins)[:5]}...")
                
                # ğŸ”‘ ASINç…§åˆçŠ¶æ³ã‚’è©³ç´°ãƒã‚§ãƒƒã‚¯
                common_asins = set(base_asins).intersection(set(source_asins))
                print(f"     - å…±é€šASIN: {len(common_asins)}ä»¶")
                
                if len(common_asins) > 0:
                    print(f"     - å…±é€šASINä¾‹: {list(common_asins)[:5]}...")
                else:
                    print(f"     âš ï¸ å…±é€šASINãŒ0ä»¶ã§ã™ï¼")
                    print(f"     ğŸ” ãƒ™ãƒ¼ã‚¹ASINå‹: {type(base_asins[0]) if len(base_asins) > 0 else 'ãªã—'}")
                    print(f"     ğŸ” ã‚½ãƒ¼ã‚¹ASINå‹: {type(source_asins[0]) if len(source_asins) > 0 else 'ãªã—'}")
                    
                    # ã•ã‚‰ã«è©³ç´°ãªæ¯”è¼ƒ
                    if len(base_asins) > 0 and len(source_asins) > 0:
                        print(f"     ğŸ” ãƒ™ãƒ¼ã‚¹ASIN[0]: '{base_asins[0]}' (é•·ã•: {len(str(base_asins[0]))})")
                        print(f"     ğŸ” ã‚½ãƒ¼ã‚¹ASIN[0]: '{source_asins[0]}' (é•·ã•: {len(str(source_asins[0]))})")
            
            # ğŸ” APIåˆ—ã®å­˜åœ¨ç¢ºèª
            if 'API' not in source_df.columns:
                print(f"     âš ï¸ APIåˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚Yahoo/Rakutenåˆ†é¡ãŒã§ãã¾ã›ã‚“ã€‚")
                continue
            
            # APIåˆ—ã®å€¤ã‚’ç¢ºèª
            api_values = source_df['API'].value_counts()
            print(f"     - APIåˆ†å¸ƒ: {dict(api_values)}")
            
            # APIåˆ—ã§åˆ†é¡
            yahoo_df = source_df[source_df['API'] == 'Yahoo'].copy()
            rakuten_df = source_df[source_df['API'] == 'Rakuten'].copy()
            
            print(f"     - Yahooè¡Œæ•°: {len(yahoo_df)}ä»¶")
            print(f"     - Rakutenè¡Œæ•°: {len(rakuten_df)}ä»¶")
            
            # åˆ—è¿½åŠ å‰ã®åˆ—æ•°ã‚’è¨˜éŒ²
            before_columns = len(result_df.columns)
            
            # å„ASINã”ã¨ã«ä¸Šä½3ä»¶ã‚’å–å¾—ã—ã¦ãƒãƒ¼ã‚¸ã™ã‚‹å‡¦ç†
            base_asin_list = result_df['ASIN'].dropna().unique()
            
            added_yahoo_data = 0
            added_rakuten_data = 0
            
            for asin in base_asin_list:
                # Yahooæƒ…å ±ã®å–å¾—ï¼ˆä¸Šä½3ä»¶ï¼‰
                yahoo_rows = yahoo_df[yahoo_df[key_column] == asin].head(3)
                for i, (_, row) in enumerate(yahoo_rows.iterrows(), 1):
                    # ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’ä»˜ã‘ãŸåˆ—åã§ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                    for col in ['ä¾¡æ ¼', 'é€æ–™æ¡ä»¶', 'å•†å“URL']:
                        if col in row and pd.notna(row[col]):
                            col_name = f'ãƒ¤ãƒ•ãƒ¼_{col}_{i}'
                            result_df.loc[result_df['ASIN'] == asin, col_name] = row[col]
                            added_yahoo_data += 1
                
                # Rakutenæƒ…å ±ã®å–å¾—ï¼ˆä¸Šä½3ä»¶ï¼‰
                rakuten_rows = rakuten_df[rakuten_df[key_column] == asin].head(3)
                for i, (_, row) in enumerate(rakuten_rows.iterrows(), 1):
                    # ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’ä»˜ã‘ãŸåˆ—åã§ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                    for col in ['ä¾¡æ ¼', 'é€æ–™æ¡ä»¶', 'å•†å“URL']:
                        if col in row and pd.notna(row[col]):
                            col_name = f'æ¥½å¤©_{col}_{i}'
                            result_df.loc[result_df['ASIN'] == asin, col_name] = row[col]
                            added_rakuten_data += 1
            
            # åˆ—è¿½åŠ å¾Œã®åˆ—æ•°ã‚’è¨˜éŒ²
            after_columns = len(result_df.columns)
            added_columns = after_columns - before_columns
            total_added_columns += added_columns
            
            print(f"     âœ… ãƒ‡ãƒ¼ã‚¿è¿½åŠ çµæœ:")
            print(f"        - è¿½åŠ åˆ—æ•°: {added_columns}åˆ—")
            print(f"        - Yahooãƒ‡ãƒ¼ã‚¿è¿½åŠ : {added_yahoo_data}å€‹")
            print(f"        - Rakutenãƒ‡ãƒ¼ã‚¿è¿½åŠ : {added_rakuten_data}å€‹")
        
        print(f"\n  ğŸ¯ Yahoo/Rakutençµåˆã‚µãƒãƒªãƒ¼:")
        print(f"     - ç·è¿½åŠ åˆ—æ•°: {total_added_columns}åˆ—")
        print(f"     - æœ€çµ‚åˆ—æ•°: {len(result_df.columns)}åˆ—")
        
        if total_added_columns == 0:
            print(f"  âš ï¸ Yahoo/Rakutenãƒ‡ãƒ¼ã‚¿ãŒè¿½åŠ ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸï¼")
            print(f"  ğŸ’¡ è€ƒãˆã‚‰ã‚Œã‚‹åŸå› :")
            print(f"     1. ASINã®å½¢å¼ãŒä¸€è‡´ã—ã¦ã„ãªã„")
            print(f"     2. ãƒ‡ãƒ¼ã‚¿ãŒç©ºã¾ãŸã¯NaNå€¤")
            print(f"     3. APIåˆ—ã®å€¤ãŒæœŸå¾…å€¤ã¨ç•°ãªã‚‹")
        else:
            print(f"  âœ… Yahoo/Rakutenãƒ‡ãƒ¼ã‚¿ã®çµåˆã«æˆåŠŸã—ã¾ã—ãŸï¼")
        
        logger.info(f"Yahoo/Rakutenãƒ‡ãƒ¼ã‚¿ã‚’æ¨ªå±•é–‹ã—ã¦çµåˆã—ã¾ã—ãŸ: {len(result_df.columns)}åˆ—ï¼ˆ{total_added_columns}åˆ—è¿½åŠ ï¼‰")
        
        return result_df

    def merge_source_data(self, base_df, source_data, source_config):
        """
        ãƒ™ãƒ¼ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆã™ã‚‹
        
        Parameters:
        -----------
        base_df : pandas.DataFrame
            ãƒ™ãƒ¼ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        source_data : dict
            ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ã‚­ãƒ¼ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ æƒ…å ±ã‚’å€¤ã¨ã™ã‚‹è¾æ›¸
        source_config : dict
            ã‚½ãƒ¼ã‚¹è¨­å®šæƒ…å ±
                
        Returns:
        --------
        pandas.DataFrame
            çµåˆå¾Œã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        """
        result_df = base_df.copy()
        source_type = source_config.get('type', 'ä¸æ˜')
        
        # Yahoo/Rakutenã®ãƒ‡ãƒ¼ã‚¿å‘ã‘ç‰¹åˆ¥å‡¦ç†
        if source_type.lower() == 'yahoo_rakuten':
            # Yahoo/Rakutenãƒ‡ãƒ¼ã‚¿ã®æ¨ªå±•é–‹å‡¦ç†
            return self._merge_yahoo_rakuten_data(result_df, source_data)
        
        # ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’ã‚µã‚¤ãƒˆåã«åŸºã¥ã„ã¦è¨­å®š
        if source_type.lower() == 'netsea':
            prefix = 'ãƒãƒƒã‚·ãƒ¼_'
        elif source_type.lower() == 'sudeli':
            prefix = 'ã‚¹ãƒ¼ãƒ‡ãƒª_'
        else:
            # ãã®ä»–ã®ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã®å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨
            prefix = source_config.get('prefix', '')
        
        try:
            # ãƒ™ãƒ¼ã‚¹DFã«JANåˆ—ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
            if 'JAN' not in result_df.columns:
                logger.warning("ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã«JANåˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚çµåˆã§ããªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
                print("âš ï¸ ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã«JANåˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚çµåˆã§ããªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
                
                # JANåˆ—ãŒãªã„å ´åˆã€å…ƒã‚³ãƒ¼ãƒ‰åˆ—ã‹ã‚‰JANã‚’å–å¾—ã—ã¦ã¿ã‚‹
                if 'å…ƒã‚³ãƒ¼ãƒ‰' in result_df.columns:
                    # ã‚³ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—ãŒEANã§ã‚ã‚Œã°å…ƒã‚³ãƒ¼ãƒ‰ã‚’JANã¨ã—ã¦ä½¿ç”¨
                    if 'ã‚³ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—' in result_df.columns:
                        result_df['JAN'] = result_df.apply(
                            lambda row: row['å…ƒã‚³ãƒ¼ãƒ‰'] if row['ã‚³ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—'] == 'EAN' else None, 
                            axis=1
                        )
                        print("ğŸ“Š å…ƒã‚³ãƒ¼ãƒ‰ã¨ã‚³ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—ã‹ã‚‰JANåˆ—ã‚’ä½œæˆã—ã¾ã—ãŸ")
            
            # JANåˆ—ã‚’æ–‡å­—åˆ—ã«å¤‰æ›ã—ã€æµ®å‹•å°æ•°ç‚¹ã®æœ«å°¾ã‚’å‰Šé™¤
            if 'JAN' in result_df.columns:
                result_df['JAN'] = result_df['JAN'].astype(str).str.replace('.0$', '', regex=True)
            
            # çµ±åˆã™ã‚‹ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«å€‹åˆ¥ã«å‡¦ç†
            for file, data in source_data.items():
                source_df = data['df']
                key_column = data['key_column']
                
                # ã‚­ãƒ¼åˆ—ã®å€¤ã‚’æ–‡å­—åˆ—ã«å¤‰æ›ã—ã€æµ®å‹•å°æ•°ç‚¹ã®æœ«å°¾ã‚’å‰Šé™¤
                source_df[key_column] = source_df[key_column].astype(str).str.replace('.0$', '', regex=True)
                
                # JANã‚³ãƒ¼ãƒ‰ã‚’ã‚­ãƒ¼ã«çµåˆ
                logger.info(f"'{key_column}'åˆ—ã‚’'JAN'ã¨ã—ã¦çµåˆ: {file}")
                print(f"ğŸ“Š {source_type}ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰'{key_column}'åˆ—ã‚’'JAN'ã¨ã—ã¦çµåˆ: {file}")
                
                # çµåˆå‰ã«ãƒãƒƒãƒã™ã‚‹JANã‚³ãƒ¼ãƒ‰ã®æ•°ã‚’ç¢ºèª
                if 'JAN' in result_df.columns:
                    # ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®JANãƒªã‚¹ãƒˆ
                    base_jans = set(result_df['JAN'].dropna().unique())
                    # ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®JANãƒªã‚¹ãƒˆ
                    source_jans = set(source_df[key_column].dropna().unique())
                    # å…±é€šã™ã‚‹JANã®æ•°
                    common_jans = base_jans.intersection(source_jans)
                    
                    # é‡è¤‡ã™ã‚‹JANã‚’ãƒã‚§ãƒƒã‚¯
                    duplicate_jans = source_df[source_df[key_column].duplicated(keep=False)][key_column].unique()
                    if len(duplicate_jans) > 0:
                        duplicate_count = len(duplicate_jans)
                        example_duplicates = list(duplicate_jans)[:3]  # æœ€å¤§3ã¤ã¾ã§è¡¨ç¤º
                        print(f"â„¹ï¸ {file}å†…ã«{duplicate_count}ä»¶ã®é‡è¤‡JANã‚’æ¤œå‡º: {', '.join(example_duplicates)}ãªã©")
                        print(f"â„¹ï¸ é‡è¤‡JANã¯å„JANã®æœ€åˆã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ç”¨ã—ã¾ã™")
                    
                    # é‡è¦ãªä¿®æ­£ï¼šå„JANã®æœ€åˆã®ã‚¨ãƒ³ãƒˆãƒªã®ã¿ã‚’ä¿æŒã™ã‚‹
                    # drop_duplicates()ãƒ¡ã‚½ãƒƒãƒ‰ã®keep='first'å¼•æ•°ã§æœ€åˆã®è¡Œã®ã¿ã‚’æ®‹ã™
                    source_df_unique = source_df.drop_duplicates(subset=[key_column], keep='first')
                    
                    # é‡è¤‡å‰Šé™¤å¾Œã®çµæœã‚’è¡¨ç¤º
                    removed_count = len(source_df) - len(source_df_unique)
                    if removed_count > 0:
                        print(f"â„¹ï¸ é‡è¤‡ã‚’é™¤å»: {len(source_df)}è¡Œ â†’ {len(source_df_unique)}è¡Œ ({removed_count}è¡Œå‰Šé™¤)")
                    
                    # ãƒãƒƒãƒã™ã‚‹JANã®ä¾‹ã‚’è¡¨ç¤ºï¼ˆæœ€å¤§5ã¤ï¼‰
                    if common_jans:
                        example_jans = list(common_jans)[:5]
                        print(f"â„¹ï¸ ãƒãƒƒãƒã™ã‚‹JANã®ä¾‹: {', '.join(example_jans)}")
                        
                        # ãƒãƒƒãƒã™ã‚‹JANãŒã‚ã‚‹å ´åˆã®ã¿å‡¦ç†ã‚’ç¶šè¡Œ
                        # åˆ—åã«ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’è¿½åŠ 
                        source_df_renamed = source_df_unique.copy()  # é‡è¤‡é™¤å»æ¸ˆã¿ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½¿ç”¨
                        rename_dict = {}
                        
                        # ã‚­ãƒ¼åˆ—ä»¥å¤–ã®åˆ—åã«ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’è¿½åŠ 
                        for col in source_df_unique.columns:
                            if col != key_column:
                                rename_dict[col] = f"{prefix}{col}"
                        
                        source_df_renamed = source_df_renamed.rename(columns=rename_dict)
                        
                        # çµåˆå‰ã®åˆ—æ•°ã¨ãƒ‡ãƒ¼ã‚¿æ•°ã‚’è¨˜éŒ²
                        pre_merge_columns = len(result_df.columns)
                        
                        # ãƒãƒƒãƒã—ãŸJANã‚³ãƒ¼ãƒ‰ã‚’æŒã¤è¡Œã®ã¿ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                        filtered_source_df = source_df_renamed[source_df_renamed[key_column].isin(common_jans)]
                        
                        if not filtered_source_df.empty:
                            # çµåˆå®Ÿè¡Œ
                            result_df = pd.merge(
                                result_df,
                                filtered_source_df,
                                left_on='JAN',
                                right_on=key_column,
                                how='left',
                                suffixes=('', f'_{file}')  # é‡è¤‡åˆ—ã®å‡¦ç†
                            )
                            
                            # çµåˆçµæœã®ãƒã‚§ãƒƒã‚¯
                            post_merge_columns = len(result_df.columns)
                            added_columns = post_merge_columns - pre_merge_columns
                            
                            # é‡è¤‡ã‚­ãƒ¼åˆ—ã‚’å‰Šé™¤
                            if key_column != 'JAN' and key_column in result_df.columns:
                                result_df = result_df.drop(columns=[key_column])
                            
                            # å®Ÿéš›ã«ãƒãƒƒãƒã—ãŸãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
                            match_count = 0
                            if added_columns > 0:
                                # è¿½åŠ ã•ã‚ŒãŸæœ€åˆã®åˆ—ã‚’è¦‹ã¤ã‘ã‚‹
                                for col in result_df.columns[-added_columns:]:
                                    if col in result_df.columns:
                                        match_count = result_df[col].notna().sum()
                                        break
                            
                            print(f"âœ… çµåˆå®Œäº†: {len(common_jans)}ä»¶ã®JANãŒãƒãƒƒãƒã€{match_count}è¡Œã®ãƒ‡ãƒ¼ã‚¿ã«æƒ…å ±è¿½åŠ ã€{added_columns}åˆ—è¿½åŠ ")
                            logger.info(f"{file}ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆã—ã¾ã—ãŸ: ãƒãƒƒãƒJAN {len(common_jans)}ä»¶ã€ãƒãƒƒãƒè¡Œ {match_count}ä»¶ã€åˆ—æ•° {added_columns}åˆ—è¿½åŠ ")
                        else:
                            print(f"âš ï¸ ãƒãƒƒãƒã™ã‚‹JANã‚³ãƒ¼ãƒ‰ãŒãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã«æ®‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚çµåˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    else:
                        print(f"âš ï¸ ãƒãƒƒãƒã™ã‚‹JANã‚³ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚çµåˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                else:
                    print(f"âš ï¸ çµåˆã‚’ã‚¹ã‚­ãƒƒãƒ—: ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã«JANåˆ—ãŒã‚ã‚Šã¾ã›ã‚“")
                    logger.warning(f"çµåˆã‚’ã‚¹ã‚­ãƒƒãƒ—: ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã«JANåˆ—ãŒã‚ã‚Šã¾ã›ã‚“")
                
            print(f"âœ… {source_type}ãƒ‡ãƒ¼ã‚¿ã®çµåˆå®Œäº†: ç¾åœ¨ {len(result_df.columns)}åˆ—")
            return result_df
            
        except Exception as e:
            logger.error(f"ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿çµåˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            import traceback
            traceback.print_exc()
            return base_df

    def merge_data(self, sp_df, keepa_df):
        """
        ãƒ‡ãƒ¼ã‚¿ã®çµåˆ
        
        Keepaãƒ‡ãƒ¼ã‚¿ã‚’åŸºæº–ã«ã—ã¦ã€SP-APIãƒ‡ãƒ¼ã‚¿ã‚’ASINã‚­ãƒ¼ã§çµåˆã—ã¾ã™ã€‚
        """
        try:
            # çµåˆå‰ã®æƒ…å ±ã‚’è¡¨ç¤º
            logger.info(f"Keepaãƒ‡ãƒ¼ã‚¿: {len(keepa_df)}ä»¶, SP-APIãƒ‡ãƒ¼ã‚¿: {len(sp_df)}ä»¶")
            print(f"ğŸ“Š çµåˆå‰ - Keepaãƒ‡ãƒ¼ã‚¿: {len(keepa_df)}ä»¶, SP-APIãƒ‡ãƒ¼ã‚¿: {len(sp_df)}ä»¶")
            
            # ä¿®æ­£: SP-APIãƒ‡ãƒ¼ã‚¿ã®å‹ã‚’äº‹å‰ã«ç¢ºèªã—ã¦ä¿®æ­£
            # ç‰¹ã«ã€Œè‡ªå·±ç™ºé€æœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆã€åˆ—ã®å‡¦ç†
            if 'è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ' in sp_df.columns:
                # æ–‡å­—åˆ—ã®'False'ã¨'True'ã‚’æ•°å€¤ã«å¤‰æ›ã—ã¦ã‹ã‚‰å‡¦ç†
                sp_df['è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ'] = sp_df['è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ'].replace(['False', 'True'], [0, 1])
                # NaNå€¤ã‚„Noneå€¤ã‚’ä¸€æ™‚çš„ãªæ•°å€¤ã«ç½®ãæ›ãˆ
                sp_df['è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ'] = sp_df['è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ'].fillna(-999)
                # æ•°å€¤å‹ã«æ˜ç¤ºçš„ã«å¤‰æ›
                sp_df['è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ'] = pd.to_numeric(sp_df['è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ'], errors='coerce')
            
            # Keepaãƒ‡ãƒ¼ã‚¿ã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ã¦ã€ASINã‚’ã‚­ãƒ¼ã«çµåˆï¼ˆå·¦çµåˆï¼‰
            merged_df = pd.merge(
                keepa_df,     # Keepaãƒ‡ãƒ¼ã‚¿ã‚’åŸºæº–ã«ã™ã‚‹
                sp_df,
                on='ASIN',
                how='left',  # Keepaãƒ‡ãƒ¼ã‚¿ã‚’åŸºæº–ã«å·¦çµåˆ
                suffixes=('', '_sp')  # Keepaã®åˆ—åã‚’å„ªå…ˆ
            )
            
            # é‡è¤‡ã™ã‚‹JANåˆ—ã®å‡¦ç†ï¼ˆKeepaãƒ‡ãƒ¼ã‚¿ã‚’å„ªå…ˆï¼‰
            if 'JAN_sp' in merged_df.columns:
                # JANåˆ—ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ã€Keepaãƒ‡ãƒ¼ã‚¿ã®å€¤ã‚’å„ªå…ˆã€ãªã„å ´åˆã¯SP-APIãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
                merged_df['JAN'] = merged_df['JAN'].combine_first(merged_df['JAN_sp'])
                # é‡è¤‡åˆ—ã‚’å‰Šé™¤
                merged_df = merged_df.drop(columns=['JAN_sp'])
                logger.info("JANåˆ—ã‚’çµ±åˆã—ã¾ã—ãŸ (Keepaå„ªå…ˆ)")
            
            # ä¿®æ­£: çµåˆå¾Œã«ã€Œè‡ªå·±ç™ºé€æœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆã€åˆ—ã®ãƒ‡ãƒ¼ã‚¿å‹ã¨NaNå‡¦ç†ã‚’ä¿®æ­£
            if 'è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ' in merged_df.columns:
                # æ–‡å­—åˆ—ã®'False'ã¨'True'ã‚’ã¾ãšæ•°å€¤ã«å¤‰æ›
                merged_df['è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ'] = merged_df['è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ'].replace(['False', 'True'], [0, 1])
                # ãƒ–ãƒ¼ãƒ«å€¤ã‚’æ•°å€¤ã«å¤‰æ›
                merged_df['è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ'] = merged_df['è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ'].replace([False, True], [0, 1])
                # -999ï¼ˆå…ƒã€…ã®NaNï¼‰ã‚’å†ã³Noneã«æˆ»ã™
                merged_df['è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ'] = merged_df['è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ'].replace(-999, None)
                # æ˜ç¤ºçš„ã«æ•°å€¤å‹ã«å¤‰æ›ã—ã¦å‹ã®ä¸€è²«æ€§ã‚’ç¢ºä¿
                merged_df['è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ'] = pd.to_numeric(merged_df['è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ'], errors='coerce')
            
            # JANåˆ—ã®çµ±è¨ˆã‚’è¡¨ç¤º
            if 'JAN' in merged_df.columns:
                jan_count = merged_df['JAN'].notna().sum()
                total_rows = len(merged_df)
                logger.info(f"JANåˆ—ã‚ã‚Š: {jan_count}/{total_rows}ä»¶ ({jan_count/total_rows*100:.1f}%)")
                print(f"â„¹ï¸ JANåˆ—ã‚ã‚Š: {jan_count}/{total_rows}ä»¶ ({jan_count/total_rows*100:.1f}%)")
            
            logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆã—ã¾ã—ãŸ: {len(merged_df)}ä»¶")
            return merged_df
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿çµåˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise

    def rearrange_columns(self, df):
        """
        ã‚«ãƒ©ãƒ ã®ä¸¦ã³æ›¿ãˆ
        
        æŒ‡å®šã•ã‚ŒãŸåˆ—é †åºã®ã‚«ãƒ©ãƒ ã‚’å…ˆé ­ã«é…ç½®ã—ã€ãã‚Œä»¥å¤–ã®ã‚«ãƒ©ãƒ ã¯æœ«å°¾ã«ä¿æŒã—ã¾ã™
        
        Parameters:
        -----------
        df : pandas.DataFrame
            ä¸¦ã³æ›¿ãˆå¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
            
        Returns:
        --------
        pandas.DataFrame
            åˆ—ãŒä¸¦ã³æ›¿ãˆã‚‰ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        """
        try:
            # æœ›ã¾ã—ã„åˆ—é †ã‚’å®šç¾©
            column_order = [
                # åŸºæœ¬æƒ…å ±1
                'ASIN', 'JAN', 'å•†å“å', 'ã‚«ãƒ†ã‚´ãƒªãƒ¼ID', 'ãƒ¡ãƒ¼ã‚«ãƒ¼å‹ç•ª', 'ãƒ¬ãƒ“ãƒ¥ãƒ¼æœ‰ç„¡', 
                'ãƒ¡ãƒ¼ã‚«ãƒ¼å', 'ãƒ–ãƒ©ãƒ³ãƒ‰å', 'ç·å‡ºå“è€…æ•°', 'ã‚»ãƒƒãƒˆæ•°', 'å•†å“è¿½è·¡æ—¥', 
                'å•†å“ç™ºå£²æ—¥', 'è¿½è·¡é–‹å§‹ã‹ã‚‰ã®çµŒéæ—¥æ•°', 'ã‚¢ãƒ€ãƒ«ãƒˆå•†å“å¯¾è±¡',
    
                # åŸºæœ¬æƒ…å ±2
                'å‚è€ƒä¾¡æ ¼', 'ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸æœ€é•·è¾º', 'ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ä¸­è¾º', 'ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸æœ€çŸ­è¾º', 
                'ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸é‡é‡', 'ç¾åœ¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°', '30æ—¥é–“å¹³å‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°', 
                '90æ—¥é–“å¹³å‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°', '180æ—¥é–“å¹³å‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°', 'amazonURL', 
                'KeepaURL', 'ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ASIN',
    
                # ä¾¡æ ¼æƒ…å ±
                'Amazonä¾¡æ ¼', 'ã‚«ãƒ¼ãƒˆä¾¡æ ¼', 'ã‚«ãƒ¼ãƒˆä¾¡æ ¼é€æ–™', 'ã‚«ãƒ¼ãƒˆä¾¡æ ¼ã®ãƒã‚¤ãƒ³ãƒˆ', 
                'ãƒªãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ ï¼ˆæ™‚é–“ï¼‰', 'FBAæœ€å®‰å€¤', 'FBAæœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ', 
                'è‡ªå·±ç™ºé€æœ€å®‰å€¤', 'è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®é€æ–™', 'è‡ªå·±ç™ºé€æœ€å®‰å€¤ã®ãƒã‚¤ãƒ³ãƒˆ', 
                'FBA_è²©å£²æ‰‹æ•°æ–™', 'FBA_é…é€ä»£è¡Œæ‰‹æ•°æ–™',
    
                # å‡ºå“è€…æƒ…å ±
                'amazonæœ¬ä½“æœ‰ç„¡1', 'amazonæœ¬ä½“æœ‰ç„¡2', 'FBAæ•°', 'è‡ªå·±ç™ºé€æ•°', 
                'FBAæœ€å®‰å€¤å‡ºå“è€…æ•°', 'è‡ªå·±ç™ºé€æœ€å®‰å€¤å‡ºå“è€…æ•°', 
                'amazon_30æ—¥é–“åœ¨åº«åˆ‡ã‚Œç‡', 'amazon_90æ—¥é–“åœ¨åº«åˆ‡ã‚Œç‡',
    
                # è²©å£²æ•°æƒ…å ±
                '30æ—¥é–“_ç·è²©å£²æ•°', '30æ—¥é–“_æ–°å“è²©å£²æ•°', '30æ—¥é–“_ä¸­å¤è²©å£²æ•°', 
                '30æ—¥é–“_ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼è²©å£²æ•°', 'Keepa30æ—¥é–“è²©å£²æ•°', 
                '90æ—¥é–“_ç·è²©å£²æ•°', '90æ—¥é–“_æ–°å“è²©å£²æ•°', '90æ—¥é–“_ä¸­å¤è²©å£²æ•°', 
                '90æ—¥é–“_ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼è²©å£²æ•°', 'Keepa90æ—¥é–“è²©å£²æ•°',
                '180æ—¥é–“_ç·è²©å£²æ•°', '180æ—¥é–“_æ–°å“è²©å£²æ•°', '180æ—¥é–“_ä¸­å¤è²©å£²æ•°', 
                '180æ—¥é–“_ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼è²©å£²æ•°', 'Keepa180æ—¥é–“è²©å£²æ•°',
    
                # ä¾¡æ ¼å±¥æ­´
                'amazonä¾¡æ ¼_ç¾åœ¨ä¾¡æ ¼', 'amazonä¾¡æ ¼_æœ€é«˜ä¾¡æ ¼', 'amazonä¾¡æ ¼_æœ€ä½ä¾¡æ ¼',
                'amazonä¾¡æ ¼_30æ—¥å¹³å‡ä¾¡æ ¼', 'amazonä¾¡æ ¼_90æ—¥å¹³å‡ä¾¡æ ¼', 
                'amazonä¾¡æ ¼_180æ—¥å¹³å‡ä¾¡æ ¼', 'æ–°å“ä¾¡æ ¼_ç¾åœ¨ä¾¡æ ¼', 'æ–°å“ä¾¡æ ¼_æœ€é«˜ä¾¡æ ¼',
                'æ–°å“ä¾¡æ ¼_æœ€ä½ä¾¡æ ¼', 'æ–°å“ä¾¡æ ¼_30æ—¥å¹³å‡ä¾¡æ ¼', 'æ–°å“ä¾¡æ ¼_90æ—¥å¹³å‡ä¾¡æ ¼',
                'æ–°å“ä¾¡æ ¼_180æ—¥å¹³å‡ä¾¡æ ¼',
    
                # ãã®ä»– â€»ã€Œãã®ä»–ã€ã®ã‚ã¨ã«åˆ—ãŒè¿½åŠ ã•ã‚Œã€å®šç¾©ã•ã‚Œã¦ã„ãªã„åˆ—ãŒã‚ã‚Šã€ãã®å¾Œã«ãƒ¤ãƒ•ãƒ¼æ¥½å¤©ã®åˆ—ãŒè¿½åŠ ã•ã‚Œã‚‹
                'ç”»åƒURL', 'å…ƒã‚³ãƒ¼ãƒ‰', 'ã‚³ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—'
            ]
            
            # å­˜åœ¨ã™ã‚‹åˆ—ã®ã¿ã‚’æŠ½å‡ºï¼ˆã‚¨ãƒ©ãƒ¼é˜²æ­¢ã®ãŸã‚ï¼‰
            specified_columns = [col for col in column_order if col in df.columns]
            
            # æŒ‡å®šã•ã‚Œã¦ã„ãªã„æ®‹ã‚Šã®åˆ—ï¼ˆè¿½åŠ ã•ã‚ŒãŸåˆ—ãªã©ï¼‰ã‚’å–å¾—
            remaining_columns = [col for col in df.columns if col not in column_order]
            
            # æŒ‡å®šåˆ— + æ®‹ã‚Šã®åˆ—ã®é †ã§æ–°ã—ã„åˆ—é †ã‚’ä½œæˆ
            new_column_order = specified_columns + remaining_columns
            
            # ä¸¦ã³æ›¿ãˆã‚’å®Ÿè¡Œ
            df = df[new_column_order]
            
            # çµæœã‚’ãƒ­ã‚°ã«è¨˜éŒ²
            logger.info(f"ã‚«ãƒ©ãƒ ã‚’ä¸¦ã³æ›¿ãˆã¾ã—ãŸ: æŒ‡å®šåˆ— {len(specified_columns)}åˆ— + è¿½åŠ åˆ— {len(remaining_columns)}åˆ—")
            print(f"ğŸ“Š ã‚«ãƒ©ãƒ ä¸¦ã³æ›¿ãˆ: æŒ‡å®šåˆ— {len(specified_columns)}åˆ— + è¿½åŠ åˆ— {len(remaining_columns)}åˆ—")
            
            return df
            
        except Exception as e:
            logger.error(f"ã‚«ãƒ©ãƒ ä¸¦ã³æ›¿ãˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise

    def save_data(self, df, output_file=None):
        """
        çµ±åˆãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
        
        Parameters:
        -----------
        df : pandas.DataFrame
            ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        output_file : str, optional
            å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ï¼‰
        """
        try:
            # JANã‚³ãƒ¼ãƒ‰ã‚’æ–‡å­—åˆ—ã¨ã—ã¦å‡¦ç†ã™ã‚‹
            if 'JAN' in df.columns:
                df['JAN'] = df['JAN'].astype(str)
                # æµ®å‹•å°æ•°ç‚¹å½¢å¼ã®å ´åˆã¯å…ƒã®å½¢å¼ã«ä¿®æ­£
                df['JAN'] = df['JAN'].replace(r'\.0$', '', regex=True)
                # æŒ‡æ•°è¡¨è¨˜ã‚’ä¿®æ­£
                df['JAN'] = df['JAN'].apply(lambda x: f"{float(x):.0f}" if re.match(r'\d+\.\d+e\+\d+', str(x).lower()) else x)
                
            # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—
            if output_file is None:
                output_file = self.config['data_integration']['output']['output_file']
                
            # ç›¸å¯¾ãƒ‘ã‚¹ã®å ´åˆã¯ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’åŸºæº–ã«ã™ã‚‹
            if not os.path.isabs(output_file):
                output_file = os.path.join(self.data_dir, output_file)
                
            # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèª
            output_dir = os.path.dirname(output_file)
            if not os.path.exists(output_dir):
                os.makedirs(output_dir)
                
            df.to_csv(output_file, index=False, encoding='utf-8-sig')
            logger.info(f"çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_file}")
            print(f"âœ… {len(df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’ {output_file} ã«ä¿å­˜ã—ã¾ã—ãŸ")
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise
    
    def process(self):
        """
        çµ±åˆå‡¦ç†ã®å®Ÿè¡Œ
        
        Keepaãƒ‡ãƒ¼ã‚¿ã‚’åŸºæº–ã«ã—ã¦ã€ä»¥ä¸‹ã®ã‚ˆã†ã«ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã—ã¾ã™ï¼š
        1. Keepaãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
        2. SP-APIãƒ‡ãƒ¼ã‚¿ã‚’ASINã‚­ãƒ¼ã§Keepaãƒ‡ãƒ¼ã‚¿ã«çµåˆ
        3. Yahoo/Rakutenãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€è¿½åŠ ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’JANã‚­ãƒ¼ã§çµåˆ
        
        Returns:
        --------
        pandas.DataFrame
            çµ±åˆå¾Œã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        """
        try:
            # å„ç¨®ãƒ‘ã‚¹ã®ç¢ºèªã¨è¡¨ç¤º
            print(f"ğŸ“‚ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.root_dir}")
            print(f"ğŸ“‚ ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.data_dir}")
            print(f"ğŸ“‚ ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.log_dir}")
            
            # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚“ã ãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¡¨ç¤º
            config = self.config['data_integration']['output']
            print(f"\nğŸ“„ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®æƒ…å ±:")
            print(f"  - SP-APIå…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {config['sp_api_input']}")
            print(f"  - Keepaå…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {config['keepa_input']}")
            print(f"  - å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {config['output_file']}")
            
            # è¿½åŠ ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’è¡¨ç¤º
            sources = self.config['data_integration'].get('sources', [])
            if sources:
                print("\nğŸ“„ è¿½åŠ ã‚½ãƒ¼ã‚¹æƒ…å ±:")
                for i, source in enumerate(sources, 1):
                    source_type = source.get('type', 'ä¸æ˜')
                    files = source.get('files', [])
                    print(f"  ã‚½ãƒ¼ã‚¹{i} ({source_type}): {', '.join(files)}")
            
            # SP-APIã¨Keepaãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
            sp_df, keepa_df = self.load_data()
            
            # SP-APIã¨Keepaãƒ‡ãƒ¼ã‚¿ã®çµåˆ
            merged_df = self.merge_data(sp_df, keepa_df)
            
            # è¿½åŠ ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®çµåˆ
            for source_config in sources:
                source_type = source_config.get('type', 'ä¸æ˜')
                print(f"\nğŸ“Š ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ ({source_type})")
                
                # ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
                source_data = self.load_source_data(source_config)
                
                if source_data:
                    # ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®çµåˆ
                    merged_df = self.merge_source_data(merged_df, source_data, source_config)
                    print(f"âœ… {source_type}ãƒ‡ãƒ¼ã‚¿ã®çµåˆå®Œäº†: ç¾åœ¨ {len(merged_df.columns)}åˆ—")
            
            # ã‚«ãƒ©ãƒ ã®ä¸¦ã³æ›¿ãˆ
            merged_df = self.rearrange_columns(merged_df)
            
            # çµ±åˆãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
            self.save_data(merged_df)
            
            # ä¿®æ­£: çµæœã®çµ±è¨ˆæƒ…å ±ï¼ˆJANåˆ—ã®æœ‰ç„¡ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã‹ã‚‰è¡¨ç¤ºï¼‰
            print("\n=== çµ±åˆçµæœã®æ¦‚è¦ ===")
            print(f"ãƒ»ç·ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(merged_df)}ä»¶")
            if 'JAN' in merged_df.columns:
                jan_count = merged_df['JAN'].notna().sum()
                print(f"ãƒ»JANåˆ—ã‚ã‚Šä»¶æ•°: {jan_count}ä»¶")
            print(f"ãƒ»ã‚«ãƒ©ãƒ æ•°: {len(merged_df.columns)}åˆ—")
            
            output_file = config['output_file']  # å¤‰æ•°ã‚’æ˜ç¤ºçš„ã«å–å¾—
            print(f"\nâœ¨ å‡¦ç†å®Œäº†ï¼ ãƒ‡ãƒ¼ã‚¿ã‚’ {output_file} ã«ä¿å­˜ã—ã¾ã—ãŸ")
            print(f"ï¼ˆKeepaãƒ‡ãƒ¼ã‚¿ã‚’åŸºæº–ã«çµåˆã—ã¾ã—ãŸï¼‰")
            
            return merged_df
            
        except Exception as e:
            logger.error(f"å®Ÿè¡Œæ™‚ã‚¨ãƒ©ãƒ¼: {str(e)}")
            print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            import traceback
            traceback.print_exc()
            return None

# ãƒ†ã‚¹ãƒˆç”¨ã®å®Ÿè¡Œã‚³ãƒ¼ãƒ‰
if __name__ == "__main__":
    # ã‚¤ãƒ³ãƒ†ã‚°ãƒ¬ãƒ¼ã‚¿ãƒ¼ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
    integrator = DataIntegrator()
    
    # çµ±åˆå‡¦ç†ã®å®Ÿè¡Œ
    result_df = integrator.process()
    
    # æˆåŠŸã—ãŸã‹ã©ã†ã‹ã®ç¢ºèª
    if result_df is not None:
        print("âœ… ãƒ‡ãƒ¼ã‚¿çµ±åˆãŒæˆåŠŸã—ã¾ã—ãŸï¼")
    else:
        print("âŒ ãƒ‡ãƒ¼ã‚¿çµ±åˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")