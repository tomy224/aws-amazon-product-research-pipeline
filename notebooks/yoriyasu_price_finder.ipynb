{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926eb4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 開発過程の補足\n",
    "\n",
    "# この Jupyter Notebook ファイルは、開発過程を示すために作成されています。  \n",
    "# セル単位でコードを実行しながら処理内容を確認する「検証用環境」として使用しています。  \n",
    "# 実際の本番環境では VSCode 上でモジュールを作成し、AWS Lambda および Step Functions によりデータ処理を実行しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02a2fa5-af52-432d-9bc2-384993ce8ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# セル1: 共通ベースクラス（BaseScraper）\n",
    "\n",
    "\"\"\"\n",
    "BaseScraper - スクレイピングの基本機能を提供する基底クラス\n",
    "\n",
    "このモジュールはスクレイピングの共通機能を持つ基底クラスを提供します。\n",
    "設定ファイルの読み込み、ブラウザの初期化、ログ機能などの共通機能を集約しています。\n",
    "\"\"\"\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "import yaml\n",
    "import json\n",
    "\n",
    "\n",
    "class BaseScraper:\n",
    "    \"\"\"\n",
    "    スクレイピングの基本機能を提供する基底クラス\n",
    "    \n",
    "    このクラスはスクレイピングに必要な以下の共通機能を提供します:\n",
    "    - 設定ファイルの読み込み\n",
    "    - ブラウザの初期化と設定\n",
    "    - ログ機能\n",
    "    - CSVファイル操作\n",
    "    \"\"\"\n",
    "\n",
    "    def _load_config(self, config_path=None):\n",
    "        \"\"\"\n",
    "        設定ファイルを読み込みます\n",
    "        \n",
    "        Args:\n",
    "            config_path (str): 設定ファイルのパス\n",
    "        \n",
    "        Returns:\n",
    "            dict: 設定データ\n",
    "        \"\"\"\n",
    "        # デフォルトのパス\n",
    "        if config_path is None:\n",
    "            config_path = os.path.join(self.root_dir, 'config', 'settings.yaml')\n",
    "        \n",
    "        print(f\"設定ファイルパス: {config_path}\")\n",
    "        \n",
    "        try:\n",
    "            if not os.path.exists(config_path):\n",
    "                raise FileNotFoundError(f\"設定ファイルが見つかりません: {config_path}\")\n",
    "                \n",
    "            # YAMLファイルを読み込む\n",
    "            with open(config_path, 'r', encoding='utf-8') as f:\n",
    "                config = yaml.safe_load(f)\n",
    "            \n",
    "            # 環境変数から認証情報を取得して設定ファイルにマージ\n",
    "            self._merge_env_variables(config)\n",
    "            \n",
    "            print(\"設定ファイルを正常に読み込みました\")\n",
    "            return config\n",
    "        except Exception as e:\n",
    "            print(f\"設定ファイルの読み込みエラー: {str(e)}\")\n",
    "            # エラーを上位に伝播させる\n",
    "            raise\n",
    "\n",
    "    def load_config(config_path=None):\n",
    "        \"\"\"\n",
    "        設定ファイルを読み込み、環境変数で値を置き換えます\n",
    "        \n",
    "        Args:\n",
    "            config_path (str): 設定ファイルのパス\n",
    "        \n",
    "        Returns:\n",
    "            dict: 設定データ\n",
    "        \"\"\"\n",
    "        # .envファイルを読み込み\n",
    "        load_dotenv()\n",
    "        \n",
    "        # デフォルトのパス\n",
    "        if config_path is None:\n",
    "            # プロジェクトルートを特定\n",
    "            current_dir = os.getcwd()\n",
    "            project_root = current_dir\n",
    "            \n",
    "            # notebooksディレクトリにいる場合はルートに戻る\n",
    "            if os.path.basename(current_dir) == 'notebooks':\n",
    "                project_root = os.path.dirname(current_dir)\n",
    "                \n",
    "            config_path = os.path.join(project_root, \"config\", \"settings.yaml\")\n",
    "        \n",
    "        print(f\"設定ファイルパス: {config_path}\")\n",
    "        \n",
    "        try:\n",
    "            if not os.path.exists(config_path):\n",
    "                raise FileNotFoundError(f\"設定ファイルが見つかりません: {config_path}\")\n",
    "                \n",
    "            with open(config_path, 'r', encoding='utf-8') as f:\n",
    "                # YAMLファイルを読み込む\n",
    "                yaml_content = f.read()\n",
    "                \n",
    "                # 環境変数プレースホルダーを置換\n",
    "                pattern = r'\\$\\{([A-Za-z0-9_]+)\\}'\n",
    "                \n",
    "                def replace_env_var(match):\n",
    "                    env_var = match.group(1)\n",
    "                    return os.environ.get(env_var, f\"${{{env_var}}}\")\n",
    "                \n",
    "                processed_yaml = re.sub(pattern, replace_env_var, yaml_content)\n",
    "                \n",
    "                # 処理済みYAMLを解析\n",
    "                config = yaml.safe_load(processed_yaml)\n",
    "            \n",
    "            print(\"設定ファイルを正常に読み込みました\")\n",
    "            return config\n",
    "        except Exception as e:\n",
    "            print(f\"設定ファイルの読み込みエラー: {str(e)}\")\n",
    "            # エラーを上位に伝播させる\n",
    "            raise\n",
    "\n",
    "    def _merge_env_variables(self, config):\n",
    "        \"\"\"環境変数から認証情報を取得し、設定ファイルにマージする\"\"\"\n",
    "        # このメソッドは子クラスでオーバーライドして実装します\n",
    "        pass\n",
    "\n",
    "    def setup_logging(self):\n",
    "        \"\"\"ログ機能のセットアップ\"\"\"\n",
    "        # すでに存在するハンドラを削除（重複を防ぐため）\n",
    "        for handler in logging.root.handlers[:]:\n",
    "            logging.root.removeHandler(handler)\n",
    "        \n",
    "        # ログファイルパスの設定\n",
    "        log_filename = f'{self.site_name}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'\n",
    "        log_file = os.path.join(self.log_dir, log_filename)\n",
    "        \n",
    "        # 基本設定\n",
    "        logging.basicConfig(\n",
    "            filename=log_file,\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            encoding='utf-8'\n",
    "        )\n",
    "        \n",
    "        # コンソールにもログを出力\n",
    "        console = logging.StreamHandler()\n",
    "        console.setLevel(logging.INFO)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        console.setFormatter(formatter)\n",
    "        logging.getLogger('').addHandler(console)\n",
    "        \n",
    "        # ログファイルの場所を明示的に表示\n",
    "        print(f\"ログファイル出力先: {log_file}\")\n",
    "        logging.info(f\"ログ機能の初期化が完了しました: {log_file}\")\n",
    "\n",
    "    def _find_project_root(self):\n",
    "        \"\"\"\n",
    "        プロジェクトのルートディレクトリを検出する\n",
    "        \"\"\"\n",
    "        # 現在のファイルの絶対パスを取得\n",
    "        current_dir = os.path.abspath(os.getcwd())\n",
    "        \n",
    "        # 親ディレクトリを探索\n",
    "        path = Path(current_dir)\n",
    "        while True:\n",
    "            # .gitディレクトリがあればそれをルートとみなす\n",
    "            if (path / '.git').exists():\n",
    "                return str(path)\n",
    "            \n",
    "            # プロジェクトのルートを示す他のファイル/ディレクトリの存在チェック\n",
    "            if (path / 'setup.py').exists() or (path / 'README.md').exists():\n",
    "                return str(path)\n",
    "            \n",
    "            # これ以上上の階層がない場合は現在のディレクトリを返す\n",
    "            if path.parent == path:\n",
    "                return str(path)\n",
    "            \n",
    "            # 親ディレクトリへ\n",
    "            path = path.parent\n",
    "\n",
    "    def _setup_browser(self):\n",
    "        \"\"\"\n",
    "        Seleniumのブラウザを設定します\n",
    "        \n",
    "        ChromeOptionsの設定とWebDriverの起動を行います\n",
    "        \"\"\"\n",
    "        # Chromeオプション設定\n",
    "        chrome_options = Options()\n",
    "        if self.headless_mode:\n",
    "            chrome_options.add_argument(\"--headless\")  # ヘッドレスモード（画面表示なし）\n",
    "        chrome_options.add_argument(\"--disable-gpu\")  # GPU無効化（ヘッドレス時に推奨）\n",
    "        chrome_options.add_argument(\"--window-size=1920x1080\")  # ウィンドウサイズ設定\n",
    "        \n",
    "        try:\n",
    "            # ドライバーパスが指定されている場合は直接使用\n",
    "            if hasattr(self, 'driver_path') and self.driver_path:\n",
    "                self.browser = webdriver.Chrome(service=Service(self.driver_path), options=chrome_options)\n",
    "            # 指定がなければWebDriverManagerを使用\n",
    "            else:\n",
    "                self.browser = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=chrome_options)\n",
    "            \n",
    "            self.wait = WebDriverWait(self.browser, 10)  # 要素が見つかるまで最大10秒待機\n",
    "            print(\"ブラウザの初期化に成功しました\")\n",
    "        except Exception as e:\n",
    "            print(f\"ブラウザの初期化に失敗しました: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _setup_session(self):\n",
    "        \"\"\"\n",
    "        requestsのセッションを設定します\n",
    "        \n",
    "        Seleniumのブラウザから取得したCookieをrequestsセッションに設定します\n",
    "        これによりログイン状態を維持したままBeautifulSoupでページ取得できます\n",
    "        \"\"\"\n",
    "        # ブラウザのCookieを取得\n",
    "        cookies = self.browser.get_cookies()\n",
    "        \n",
    "        # requestsセッションを作成\n",
    "        self.session = requests.Session()\n",
    "        \n",
    "        # Seleniumから取得したCookieをrequestsセッションに追加\n",
    "        for cookie in cookies:\n",
    "            self.session.cookies.set(cookie['name'], cookie['value'])\n",
    "\n",
    "    def prepare_csv(self):\n",
    "        \"\"\"\n",
    "        CSVファイルを初期化します\n",
    "        \n",
    "        既存のファイルがある場合は削除し、新しいヘッダー付きのCSVを作成します\n",
    "        \"\"\"\n",
    "        # 既存ファイルがあれば削除\n",
    "        if os.path.exists(self.csv_filename):\n",
    "            os.remove(self.csv_filename)\n",
    "        \n",
    "        # カラム構成でCSVを初期化\n",
    "        df = pd.DataFrame(columns=self.columns)\n",
    "        df.to_csv(self.csv_filename, index=False, encoding=\"utf-8-sig\")\n",
    "        \n",
    "        print(f\"CSVファイル {self.csv_filename} を初期化しました\")\n",
    "\n",
    "    def save_to_csv(self, data):\n",
    "        \"\"\"\n",
    "        データをCSVファイルに保存します\n",
    "        \n",
    "        Args:\n",
    "            data (list): 保存するデータ（リストのリスト形式）\n",
    "            \n",
    "        Returns:\n",
    "            bool: 保存成功時はTrue、失敗時はFalse\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if data:\n",
    "                # データフレームに変換\n",
    "                df = pd.DataFrame(data, columns=self.columns)\n",
    "                \n",
    "                # CSVに追記（ヘッダーなし）\n",
    "                df.to_csv(self.csv_filename, mode=\"a\", index=False, header=False, encoding=\"utf-8-sig\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"保存するデータがありません\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print(f\"CSV保存エラー: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def __init__(self, site_name, config_path=None, headless_mode=False):\n",
    "        \"\"\"\n",
    "        BaseScraperの初期化\n",
    "        \n",
    "        Args:\n",
    "            site_name (str): スクレイピングするサイト名（ログファイル名などに使用）\n",
    "            config_path (str): 設定ファイルのパス（指定しない場合はデフォルト値を使用）\n",
    "            headless_mode (bool): ブラウザを画面に表示せずに実行する場合はTrue\n",
    "        \"\"\"\n",
    "        # サイト名の設定\n",
    "        self.site_name = site_name\n",
    "        \n",
    "        # プロジェクトルートディレクトリの検出\n",
    "        self.root_dir = self._find_project_root()\n",
    "        \n",
    "        # 環境変数の読み込み\n",
    "        load_dotenv(os.path.join(self.root_dir, '.env'))\n",
    "        \n",
    "        # ディレクトリパスの設定\n",
    "        self.data_dir = os.path.join(self.root_dir, 'data')\n",
    "        self.log_dir = os.path.join(self.root_dir, 'logs')\n",
    "        \n",
    "        # ディレクトリが存在しない場合は作成\n",
    "        os.makedirs(self.data_dir, exist_ok=True)\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "        \n",
    "        # 設定ファイルの読み込み\n",
    "        self.config = self._load_config(config_path)\n",
    "        \n",
    "        # 基本設定\n",
    "        self.headless_mode = headless_mode\n",
    "        self.browser = None\n",
    "        self.wait = None\n",
    "        self.session = None\n",
    "        \n",
    "        # 実行時間計測用\n",
    "        self.start_time = None\n",
    "        self.end_time = None\n",
    "\n",
    "        # ログ設定を初期化時に行う\n",
    "        self.setup_logging()\n",
    "\n",
    "    def close(self): \n",
    "        \"\"\"\n",
    "        ブラウザを閉じてリソースを解放します\n",
    "        \"\"\"\n",
    "        if self.browser:\n",
    "            self.browser.quit()\n",
    "            print(\"ブラウザを閉じました\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedba342-b5ed-4335-bb30-1b76dfe55211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# セル2: ヨリヤス価格検索クラス（YoriyasuPriceFinder）\n",
    "\n",
    "\"\"\"\n",
    "YoriyasuPriceFinder - ヨリヤスから商品の最安価格情報を取得するモジュール\n",
    "\n",
    "このモジュールはJANコードリストに基づいて、ヨリヤスで各商品の最安値情報を取得します。\n",
    "BaseScraper を継承し、ヨリヤス固有の機能を実装しています。\n",
    "\"\"\"\n",
    "\n",
    "class YoriyasuPriceFinder(BaseScraper):\n",
    "    \"\"\"\n",
    "    ヨリヤスWebサイトから商品の最安値情報を取得するクラス\n",
    "    \n",
    "    JANコードのリストを入力として、各商品の最安値情報（価格、送料、販売サイト等）を取得します。\n",
    "    \"\"\"\n",
    "    \n",
    "    def _merge_env_variables(self, config):\n",
    "        \"\"\"環境変数から設定情報を取得し、設定ファイルにマージする\"\"\"\n",
    "        # ヨリヤスはログイン不要なのでその部分は省略\n",
    "        \n",
    "        # デフォルトの出力設定（なければ設定）\n",
    "        if 'price_finder' not in config:\n",
    "            config['price_finder'] = {}\n",
    "        if 'yoriyasu' not in config['price_finder']:\n",
    "            config['price_finder']['yoriyasu'] = {}\n",
    "        if 'output' not in config['price_finder']['yoriyasu']:\n",
    "            config['price_finder']['yoriyasu']['output'] = {\n",
    "                'input_file': 'jan_list.csv',  # JANコードリストの入力ファイル\n",
    "                'output_file': 'yoriyasu_prices.csv',  # 出力ファイル\n",
    "                'log_dir': 'logs'\n",
    "            }\n",
    "    \n",
    "    def __init__(self, config_path=None, headless_mode=False):\n",
    "        \"\"\"\n",
    "        YoriyasuPriceFinderの初期化\n",
    "        \n",
    "        Args:\n",
    "            config_path (str): 設定ファイルのパス（指定しない場合はデフォルト値を使用）\n",
    "            headless_mode (bool): ブラウザを画面に表示せずに実行する場合はTrue\n",
    "        \"\"\"\n",
    "        # 親クラス(BaseScraper)の初期化\n",
    "        super().__init__('yoriyasu_price', config_path, headless_mode)\n",
    "        \n",
    "        # ヨリヤス固有の設定\n",
    "        self.yoriyasu_config = self.config['price_finder']['yoriyasu']\n",
    "        self.base_url = \"https://yoriyasu.jp/products\"\n",
    "        \n",
    "        # 出力設定（設定ファイルから読み込み）\n",
    "        output_config = self.yoriyasu_config.get('output', {})\n",
    "        self.input_file = os.path.join(self.data_dir, output_config.get('input_file', 'jan_list.csv'))\n",
    "        csv_filename = output_config.get('output_file', 'yoriyasu_prices.csv')\n",
    "        \n",
    "        # CSVのフルパスを設定\n",
    "        self.csv_filename = os.path.join(self.data_dir, csv_filename)\n",
    "        print(f\"入力ファイル: {self.input_file}\")\n",
    "        print(f\"CSVファイル出力先: {self.csv_filename}\")\n",
    "        \n",
    "        # 1行に複数サイト情報を含めるように変更\n",
    "        # 4サイト分のカラムを用意\n",
    "        self.max_sites = 4  # 保存する最大サイト数\n",
    "        \n",
    "        # カラム設定（水平方向に拡張）\n",
    "        self.columns = [\"JANコード\"]\n",
    "        for i in range(1, self.max_sites + 1):\n",
    "            self.columns.extend([\n",
    "                f\"商品名{i}\", f\"仕入価格{i}\", f\"送料{i}\", \n",
    "                f\"出品者名{i}\", f\"仕入れサイト{i}\", f\"仕入れURL{i}\"\n",
    "            ])\n",
    "        \n",
    "        # ブラウザ設定\n",
    "        self._setup_browser()\n",
    "        \n",
    "        # 待機時間のランダム化用\n",
    "        import random\n",
    "        self.random = random\n",
    "\n",
    "        # 監視用のカウンター\n",
    "        self.total_requests = 0\n",
    "        self.failed_requests = 0\n",
    "        self.status_codes = {}\n",
    "        self.avg_response_time = 0\n",
    "\n",
    "        self.last_request_time = None\n",
    "        self.request_intervals = []\n",
    "        \n",
    "    \n",
    "    def load_jan_codes(self):\n",
    "        \"\"\"\n",
    "        JANコードリストを読み込みます\n",
    "        \n",
    "        Returns:\n",
    "            list: JANコードのリスト\n",
    "        \"\"\"\n",
    "        jan_codes = []\n",
    "        \n",
    "        try:\n",
    "            # ファイルの存在確認\n",
    "            if not os.path.exists(self.input_file):\n",
    "                raise FileNotFoundError(f\"JANコードリストファイルが見つかりません: {self.input_file}\")\n",
    "            \n",
    "            # CSVからJANコードを読み込む\n",
    "            import csv\n",
    "            with open(self.input_file, 'r', encoding='utf-8-sig') as f:\n",
    "                reader = csv.reader(f)\n",
    "                # ヘッダー行があるかチェック\n",
    "                header = next(reader, None)\n",
    "                \n",
    "                # ヘッダー行があれば、JANコードのカラムを探す\n",
    "                jan_column_index = 0\n",
    "                if header:\n",
    "                    for i, col in enumerate(header):\n",
    "                        if 'JAN' in col.upper() or 'コード' in col or 'CODE' in col.upper():\n",
    "                            jan_column_index = i\n",
    "                            break\n",
    "                \n",
    "                # JANコードを読み込む\n",
    "                for row in reader:\n",
    "                    if row and len(row) > jan_column_index:\n",
    "                        jan_code = row[jan_column_index].strip()\n",
    "                        if jan_code and jan_code.isdigit() and len(jan_code) >= 8:  # 有効なJANコードのみ\n",
    "                            jan_codes.append(jan_code)\n",
    "            \n",
    "            print(f\"{len(jan_codes)}件のJANコードを読み込みました\")\n",
    "            return jan_codes\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"JANコード読み込みエラー: {str(e)}\")\n",
    "            logging.error(f\"JANコード読み込みエラー: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def get_price_info(self, jan_code):\n",
    "        \"\"\"\n",
    "        指定されたJANコードで検索し、最安値情報を取得します\n",
    "        \n",
    "        Args:\n",
    "            jan_code (str): 検索するJANコード\n",
    "            \n",
    "        Returns:\n",
    "            list: 商品の最安値情報のリスト（1行に複数サイト情報）\n",
    "        \"\"\"\n",
    "        price_data_row = [jan_code]  # 1行のデータ（JANコードから始まる）\n",
    "        search_url = f\"{self.base_url}?sort=priceLow&page=1&keyword={jan_code}\"\n",
    "        print(f\"JANコード {jan_code} の最安値検索中... ({search_url})\")\n",
    "        \n",
    "        try:\n",
    "            # 検索ページにアクセス\n",
    "            self.browser.get(search_url)\n",
    "            \n",
    "            # より確実な待機 - ページ全体が読み込まれるまで待機（最大15秒） 「EC.visibility_of_element_located」があるのでコメントアウト\n",
    "            # WebDriverWait(self.browser, 15).until(\n",
    "            #     EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "            # )\n",
    "\n",
    "            WebDriverWait(self.browser, 15).until(\n",
    "                EC.visibility_of_element_located((By.CLASS_NAME, \"styles_priceInfo__kzr_s\"))\n",
    "            ) # サイト情報ボックスの要素が表示されるまで待機\n",
    "            \n",
    "            # # 追加の待機時間（JavaScriptの実行を待つ） 「EC.visibility_of_element_located」があるのでコメントアウト\n",
    "            # time.sleep(0.3)\n",
    "            \n",
    "            # ページのHTMLを取得\n",
    "            html_content = self.browser.page_source\n",
    "            print(f\"ページHTMLを取得しました（長さ: {len(html_content)}文字）\")\n",
    "            \n",
    "            # BeautifulSoupでHTMLを解析\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            \n",
    "            # 「0件」かどうかチェック\n",
    "            no_results = soup.find(string=lambda s: \"（0件）\" in s if s else False)\n",
    "            if no_results:\n",
    "                print(f\"JANコード {jan_code} の商品は見つかりませんでした（0件）\")\n",
    "                \n",
    "                # 「0件」の場合はエラーとしてカウント\n",
    "                self.total_requests += 1\n",
    "                self.failed_requests += 1\n",
    "                \n",
    "                # 「0件」の場合は通常より長めに待機（2秒）\n",
    "                print(\"「0件」結果のため、追加で2秒待機します...\")\n",
    "                time.sleep(2.0)\n",
    "                \n",
    "                return []\n",
    "            \n",
    "            # 商品リストを取得 - style属性で特定する\n",
    "            product_stacks = soup.find_all(\"div\", style=lambda s: \"width: 100%; gap: calc(0.425rem); position: relative;\" in s if s else False)\n",
    "            \n",
    "            # 商品リストが見つからない場合、別の方法も試す\n",
    "            if not product_stacks:\n",
    "                # クラス名の部分一致でも試してみる\n",
    "                product_stacks = soup.find_all(class_=lambda c: \"styles_stack__\" in c if c else False)\n",
    "                print(f\"style属性での検索失敗、クラス名部分一致で {len(product_stacks)} 件見つかりました\")\n",
    "            \n",
    "            # 商品リストが見つからない場合\n",
    "            if not product_stacks:\n",
    "                print(f\"JANコード {jan_code} の商品リストが見つかりませんでした\")\n",
    "                return []\n",
    "            \n",
    "            # 商品スタックを選択（最初のスタックが通常は商品リスト）\n",
    "            product_stack = product_stacks[0]\n",
    "            \n",
    "            # 各商品のaタグを取得 - 直接の子要素として探す\n",
    "            product_links = product_stack.find_all(\"a\", href=True, recursive=True)\n",
    "            \n",
    "            print(f\"商品リンク数: {len(product_links)}\")\n",
    "            \n",
    "            if not product_links:\n",
    "                # 別の方法でaタグを探す - 階層を気にせず全て探す\n",
    "                product_links = soup.find_all(\"a\", href=lambda h: \"amazon.co.jp\" in h or \n",
    "                                                               \"rakuten.co.jp\" in h or \n",
    "                                                               \"yahoo.co.jp\" in h or \n",
    "                                                               \"wowma.jp\" in h or\n",
    "                                                               \"valuecommerce.com\" in h or\n",
    "                                                               \"linksynergy.com\" in h\n",
    "                                              if h else False)\n",
    "                print(f\"別の方法で見つけた商品リンク数: {len(product_links)}\")\n",
    "                \n",
    "                if not product_links:\n",
    "                    print(f\"JANコード {jan_code} の商品リンクが見つかりませんでした\")\n",
    "                    return []\n",
    "            \n",
    "            # 最大サイト数に制限\n",
    "            product_links = product_links[:self.max_sites]\n",
    "            site_count = 0\n",
    "            \n",
    "            # 各商品の情報を抽出\n",
    "            for product_link in product_links:\n",
    "                try:\n",
    "                    # サイト番号（1から始まる）\n",
    "                    site_count += 1\n",
    "                    \n",
    "                    # 商品URL - アフィリエイトリンクの場合は対応\n",
    "                    raw_url = product_link[\"href\"]\n",
    "                    \n",
    "                    # 実際の商品URLを抽出（アフィリエイトリンクの場合）\n",
    "                    if \"amazon.co.jp\" in raw_url:\n",
    "                        # Amazonのアフィリエイトリンク処理\n",
    "                        import re\n",
    "                        amazon_id_match = re.search(r'dp/([A-Za-z0-9]+)', raw_url)\n",
    "                        if amazon_id_match:\n",
    "                            product_id = amazon_id_match.group(1)\n",
    "                            # 複数商品表示用のリンクに変換\n",
    "                            product_url = f\"https://www.amazon.co.jp/gp/offer-listing/{product_id}\"\n",
    "                        else:\n",
    "                            product_url = raw_url\n",
    "                    elif \"valuecommerce.com\" in raw_url and \"vc_url=\" in raw_url:\n",
    "                        # ValueCommerceのアフィリエイトリンク（ヤフーショッピングやLOHACO）\n",
    "                        import urllib.parse\n",
    "                        vc_url_param = re.search(r'vc_url=([^&]+)', raw_url)\n",
    "                        if vc_url_param:\n",
    "                            product_url = urllib.parse.unquote(vc_url_param.group(1))\n",
    "                        else:\n",
    "                            product_url = raw_url\n",
    "                    elif \"rakuten.co.jp\" in raw_url or \"hb.afl.rakuten.co.jp\" in raw_url:\n",
    "                        # 楽天のアフィリエイトリンク\n",
    "                        import urllib.parse\n",
    "                        pc_param = re.search(r'pc=([^&]+)', raw_url)\n",
    "                        if pc_param:\n",
    "                            product_url = urllib.parse.unquote(pc_param.group(1))\n",
    "                        else:\n",
    "                            # 別の形式の楽天リンクの可能性も確認\n",
    "                            rakuten_item_match = re.search(r'(https://item.rakuten.co.jp/[^/]+/[^/&?]+)', raw_url)\n",
    "                            if rakuten_item_match:\n",
    "                                product_url = rakuten_item_match.group(1)\n",
    "                            else:\n",
    "                                product_url = raw_url\n",
    "                    elif \"linksynergy.com\" in raw_url and \"murl=\" in raw_url:\n",
    "                        # LinkSynergyのアフィリエイトリンク（auPAY等）\n",
    "                        import urllib.parse\n",
    "                        murl_param = re.search(r'murl=([^&]+)', raw_url)\n",
    "                        if murl_param:\n",
    "                            product_url = urllib.parse.unquote(murl_param.group(1))\n",
    "                        else:\n",
    "                            product_url = raw_url\n",
    "                    else:\n",
    "                        # その他のリンク（通常のURLまたは未知のアフィリエイトリンク）\n",
    "                        product_url = raw_url\n",
    "                    \n",
    "                    print(f\"商品URL{site_count}: {product_url}\")\n",
    "                    \n",
    "                    # 商品名 - 複数の可能性のあるクラスで探す\n",
    "                    name_element = product_link.find(class_=lambda c: \"saleName\" in c if c else False) or \\\n",
    "                                   product_link.find(\"p\")  # フォールバック: 最初のp要素\n",
    "                    \n",
    "                    if not name_element:\n",
    "                        print(f\"商品名要素{site_count}が見つかりません\")\n",
    "                        # 空の値でデータを追加（行の構造を維持するため）\n",
    "                        price_data_row.extend([\"\", \"\", \"\", \"\", \"\", \"\"])\n",
    "                        continue\n",
    "                        \n",
    "                    product_name = name_element.text.strip()\n",
    "                    print(f\"商品名{site_count}: {product_name}\")\n",
    "                    \n",
    "                    # 価格情報を含む要素 - flexスタイルで探す（複数の方法で試行）\n",
    "                    price_container = product_link.find(\"div\", style=lambda s: \"flex-basis: 30%\" in s if s else False) or \\\n",
    "                                     product_link.find(\"div\", style=lambda s: \"flex-grow: 1\" in s if s else False)\n",
    "                    \n",
    "                    if not price_container:\n",
    "                        print(f\"価格コンテナ{site_count}が見つかりません\")\n",
    "                        # 空の値でデータを追加（行の構造を維持するため）\n",
    "                        price_data_row.extend([product_name, \"\", \"\", \"\", \"\", product_url])\n",
    "                        continue\n",
    "                    \n",
    "                    # 送料情報 - クラス名で探す（複数の可能性を試す）\n",
    "                    shipping_element = price_container.find(class_=lambda c: \"shipping\" in c.lower() if c else False) or \\\n",
    "                                      price_container.find(\"span\")  # フォールバック: 最初のspan\n",
    "                    shipping = shipping_element.text.strip() if shipping_element else \"不明\"\n",
    "                    print(f\"送料{site_count}: {shipping}\")\n",
    "                    \n",
    "                    # 価格情報 - クラス名で探す（複数の可能性を試す）\n",
    "                    price_element = price_container.find(class_=lambda c: \"price\" in c.lower() if c else False) or \\\n",
    "                                   price_container.find(\"span\", recursive=True)  # フォールバック\n",
    "                    \n",
    "                    price_value = \"\"\n",
    "                    if price_element:\n",
    "                        # 価格から数値のみを抽出\n",
    "                        price_text = price_element.text.strip()\n",
    "                        price_match = re.search(r'[¥￥]\\s*([\\d,]+)', price_text)\n",
    "                        price_value = price_match.group(1).replace(',', '') if price_match else \"\"\n",
    "                        print(f\"価格{site_count}: {price_value}\")\n",
    "                    \n",
    "                    # 出品者とサイト情報を含む要素 - スタイルで探す\n",
    "                    shop_container = product_link.find(\"div\", style=lambda s: \"flex-grow: 999\" in s if s else False) or \\\n",
    "                                    product_link.find(\"div\", style=lambda s: \"min-width: 70%\" in s if s else False)\n",
    "                    \n",
    "                    shop_name = \"不明\"\n",
    "                    site_name = \"不明\"\n",
    "                    \n",
    "                    if shop_container:\n",
    "                        # 出品者情報のコンテナ\n",
    "                        shop_info = shop_container.find(class_=lambda c: \"shopInfo\" in c if c else False) or \\\n",
    "                                   shop_container.find(\"div\")  # フォールバック\n",
    "                        \n",
    "                        if shop_info:\n",
    "                            # 出品者名\n",
    "                            shop_name_element = shop_info.find(class_=lambda c: \"shopName\" in c if c else False) or \\\n",
    "                                              shop_info.find(\"span\")  # フォールバック\n",
    "                            shop_name = shop_name_element.text.strip() if shop_name_element else \"不明\"\n",
    "                            \n",
    "                            # 仕入れサイト（画像のalt属性）\n",
    "                            site_image = shop_info.find(\"img\")\n",
    "                            site_name = site_image.get(\"alt\", \"不明\") if site_image else \"不明\"\n",
    "                    \n",
    "                    print(f\"出品者名{site_count}: {shop_name}, 仕入れサイト{site_count}: {site_name}\")\n",
    "                    \n",
    "                    # データ行に追加\n",
    "                    price_data_row.extend([\n",
    "                        product_name,    # 商品名\n",
    "                        price_value,     # 仕入価格\n",
    "                        shipping,        # 送料\n",
    "                        shop_name,       # 出品者名\n",
    "                        site_name,       # 仕入れサイト\n",
    "                        product_url      # 仕入れURL\n",
    "                    ])\n",
    "                    \n",
    "                    print(f\"サイト{site_count}のデータを追加しました\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"サイト{site_count}の情報抽出エラー: {str(e)}\")\n",
    "                    logging.error(f\"サイト{site_count}の情報抽出エラー: {str(e)}\")\n",
    "                    logging.error(traceback.format_exc())\n",
    "                    \n",
    "                    # エラーが発生した場合も、行の構造を維持するために空の値を追加\n",
    "                    price_data_row.extend([\"\", \"\", \"\", \"\", \"\", \"\"])\n",
    "                    continue\n",
    "            \n",
    "            # 不足している分のサイト情報を空データで埋める\n",
    "            while site_count < self.max_sites:\n",
    "                site_count += 1\n",
    "                price_data_row.extend([\"\", \"\", \"\", \"\", \"\", \"\"])\n",
    "            \n",
    "            # 1行分のデータを返す（1JANコードに対して1行）\n",
    "            if len(price_data_row) > 1:  # JANコード以外のデータがある場合\n",
    "                return [price_data_row]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"検索ページアクセスエラー (JANコード {jan_code}): {str(e)}\")\n",
    "            logging.error(f\"検索ページアクセスエラー (JANコード {jan_code}): {str(e)}\")\n",
    "            logging.error(traceback.format_exc())  # 詳細なエラーログ\n",
    "        \n",
    "        return []\n",
    "\n",
    "\n",
    "    def monitor_request(self, url):\n",
    "        \"\"\"\n",
    "        HTTPリクエストを監視し、ステータスコード、レスポンスタイム、エラーをチェックする\n",
    "        \n",
    "        Args:\n",
    "            url (str): リクエスト先のURL\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (成功したかどうか, ステータスコード, レスポンスタイム)\n",
    "        \"\"\"\n",
    "        import time\n",
    "        import requests\n",
    "        \n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            # Seleniumのブラウザは既にURLにアクセスしているため\n",
    "            # 別途requestsで直接アクセスして状態をチェック\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "            }\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            status_code = response.status_code\n",
    "            success = 200 <= status_code < 400\n",
    "        except Exception as e:\n",
    "            logging.error(f\"リクエスト監視エラー: {str(e)}\")\n",
    "            status_code = 0\n",
    "            success = False\n",
    "        \n",
    "        response_time = time.time() - start_time\n",
    "        \n",
    "        # 結果をログに記録\n",
    "        log_msg = f\"URL: {url}, ステータス: {status_code}, 応答時間: {response_time:.2f}秒, 成功: {success}\"\n",
    "        if success:\n",
    "            logging.info(log_msg)\n",
    "        else:\n",
    "            logging.warning(log_msg)\n",
    "        \n",
    "        return success, status_code, response_time\n",
    "    \n",
    "    \n",
    "\n",
    "    def check_error_rate(self):\n",
    "        \"\"\"\n",
    "        エラー率を計算し、高すぎる場合は警告またはスクレイピングを一時停止する\n",
    "        \n",
    "        Returns:\n",
    "            bool: 続行可能な場合はTrue、一時停止すべき場合はFalse\n",
    "        \"\"\"\n",
    "        if self.total_requests == 0:\n",
    "            return True\n",
    "        \n",
    "        error_rate = self.failed_requests / self.total_requests\n",
    "        \n",
    "        # エラー率をログに記録\n",
    "        logging.info(f\"現在のリクエスト数: {self.total_requests}, エラー数: {self.failed_requests}, エラー率: {error_rate:.2%}\")\n",
    "        logging.info(f\"ステータスコード分布: {self.status_codes}\")\n",
    "        logging.info(f\"平均応答時間: {self.avg_response_time:.2f}秒\")\n",
    "        \n",
    "        # エラー率が30%を超える場合は警告\n",
    "        if error_rate > 0.3:\n",
    "            logging.warning(f\"エラー率が高すぎます ({error_rate:.2%}). IPBANの可能性があります。\")\n",
    "            \n",
    "            # エラー率が50%を超える場合は一時停止を推奨\n",
    "            if error_rate > 0.5:\n",
    "                logging.error(\"エラー率が50%を超えました。スクレイピングを一時停止します。\")\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "\n",
    "    def refresh_session(self):\n",
    "        \"\"\"\n",
    "        ブラウザのセッションをリフレッシュし、新しいCookieを取得する\n",
    "        \"\"\"\n",
    "        logging.info(\"ブラウザセッションをリフレッシュします...\")\n",
    "        \n",
    "        try:\n",
    "            # 現在のブラウザを閉じる\n",
    "            if self.browser:\n",
    "                self.browser.quit()\n",
    "            \n",
    "            # 新しいブラウザを起動\n",
    "            self._setup_browser()\n",
    "            \n",
    "            # ホームページにアクセスして新しいCookieを取得\n",
    "            self.browser.get(\"https://yoriyasu.jp/\")\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # リクエスト関連の統計をリセット\n",
    "            self.total_requests = 0\n",
    "            self.failed_requests = 0\n",
    "            self.status_codes = {}\n",
    "            self.avg_response_time = 0\n",
    "            \n",
    "            logging.info(\"ブラウザセッションのリフレッシュに成功しました\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logging.error(f\"セッションリフレッシュエラー: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    \n",
    "    def find_all_prices(self):\n",
    "        \"\"\"\n",
    "        すべてのJANコードに対して最安値情報を検索します\n",
    "        \n",
    "        Returns:\n",
    "            int: 取得した商品データの行数\n",
    "        \"\"\"\n",
    "        # 実行時間測定開始\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "        # CSVを初期化\n",
    "        self.prepare_csv()\n",
    "        \n",
    "        # JANコードの読み込み\n",
    "        jan_codes = self.load_jan_codes()\n",
    "        if not jan_codes:\n",
    "            print(\"有効なJANコードがありません。処理を終了します。\")\n",
    "            return 0\n",
    "        \n",
    "        # 各JANコードの処理\n",
    "        total_rows = 0\n",
    "        request_count = 0  # リクエスト数のカウンター\n",
    "        \n",
    "        # 進捗表示用\n",
    "        total_jan_codes = len(jan_codes)\n",
    "        \n",
    "        for i, jan_code in enumerate(jan_codes, 1):\n",
    "            print(f\"\\n----- JANコード {jan_code} の処理中 ({i}/{total_jan_codes}) -----\")\n",
    "\n",
    "            # リクエスト間隔を測定\n",
    "            current_time = time.time()\n",
    "            if self.last_request_time is not None:\n",
    "                interval = current_time - self.last_request_time\n",
    "                self.request_intervals.append(interval)\n",
    "                avg_interval = sum(self.request_intervals) / len(self.request_intervals)\n",
    "                print(f\"前回のリクエストから {interval:.2f}秒、平均間隔: {avg_interval:.2f}秒\")\n",
    "            self.last_request_time = current_time\n",
    "            \n",
    "            \n",
    "            # 100件ごとにセッションをリフレッシュ\n",
    "            request_count += 1\n",
    "            if request_count > 50:\n",
    "                print(\"50件のリクエスト完了。セッションをリフレッシュします...\")\n",
    "                self.refresh_session()\n",
    "                request_count = 0\n",
    "            \n",
    "            # 最安値情報の取得（1行のデータを取得）\n",
    "            price_data = self.get_price_info(jan_code)\n",
    "            \n",
    "            # エラー率をチェック\n",
    "            if not self.check_error_rate():\n",
    "                print(\"エラー率が高すぎるため、一時停止します。15分後に再開してください。\")\n",
    "                logging.error(\"エラー率が閾値を超えたため、スクレイピングを中断します。\")\n",
    "                break\n",
    "            \n",
    "            # データを保存\n",
    "            if price_data:\n",
    "                self.save_to_csv(price_data)\n",
    "                total_rows += len(price_data)\n",
    "                print(f\"JANコード {jan_code} の最安値データを保存しました\")\n",
    "            \n",
    "            # 連続アクセスによるブロック防止のための待機（少しランダム化）\n",
    "            if i < total_jan_codes:\n",
    "                wait_time = 0.0 + self.random.random() * 0.4 \n",
    "                print(f\"{wait_time:.2f}秒待機中...\")\n",
    "                time.sleep(wait_time)\n",
    "        \n",
    "        # 実行時間測定終了\n",
    "        self.end_time = time.time()\n",
    "        elapsed_time = self.end_time - self.start_time\n",
    "        \n",
    "        print(f\"\\n===== 最安値検索完了 - 合計 {total_rows} 件 ====\")\n",
    "        print(f\"実行時間: {elapsed_time:.2f} 秒\")\n",
    "        \n",
    "        # リクエスト統計\n",
    "        if self.request_intervals:\n",
    "            avg_interval = sum(self.request_intervals) / len(self.request_intervals)\n",
    "            min_interval = min(self.request_intervals)\n",
    "            max_interval = max(self.request_intervals)\n",
    "            print(f\"リクエスト間隔: 平均 {avg_interval:.2f}秒 (最小 {min_interval:.2f}秒、最大 {max_interval:.2f}秒)\")\n",
    "            print(f\"リクエスト数: {len(self.request_intervals) + 1}\")\n",
    "        \n",
    "        return total_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1c3d57-ca0b-4697-9dbf-a1b59770e4bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# セル3: 価格検索のインスタンスを作成して実行するコード\n",
    "if __name__ == \"__main__\":\n",
    "    import traceback  # 詳細なエラーログ用\n",
    "    \n",
    "    # 価格検索のインスタンスを作成\n",
    "    price_finder = YoriyasuPriceFinder(headless_mode=False)\n",
    "\n",
    "    try:\n",
    "        # JANコードリストに基づいて全商品の最安値情報を検索\n",
    "        price_finder.find_all_prices()\n",
    "    finally:\n",
    "        # 終了処理\n",
    "        price_finder.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6f2044-f6c5-4fc2-8675-a97408855cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
