{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6134fe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 開発過程の補足\n",
    "\n",
    "# この Jupyter Notebook ファイルは、開発過程を示すために作成されています。  \n",
    "# セル単位でコードを実行しながら処理内容を確認する「検証用環境」として使用しています。  \n",
    "# 実際の本番環境では VSCode 上でモジュールを作成し、AWS Lambda および Step Functions によりデータ処理を実行しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556d8c15-6667-4b05-9562-52af5826e3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# セル1: 共通ベースクラス（BaseScraper）\n",
    "\n",
    "\"\"\"\n",
    "BaseScraper - スクレイピングの基本機能を提供する基底クラス\n",
    "\n",
    "このモジュールはスクレイピングの共通機能を持つ基底クラスを提供します。\n",
    "設定ファイルの読み込み、ブラウザの初期化、ログ機能などの共通機能を集約しています。\n",
    "\"\"\"\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "import yaml\n",
    "import json\n",
    "\n",
    "\n",
    "class BaseScraper:\n",
    "    \"\"\"\n",
    "    スクレイピングの基本機能を提供する基底クラス\n",
    "    \n",
    "    このクラスはスクレイピングに必要な以下の共通機能を提供します:\n",
    "    - 設定ファイルの読み込み\n",
    "    - ブラウザの初期化と設定\n",
    "    - ログ機能\n",
    "    - CSVファイル操作\n",
    "    \"\"\"\n",
    "\n",
    "    def _load_config(self, config_path=None):\n",
    "        \"\"\"\n",
    "        設定ファイルを読み込みます\n",
    "        \n",
    "        Args:\n",
    "            config_path (str): 設定ファイルのパス\n",
    "        \n",
    "        Returns:\n",
    "            dict: 設定データ\n",
    "        \"\"\"\n",
    "        # デフォルトのパス\n",
    "        if config_path is None:\n",
    "            config_path = os.path.join(self.root_dir, 'config', 'settings.yaml')\n",
    "        \n",
    "        print(f\"設定ファイルパス: {config_path}\")\n",
    "        \n",
    "        try:\n",
    "            if not os.path.exists(config_path):\n",
    "                raise FileNotFoundError(f\"設定ファイルが見つかりません: {config_path}\")\n",
    "                \n",
    "            # YAMLファイルを読み込む\n",
    "            with open(config_path, 'r', encoding='utf-8') as f:\n",
    "                config = yaml.safe_load(f)\n",
    "            \n",
    "            # 環境変数から認証情報を取得して設定ファイルにマージ\n",
    "            self._merge_env_variables(config)\n",
    "            \n",
    "            print(\"設定ファイルを正常に読み込みました\")\n",
    "            return config\n",
    "        except Exception as e:\n",
    "            print(f\"設定ファイルの読み込みエラー: {str(e)}\")\n",
    "            # エラーを上位に伝播させる\n",
    "            raise\n",
    "\n",
    "    def load_config(config_path=None):\n",
    "        \"\"\"\n",
    "        設定ファイルを読み込み、環境変数で値を置き換えます\n",
    "        \n",
    "        Args:\n",
    "            config_path (str): 設定ファイルのパス\n",
    "        \n",
    "        Returns:\n",
    "            dict: 設定データ\n",
    "        \"\"\"\n",
    "        # .envファイルを読み込み\n",
    "        load_dotenv()\n",
    "        \n",
    "        # デフォルトのパス\n",
    "        if config_path is None:\n",
    "            # プロジェクトルートを特定\n",
    "            current_dir = os.getcwd()\n",
    "            project_root = current_dir\n",
    "            \n",
    "            # notebooksディレクトリにいる場合はルートに戻る\n",
    "            if os.path.basename(current_dir) == 'notebooks':\n",
    "                project_root = os.path.dirname(current_dir)\n",
    "                \n",
    "            config_path = os.path.join(project_root, \"config\", \"settings.yaml\")\n",
    "        \n",
    "        print(f\"設定ファイルパス: {config_path}\")\n",
    "        \n",
    "        try:\n",
    "            if not os.path.exists(config_path):\n",
    "                raise FileNotFoundError(f\"設定ファイルが見つかりません: {config_path}\")\n",
    "                \n",
    "            with open(config_path, 'r', encoding='utf-8') as f:\n",
    "                # YAMLファイルを読み込む\n",
    "                yaml_content = f.read()\n",
    "                \n",
    "                # 環境変数プレースホルダーを置換\n",
    "                pattern = r'\\$\\{([A-Za-z0-9_]+)\\}'\n",
    "                \n",
    "                def replace_env_var(match):\n",
    "                    env_var = match.group(1)\n",
    "                    return os.environ.get(env_var, f\"${{{env_var}}}\")\n",
    "                \n",
    "                processed_yaml = re.sub(pattern, replace_env_var, yaml_content)\n",
    "                \n",
    "                # 処理済みYAMLを解析\n",
    "                config = yaml.safe_load(processed_yaml)\n",
    "            \n",
    "            print(\"設定ファイルを正常に読み込みました\")\n",
    "            return config\n",
    "        except Exception as e:\n",
    "            print(f\"設定ファイルの読み込みエラー: {str(e)}\")\n",
    "            # エラーを上位に伝播させる\n",
    "            raise\n",
    "\n",
    "    def _merge_env_variables(self, config):\n",
    "        \"\"\"環境変数から認証情報を取得し、設定ファイルにマージする\"\"\"\n",
    "        # このメソッドは子クラスでオーバーライドして実装します\n",
    "        pass\n",
    "\n",
    "    def setup_logging(self):\n",
    "        \"\"\"ログ機能のセットアップ\"\"\"\n",
    "        # すでに存在するハンドラを削除（重複を防ぐため）\n",
    "        for handler in logging.root.handlers[:]:\n",
    "            logging.root.removeHandler(handler)\n",
    "        \n",
    "        # ログファイルパスの設定\n",
    "        log_filename = f'{self.site_name}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'\n",
    "        log_file = os.path.join(self.log_dir, log_filename)\n",
    "        \n",
    "        # 基本設定\n",
    "        logging.basicConfig(\n",
    "            filename=log_file,\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            encoding='utf-8'\n",
    "        )\n",
    "        \n",
    "        # コンソールにもログを出力\n",
    "        console = logging.StreamHandler()\n",
    "        console.setLevel(logging.INFO)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        console.setFormatter(formatter)\n",
    "        logging.getLogger('').addHandler(console)\n",
    "        \n",
    "        # ログファイルの場所を明示的に表示\n",
    "        print(f\"ログファイル出力先: {log_file}\")\n",
    "        logging.info(f\"ログ機能の初期化が完了しました: {log_file}\")\n",
    "\n",
    "    def _find_project_root(self):\n",
    "        \"\"\"\n",
    "        プロジェクトのルートディレクトリを検出する\n",
    "        \"\"\"\n",
    "        # 現在のファイルの絶対パスを取得\n",
    "        current_dir = os.path.abspath(os.getcwd())\n",
    "        \n",
    "        # 親ディレクトリを探索\n",
    "        path = Path(current_dir)\n",
    "        while True:\n",
    "            # .gitディレクトリがあればそれをルートとみなす\n",
    "            if (path / '.git').exists():\n",
    "                return str(path)\n",
    "            \n",
    "            # プロジェクトのルートを示す他のファイル/ディレクトリの存在チェック\n",
    "            if (path / 'setup.py').exists() or (path / 'README.md').exists():\n",
    "                return str(path)\n",
    "            \n",
    "            # これ以上上の階層がない場合は現在のディレクトリを返す\n",
    "            if path.parent == path:\n",
    "                return str(path)\n",
    "            \n",
    "            # 親ディレクトリへ\n",
    "            path = path.parent\n",
    "\n",
    "    def _setup_browser(self):\n",
    "        \"\"\"\n",
    "        Seleniumのブラウザを設定します\n",
    "        \n",
    "        ChromeOptionsの設定とWebDriverの起動を行います\n",
    "        \"\"\"\n",
    "        # Chromeオプション設定\n",
    "        chrome_options = Options()\n",
    "        if self.headless_mode:\n",
    "            chrome_options.add_argument(\"--headless\")  # ヘッドレスモード（画面表示なし）\n",
    "        chrome_options.add_argument(\"--disable-gpu\")  # GPU無効化（ヘッドレス時に推奨）\n",
    "        chrome_options.add_argument(\"--window-size=1920x1080\")  # ウィンドウサイズ設定\n",
    "        \n",
    "        try:\n",
    "            # ドライバーパスが指定されている場合は直接使用\n",
    "            if hasattr(self, 'driver_path') and self.driver_path:\n",
    "                self.browser = webdriver.Chrome(service=Service(self.driver_path), options=chrome_options)\n",
    "            # 指定がなければWebDriverManagerを使用\n",
    "            else:\n",
    "                self.browser = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=chrome_options)\n",
    "            \n",
    "            self.wait = WebDriverWait(self.browser, 10)  # 要素が見つかるまで最大10秒待機\n",
    "            print(\"ブラウザの初期化に成功しました\")\n",
    "        except Exception as e:\n",
    "            print(f\"ブラウザの初期化に失敗しました: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _setup_session(self):\n",
    "        \"\"\"\n",
    "        requestsのセッションを設定します\n",
    "        \n",
    "        Seleniumのブラウザから取得したCookieをrequestsセッションに設定します\n",
    "        これによりログイン状態を維持したままBeautifulSoupでページ取得できます\n",
    "        \"\"\"\n",
    "        # ブラウザのCookieを取得\n",
    "        cookies = self.browser.get_cookies()\n",
    "        \n",
    "        # requestsセッションを作成\n",
    "        self.session = requests.Session()\n",
    "        \n",
    "        # Seleniumから取得したCookieをrequestsセッションに追加\n",
    "        for cookie in cookies:\n",
    "            self.session.cookies.set(cookie['name'], cookie['value'])\n",
    "\n",
    "    def prepare_csv(self):\n",
    "        \"\"\"\n",
    "        CSVファイルを初期化します\n",
    "        \n",
    "        既存のファイルがある場合は削除し、新しいヘッダー付きのCSVを作成します\n",
    "        \"\"\"\n",
    "        # 既存ファイルがあれば削除\n",
    "        if os.path.exists(self.csv_filename):\n",
    "            os.remove(self.csv_filename)\n",
    "        \n",
    "        # カラム構成でCSVを初期化\n",
    "        df = pd.DataFrame(columns=self.columns)\n",
    "        df.to_csv(self.csv_filename, index=False, encoding=\"utf-8-sig\")\n",
    "        \n",
    "        print(f\"CSVファイル {self.csv_filename} を初期化しました\")\n",
    "\n",
    "    def save_to_csv(self, data):\n",
    "        \"\"\"\n",
    "        データをCSVファイルに保存します\n",
    "        \n",
    "        Args:\n",
    "            data (list): 保存するデータ（リストのリスト形式）\n",
    "            \n",
    "        Returns:\n",
    "            bool: 保存成功時はTrue、失敗時はFalse\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if data:\n",
    "                # データフレームに変換\n",
    "                df = pd.DataFrame(data, columns=self.columns)\n",
    "                \n",
    "                # CSVに追記（ヘッダーなし）\n",
    "                df.to_csv(self.csv_filename, mode=\"a\", index=False, header=False, encoding=\"utf-8-sig\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"保存するデータがありません\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print(f\"CSV保存エラー: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def __init__(self, site_name, config_path=None, headless_mode=False):\n",
    "        \"\"\"\n",
    "        BaseScraperの初期化\n",
    "        \n",
    "        Args:\n",
    "            site_name (str): スクレイピングするサイト名（ログファイル名などに使用）\n",
    "            config_path (str): 設定ファイルのパス（指定しない場合はデフォルト値を使用）\n",
    "            headless_mode (bool): ブラウザを画面に表示せずに実行する場合はTrue\n",
    "        \"\"\"\n",
    "        # サイト名の設定\n",
    "        self.site_name = site_name\n",
    "        \n",
    "        # プロジェクトルートディレクトリの検出\n",
    "        self.root_dir = self._find_project_root()\n",
    "        \n",
    "        # 環境変数の読み込み\n",
    "        load_dotenv(os.path.join(self.root_dir, '.env'))\n",
    "        \n",
    "        # ディレクトリパスの設定\n",
    "        self.data_dir = os.path.join(self.root_dir, 'data')\n",
    "        self.log_dir = os.path.join(self.root_dir, 'logs')\n",
    "        \n",
    "        # ディレクトリが存在しない場合は作成\n",
    "        os.makedirs(self.data_dir, exist_ok=True)\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "        \n",
    "        # 設定ファイルの読み込み\n",
    "        self.config = self._load_config(config_path)\n",
    "        \n",
    "        # 基本設定\n",
    "        self.headless_mode = headless_mode\n",
    "        self.browser = None\n",
    "        self.wait = None\n",
    "        self.session = None\n",
    "        \n",
    "        # 実行時間計測用\n",
    "        self.start_time = None\n",
    "        self.end_time = None\n",
    "\n",
    "        # ログ設定を初期化時に行う\n",
    "        self.setup_logging()\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        ブラウザを閉じてリソースを解放します\n",
    "        \"\"\"\n",
    "        if self.browser:\n",
    "            self.browser.quit()\n",
    "            print(\"ブラウザを閉じました\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23b3e0a-e398-4bf0-bc0a-c590780877e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# セル2: NETSEA個別クラス（NetseaScraper）\n",
    "\n",
    "\"\"\"\n",
    "NetseaScraper - ネッシーから商品情報を取得するスクレイピングモジュール\n",
    "\n",
    "このモジュールはネッシーのウェブサイトから商品情報を取得するための\n",
    "クラスを提供します。BaseScraper を継承し、NETSEA固有の機能を実装しています。\n",
    "\"\"\"\n",
    "\n",
    "class NetseaScraper(BaseScraper):\n",
    "    \"\"\"\n",
    "    ネッシーのWebサイトから商品情報をスクレイピングするクラス\n",
    "    \n",
    "    このクラスはSeleniumでログインと商品一覧ページの処理を行い、\n",
    "    BeautifulSoupで個別商品ページの詳細情報を取得するハイブリッド方式を採用しています。\n",
    "    \"\"\"\n",
    "    \n",
    "    def _merge_env_variables(self, config):\n",
    "        \"\"\"環境変数から認証情報を取得し、設定ファイルにマージする\"\"\"\n",
    "        # ネッシーの認証情報\n",
    "        username = os.getenv('NETSEA_USERNAME')\n",
    "        password = os.getenv('NETSEA_PASSWORD')\n",
    "        \n",
    "        if username and password:\n",
    "            if 'scrapers' not in config:\n",
    "                config['scrapers'] = {}\n",
    "            if 'netsea' not in config['scrapers']:\n",
    "                config['scrapers']['netsea'] = {}\n",
    "            if 'login' not in config['scrapers']['netsea']:\n",
    "                config['scrapers']['netsea']['login'] = {}\n",
    "            \n",
    "            config['scrapers']['netsea']['login']['username'] = username\n",
    "            config['scrapers']['netsea']['login']['password'] = password\n",
    "            print(\"ネッシーのログイン情報を環境変数から設定しました\")\n",
    "        else:\n",
    "            print(\"警告: 環境変数からネッシーのログイン情報を取得できませんでした\")\n",
    "            \n",
    "        # デフォルトの出力設定（なければ設定）\n",
    "        if 'output' not in config['scrapers']['netsea']:\n",
    "            config['scrapers']['netsea']['output'] = {\n",
    "                'csv_filename': 'netsea_scraping.csv',\n",
    "                'log_dir': 'logs'\n",
    "            }\n",
    "    \n",
    "    def __init__(self, config_path=None, headless_mode=False):\n",
    "        \"\"\"\n",
    "        NetseaScraperの初期化\n",
    "        \n",
    "        Args:\n",
    "            config_path (str): 設定ファイルのパス（指定しない場合はデフォルト値を使用）\n",
    "            headless_mode (bool): ブラウザを画面に表示せずに実行する場合はTrue\n",
    "        \"\"\"\n",
    "        # 親クラス(BaseScraper)の初期化\n",
    "        super().__init__('netsea', config_path, headless_mode)\n",
    "        \n",
    "        # NETSEA固有の設定\n",
    "        self.netsea_config = self.config['scrapers']['netsea']\n",
    "        self.base_url = \"https://www.netsea.jp\"\n",
    "        \n",
    "        # 出力設定（設定ファイルから読み込み）\n",
    "        output_config = self.netsea_config.get('output', {})\n",
    "        csv_filename = output_config.get('csv_filename', \"netsea_scraping.csv\")\n",
    "        \n",
    "        # CSVのフルパスを設定\n",
    "        self.csv_filename = os.path.join(self.data_dir, csv_filename)\n",
    "        print(f\"CSVファイル出力先: {self.csv_filename}\")\n",
    "        \n",
    "        # カラム設定（NETSEA固有）\n",
    "        self.columns = [\"卸業者名\", \"商品名\", \"JANコード\", \"価格\", \"セット数\"]\n",
    "        \n",
    "        # ブラウザ設定\n",
    "        self._setup_browser()\n",
    "        \n",
    "        # # ログ設定\n",
    "        # self.setup_logging()\n",
    "    \n",
    "    def login(self, username=None, password=None):\n",
    "        \"\"\"\n",
    "        ネッシーにログインします\n",
    "        \n",
    "        Args:\n",
    "            username (str): ネッシーのログインユーザー名（指定なしの場合は設定ファイルから読み込み）\n",
    "            password (str): ネッシーのパスワード（指定なしの場合は設定ファイルから読み込み）\n",
    "            \n",
    "        Returns:\n",
    "            bool: ログイン成功時はTrue、失敗時はFalse\n",
    "        \"\"\"\n",
    "        # ユーザー名とパスワードが指定されていない場合は設定ファイルから読み込む\n",
    "        if username is None or password is None:\n",
    "            username = self.netsea_config['login']['username']\n",
    "            password = self.netsea_config['login']['password']\n",
    "            \n",
    "        try:\n",
    "            # ログインページにアクセス\n",
    "            self.browser.get(f\"{self.base_url}/login\")\n",
    "            \n",
    "            # ログインフォームに入力\n",
    "            self.wait.until(EC.presence_of_element_located((By.NAME, \"login_id\"))).send_keys(username)\n",
    "            self.browser.find_element(By.NAME, \"password\").send_keys(password)\n",
    "            self.browser.find_element(By.NAME, \"submit\").click()\n",
    "            \n",
    "            # ログイン成功の確認（ログイン後ページの特定要素が表示されるか）\n",
    "            self.wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"globalNav\")))\n",
    "            \n",
    "            # ログイン後、Cookieを取得してrequestsセッションを準備\n",
    "            self._setup_session()\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"ログインエラー: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def get_product_urls(self, page):\n",
    "        \"\"\"\n",
    "        商品一覧ページから個別商品ページのURLリストを取得します\n",
    "        \n",
    "        Args:\n",
    "            page (int): 取得する商品一覧ページの番号\n",
    "            \n",
    "        Returns:\n",
    "            list: 個別商品ページのURLリスト\n",
    "        \"\"\"\n",
    "        product_urls = []\n",
    "        print(f\"現在 {page} ページ目をスクレイピング中...\")\n",
    "        \n",
    "        try:\n",
    "            # 商品一覧ページにアクセス（ここではショップID 5984 を例として使用）\n",
    "            url = f\"{self.base_url}/shop/5984?sort=sales&page={page}\"\n",
    "            self.browser.get(url)\n",
    "            \n",
    "            # 商品リストが表示されるまで待機\n",
    "            self.wait.until(EC.visibility_of_element_located((By.ID, \"searchResultsArea\")))\n",
    "            \n",
    "            # 商品リスト領域を取得\n",
    "            product_grid = self.browser.find_element(By.ID, \"searchResultsArea\")\n",
    "            \n",
    "            # 各商品要素を取得\n",
    "            products = product_grid.find_elements(By.CLASS_NAME, \"showcaseType01\")\n",
    "            \n",
    "            # 各商品のURLを取得\n",
    "            for product in products:\n",
    "                try:\n",
    "                    title_block = product.find_element(By.CLASS_NAME, \"showcaseHd\")\n",
    "                    product_url = title_block.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
    "                    product_urls.append(product_url)\n",
    "                except Exception as e:\n",
    "                    print(f\"商品URL取得エラー（ページ {page}）: {str(e)}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"ページ {page} の商品リスト取得エラー: {str(e)}\")\n",
    "        \n",
    "        return product_urls\n",
    "    \n",
    "    def get_product_urls_from_url_bs4(self, url, page_number):\n",
    "        \"\"\"\n",
    "        指定されたURLから商品URLリストをBeautifulSoupで取得します\n",
    "        \n",
    "        Args:\n",
    "            url (str): 商品一覧ページのURL\n",
    "            page_number (int): ページ番号（表示用）\n",
    "        \n",
    "        Returns:\n",
    "            list: 商品URLのリスト\n",
    "        \"\"\"\n",
    "        product_urls = []\n",
    "        print(f\"現在 {page_number} ページ目をスクレイピング中... ({url})\")\n",
    "        \n",
    "        try:\n",
    "            # requestsを使ってページを取得\n",
    "            response = self.session.get(url)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # 商品リスト領域を探す\n",
    "            search_results = soup.find(id=\"searchResultsArea\")\n",
    "            if not search_results:\n",
    "                print(f\"ページ {page_number} の商品リスト領域が見つかりません\")\n",
    "                return product_urls\n",
    "                \n",
    "            # 各商品要素を取得\n",
    "            products = search_results.find_all(class_=\"showcaseType01\")\n",
    "            \n",
    "            # 各商品のURLを取得\n",
    "            for product in products:\n",
    "                try:\n",
    "                    title_block = product.find(class_=\"showcaseHd\")\n",
    "                    if title_block and title_block.a:\n",
    "                        product_url = title_block.a.get('href')\n",
    "                        product_urls.append(product_url)\n",
    "                except Exception as e:\n",
    "                    print(f\"商品URL取得エラー（ページ {page_number}）: {str(e)}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"ページ {page_number} の商品リスト取得エラー: {str(e)}\")\n",
    "        \n",
    "        return product_urls\n",
    "    \n",
    "    def get_product_data(self, product_urls):\n",
    "        \"\"\"\n",
    "        商品URLリストから詳細データを取得します\n",
    "        \n",
    "        Args:\n",
    "            product_urls (list): 個別商品ページのURLリスト\n",
    "            \n",
    "        Returns:\n",
    "            list: 各商品の詳細データのリスト\n",
    "        \"\"\"\n",
    "        page_data = []\n",
    "        \n",
    "        for product_url in product_urls:\n",
    "            try:\n",
    "                # requestsとBeautifulSoupを使用してページを取得（高速化のため）\n",
    "                response = self.session.get(product_url)\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                # 卸業者名（ブランド）を抽出\n",
    "                brand = \"不明\"  # デフォルト値\n",
    "                for script in soup.find_all('script'):\n",
    "                    if script.string and 'brand' in script.string:\n",
    "                        brand_match = re.search(r'brand: [\\\\\\'\"](.+?)[\\\\\\'\"]', script.string)\n",
    "                        if brand_match:\n",
    "                            brand = brand_match.group(1)\n",
    "                            break\n",
    "                \n",
    "                # JANコード（gtin13）を抽出 - 修正版\n",
    "                jan_code = \"\"  # デフォルト値\n",
    "                ld_json_scripts = soup.find_all('script', type='application/ld+json')\n",
    "                for script in ld_json_scripts:\n",
    "                    if script.string:\n",
    "                        try:\n",
    "                            # JSONデータをパース\n",
    "                            product_data = json.loads(script.string)\n",
    "                            # Product タイプのJSONで、gtin13が含まれている場合のみ\n",
    "                            if product_data.get('@type') == 'Product' and 'gtin13' in product_data:\n",
    "                                jan_code = product_data.get('gtin13', '')\n",
    "                                break  # 見つかったら終了\n",
    "                        except json.JSONDecodeError:\n",
    "                            # JSON解析エラーの場合は正規表現で抽出を試みる\n",
    "                            jan_match = re.search(r'\"gtin13\":\"(\\d+)\"', script.string)\n",
    "                            if jan_match:\n",
    "                                jan_code = jan_match.group(1)\n",
    "                                break  # 見つかったら終了\n",
    "                \n",
    "                # JANコードが取得できなかった場合、URLから抽出する従来の方法をフォールバックとして使用\n",
    "                if not jan_code:\n",
    "                    jan_code = product_url.split('/')[-1]\n",
    "                    print(f\"警告: gtin13が見つからないため、URL末尾をJANコードとして使用: {jan_code}\")\n",
    "                \n",
    "                # 商品データを含むスクリプトを探す\n",
    "                script_text = None\n",
    "                for script in soup.find_all('script'):\n",
    "                    if script.string and 'ecItemSetList' in script.string:\n",
    "                        script_text = script.string\n",
    "                        break\n",
    "                \n",
    "                if script_text:\n",
    "                    # 正規表現で商品情報を抽出（商品名、価格、セット数）\n",
    "                    items = re.findall(r'_id: .*?_name: [\\\\\\'\"](.+?)[\\\\\\'\"].*?_priceExcTax: [\\\\\\'\"](\\d+)[\\\\\\'\"].*?_numInSet: [\\\\\\'\"](\\d+)[\\\\\\'\"]', \n",
    "                                      script_text, re.DOTALL)\n",
    "                    \n",
    "                    # 抽出した情報を整形して追加\n",
    "                    for name, price, num_in_set in items:\n",
    "                        row_data = [\n",
    "                            brand,          # 卸業者名\n",
    "                            name,           # 商品名\n",
    "                            jan_code,       # JANコード（新しい方法で取得）\n",
    "                            price,          # 価格\n",
    "                            num_in_set      # セット数\n",
    "                        ]\n",
    "                        page_data.append(row_data)\n",
    "                else:\n",
    "                    print(f\"商品データが見つかりませんでした: {product_url}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"個別ページ {product_url} のデータ取得エラー: {str(e)}\")\n",
    "        \n",
    "        return page_data\n",
    "    \n",
    "    def scrape_pages(self, start_page=1, end_page=1):\n",
    "        \"\"\"\n",
    "        指定したページ範囲の商品情報をスクレイピングします\n",
    "        \n",
    "        Args:\n",
    "            start_page (int): 開始ページ番号\n",
    "            end_page (int): 終了ページ番号\n",
    "            \n",
    "        Returns:\n",
    "            int: 取得した商品データの総数\n",
    "        \"\"\"\n",
    "        # 実行時間測定開始\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "        # CSVを初期化\n",
    "        self.prepare_csv()\n",
    "        \n",
    "        total_items = 0\n",
    "        \n",
    "        # 指定ページ範囲をスクレイピング\n",
    "        for page in range(start_page, end_page + 1):\n",
    "            # 商品URLを取得\n",
    "            product_urls = self.get_product_urls(page)\n",
    "            \n",
    "            # 商品データを取得\n",
    "            page_data = self.get_product_data(product_urls)\n",
    "            \n",
    "            # 1ページごとにCSVに保存\n",
    "            if page_data:\n",
    "                self.save_to_csv(page_data)\n",
    "                total_items += len(page_data)\n",
    "                print(f\"ページ {page} のデータ ({len(page_data)}件) をCSVに保存しました\")\n",
    "        \n",
    "        # 実行時間測定終了\n",
    "        self.end_time = time.time()\n",
    "        elapsed_time = self.end_time - self.start_time\n",
    "        \n",
    "        print(f\"スクレイピング完了！ 合計{total_items}件のデータを取得しました\")\n",
    "        print(f\"実行時間: {elapsed_time:.2f} 秒\")\n",
    "        \n",
    "        return total_items\n",
    "    \n",
    "    def scrape_all_targets(self):\n",
    "        \"\"\"\n",
    "        設定ファイルに指定されたすべてのターゲットページをスクレイピングします\n",
    "        \n",
    "        Returns:\n",
    "            int: 取得した商品データの総数\n",
    "        \"\"\"\n",
    "        # 実行時間測定開始\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "        # CSVを初期化\n",
    "        self.prepare_csv()\n",
    "        \n",
    "        # ログイン（設定ファイルの情報を使用）\n",
    "        if not self.login():\n",
    "            print(\"ログインに失敗しました\")\n",
    "            return 0\n",
    "        \n",
    "        # 各ターゲットページの処理\n",
    "        total_items = 0\n",
    "        \n",
    "        for target in self.netsea_config['target_pages']:\n",
    "            print(f\"\\n===== {target['name']} の処理を開始 =====\")\n",
    "            \n",
    "            # URLとページ範囲を取得\n",
    "            base_url = target['url']\n",
    "            sort = target.get('sort', '')  # ソート条件（指定がなければ空文字）\n",
    "            start_page = target.get('start_page', 1)\n",
    "            end_page = target.get('end_page', 1)\n",
    "            \n",
    "            # 各ページの処理\n",
    "            for page in range(start_page, end_page + 1):\n",
    "                # URLを構築\n",
    "                page_url = f\"{base_url}?sort={sort}&page={page}\"\n",
    "                \n",
    "                # 商品URLを取得\n",
    "                product_urls = self.get_product_urls_from_url_bs4(page_url, page)\n",
    "                \n",
    "                # 商品データを取得\n",
    "                page_data = self.get_product_data(product_urls)\n",
    "                \n",
    "                # データを保存\n",
    "                if page_data:\n",
    "                    self.save_to_csv(page_data)\n",
    "                    total_items += len(page_data)\n",
    "                    print(f\"ページ {page} のデータ ({len(page_data)}件) をCSVに保存しました\")\n",
    "        \n",
    "        # 実行時間測定終了\n",
    "        self.end_time = time.time()\n",
    "        elapsed_time = self.end_time - self.start_time\n",
    "        \n",
    "        print(f\"\\n===== スクレイピング完了 - 合計 {total_items} 件 =====\")\n",
    "        print(f\"実行時間: {elapsed_time:.2f} 秒\")\n",
    "        \n",
    "        return total_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36d75dd-929d-4759-9f4c-e707352d163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# スクレイパーのインスタンスを作成して実行するコード\n",
    "if __name__ == \"__main__\":\n",
    "    # スクレイパーのインスタンスを作成\n",
    "    scraper = NetseaScraper(headless_mode=False)\n",
    "\n",
    "    try:\n",
    "        # 設定ファイルに基づいて全ターゲットをスクレイピング\n",
    "        scraper.scrape_all_targets()\n",
    "    finally:\n",
    "        # 終了処理\n",
    "        scraper.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7daafe-4479-4857-bdc4-94cda1f20f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efa00a5-d2d6-4fbe-b9fd-1c4b590c5eda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef5ad4e-5645-4cfd-9af5-8e88c3c1a0d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974c274e-7676-4be3-8a44-509af1c90bc5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # スクレイパーのインスタンスを作成\n",
    "# scraper = NetseaScraper(headless_mode=False)\n",
    "\n",
    "# try:\n",
    "#     # 設定ファイルに基づいて全ターゲットをスクレイピング\n",
    "#     scraper.scrape_all_targets()\n",
    "# finally:\n",
    "#     # 終了処理\n",
    "#     scraper.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fce22ca-8e8b-4e77-b1cd-dae8d3732c1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82b5770-ac67-4d6e-b4d8-1a3e1a030f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 413秒/10ページ(628商品) = 0.657秒/1商品　　※ネッシーブレイカーは1.00秒/1商品のため、私のコードのほうが優れている！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045b26b4-c605-42d4-8b63-3a0fbeac782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 401秒/10ページ(628商品) = 0.638秒/1商品\n",
    "# 5716秒(1.5時間)/166ページ(10250商品) = 0.55秒/1商品　25/3/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9e604b-ca38-48a4-b9fc-2d27826c1976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895b4ac0-7bec-4dff-bfac-52af9f2010b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c51e02-4d6c-45cf-b49c-9075cebffa9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dc90cf-852e-4a3a-a800-638eff422b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# メソッドの実行順序\n",
    "# NetseaScraperを実行すると、以下の順序でメソッドが呼び出されます：\n",
    "\n",
    "# 初期化フェーズ:\n",
    "\n",
    "# scraper = NetseaScraper(headless_mode=False) で初期化\n",
    "# __init__ メソッド内で以下の順序で処理:\n",
    "\n",
    "# _find_project_root() - プロジェクトルート検出\n",
    "# 環境変数の読み込み\n",
    "# ディレクトリパスの設定\n",
    "# _load_config() - 設定ファイル読み込み\n",
    "\n",
    "# _merge_env_variables() - 環境変数の設定ファイルへのマージ\n",
    "\n",
    "\n",
    "# setup_browser() - ブラウザの初期化\n",
    "# setup_logging() - ログの初期化\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# スクレイピング実行フェーズ:\n",
    "\n",
    "# scraper.scrape_all_targets() の呼び出し\n",
    "# scrape_all_targets() メソッド内で:\n",
    "\n",
    "# prepare_csv() - CSV初期化\n",
    "# login() - ネッシーにログイン\n",
    "\n",
    "# _setup_session() - リクエストセッションの初期化\n",
    "\n",
    "\n",
    "# 各ターゲットページに対して:\n",
    "\n",
    "# get_product_urls_from_url_bs4() - 商品URLの取得\n",
    "# get_product_data() - 各商品の詳細データ取得\n",
    "# save_to_csv() - データをCSVに保存\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 終了フェーズ:\n",
    "\n",
    "# scraper.close() - ブラウザのクローズとリソース解放\n",
    "\n",
    "\n",
    "\n",
    "# この分割により、共通機能（初期化、設定読み込み、ブラウザ設定など）と固有機能（ネッシー専用のスクレイピングロジック）が明確に分離され"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
